[
  {
    "name": "README.md",
    "relative_path": "README.md",
    "path": "docs/README.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/README.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>README.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>README.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/README.md\">docs/README.md</a></em></p>\n                <hr>\n                <h1 id=\"documentation\">Documentation</h1>\n<h3 id=\"getting-started\">Getting Started</h3>\n<ul>\n<li><a href=\"../README.md#quickstart\">Quickstart</a></li>\n<li><a href=\"./examples.md\">Examples</a></li>\n<li><a href=\"./import.md\">Importing models</a></li>\n<li><a href=\"./macos.md\">MacOS Documentation</a></li>\n<li><a href=\"./linux.md\">Linux Documentation</a></li>\n<li><a href=\"./windows.md\">Windows Documentation</a></li>\n<li><a href=\"./docker.md\">Docker Documentation</a></li>\n</ul>\n<h3 id=\"reference\">Reference</h3>\n<ul>\n<li><a href=\"./api.md\">API Reference</a></li>\n<li><a href=\"./modelfile.md\">Modelfile Reference</a></li>\n<li><a href=\"./openai.md\">OpenAI Compatibility</a></li>\n</ul>\n<h3 id=\"resources\">Resources</h3>\n<ul>\n<li><a href=\"./troubleshooting.md\">Troubleshooting Guide</a></li>\n<li><a href=\"./faq.md\">FAQ</a></li>\n<li><a href=\"./development.md\">Development guide</a></li>\n</ul>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Documentation\n\n### Getting Started\n* [Quickstart](../README.md#quickstart)\n* [Examples](./examples.md)\n* [Importing models](./import.md)\n* [MacOS Documentation](./macos.md)\n* [Linux Documentation](./linux.md)\n* [Windows Documentation](./windows.md)\n* [Docker Documentation](./docker.md)\n\n### Reference\n\n* [API Reference](./api.md)\n* [Modelfile Reference](./modelfile.md)\n* [OpenAI Compatibility](./openai.md)\n\n### Resources\n\n* [Troubleshooting Guide](./troubleshooting.md)\n* [FAQ](./faq.md)\n* [Development guide](./development.md)\n"
  },
  {
    "name": "api.md",
    "relative_path": "api.md",
    "path": "docs/api.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/api.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>api.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>api.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/api.md\">docs/api.md</a></em></p>\n                <hr>\n                <h1 id=\"api\">API</h1>\n<h2 id=\"endpoints\">Endpoints</h2>\n<ul>\n<li><a href=\"#generate-a-completion\">Generate a completion</a></li>\n<li><a href=\"#generate-a-chat-completion\">Generate a chat completion</a></li>\n<li><a href=\"#create-a-model\">Create a Model</a></li>\n<li><a href=\"#list-local-models\">List Local Models</a></li>\n<li><a href=\"#show-model-information\">Show Model Information</a></li>\n<li><a href=\"#copy-a-model\">Copy a Model</a></li>\n<li><a href=\"#delete-a-model\">Delete a Model</a></li>\n<li><a href=\"#pull-a-model\">Pull a Model</a></li>\n<li><a href=\"#push-a-model\">Push a Model</a></li>\n<li><a href=\"#generate-embeddings\">Generate Embeddings</a></li>\n<li><a href=\"#list-running-models\">List Running Models</a></li>\n<li><a href=\"#version\">Version</a></li>\n</ul>\n<h2 id=\"conventions\">Conventions</h2>\n<h3 id=\"model-names\">Model names</h3>\n<p>Model names follow a <code>model:tag</code> format, where <code>model</code> can have an optional namespace such as <code>example/model</code>. Some examples are <code>orca-mini:3b-q8_0</code> and <code>llama3:70b</code>. The tag is optional and, if not provided, will default to <code>latest</code>. The tag is used to identify a specific version.</p>\n<h3 id=\"durations\">Durations</h3>\n<p>All durations are returned in nanoseconds.</p>\n<h3 id=\"streaming-responses\">Streaming responses</h3>\n<p>Certain endpoints stream responses as JSON objects. Streaming can be disabled by providing <code>{\"stream\": false}</code> for these endpoints.</p>\n<h2 id=\"generate-a-completion\">Generate a completion</h2>\n<pre class=\"codehilite\"><code>POST /api/generate\n</code></pre>\n\n<p>Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.</p>\n<h3 id=\"parameters\">Parameters</h3>\n<ul>\n<li><code>model</code>: (required) the <a href=\"#model-names\">model name</a></li>\n<li><code>prompt</code>: the prompt to generate a response for</li>\n<li><code>suffix</code>: the text after the model response</li>\n<li><code>images</code>: (optional) a list of base64-encoded images (for multimodal models such as <code>llava</code>)</li>\n<li><code>think</code>: (for thinking models) should the model think before responding?</li>\n</ul>\n<p>Advanced parameters (optional):</p>\n<ul>\n<li><code>format</code>: the format to return a response in. Format can be <code>json</code> or a JSON schema</li>\n<li><code>options</code>: additional model parameters listed in the documentation for the <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> such as <code>temperature</code></li>\n<li><code>system</code>: system message to (overrides what is defined in the <code>Modelfile</code>)</li>\n<li><code>template</code>: the prompt template to use (overrides what is defined in the <code>Modelfile</code>)</li>\n<li><code>stream</code>: if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>\n<li><code>raw</code>: if <code>true</code> no formatting will be applied to the prompt. You may choose to use the <code>raw</code> parameter if you are specifying a full templated prompt in your request to the API</li>\n<li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li>\n<li><code>context</code> (deprecated): the context parameter returned from a previous request to <code>/generate</code>, this can be used to keep a short conversational memory</li>\n</ul>\n<h4 id=\"structured-outputs\">Structured outputs</h4>\n<p>Structured outputs are supported by providing a JSON schema in the <code>format</code> parameter. The model will generate a response that matches the schema. See the <a href=\"#request-structured-outputs\">structured outputs</a> example below.</p>\n<h4 id=\"json-mode\">JSON mode</h4>\n<p>Enable JSON mode by setting the <code>format</code> parameter to <code>json</code>. This will structure the response as a valid JSON object. See the JSON mode <a href=\"#request-json-mode\">example</a> below.</p>\n<blockquote>\n<p>[!IMPORTANT]\nIt's important to instruct the model to use JSON in the <code>prompt</code>. Otherwise, the model may generate large amounts whitespace.</p>\n</blockquote>\n<h3 id=\"examples\">Examples</h3>\n<h4 id=\"generate-request-streaming\">Generate request (Streaming)</h4>\n<h5 id=\"request\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;\n}'\n</code></pre>\n\n<h5 id=\"response\">Response</h5>\n<p>A stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-08-04T08:52:19.385406455-07:00&quot;,\n  &quot;response&quot;: &quot;The&quot;,\n  &quot;done&quot;: false\n}\n</code></pre>\n\n<p>The final response in the stream also includes additional data about the generation:</p>\n<ul>\n<li><code>total_duration</code>: time spent generating the response</li>\n<li><code>load_duration</code>: time spent in nanoseconds loading the model</li>\n<li><code>prompt_eval_count</code>: number of tokens in the prompt</li>\n<li><code>prompt_eval_duration</code>: time spent in nanoseconds evaluating the prompt</li>\n<li><code>eval_count</code>: number of tokens in the response</li>\n<li><code>eval_duration</code>: time in nanoseconds spent generating the response</li>\n<li><code>context</code>: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory</li>\n<li><code>response</code>: empty if the response was streamed, if not streamed, this will contain the full response</li>\n</ul>\n<p>To calculate how fast the response is generated in tokens per second (token/s), divide <code>eval_count</code> / <code>eval_duration</code> * <code>10^9</code>.</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,\n  &quot;response&quot;: &quot;&quot;,\n  &quot;done&quot;: true,\n  &quot;context&quot;: [1, 2, 3],\n  &quot;total_duration&quot;: 10706818083,\n  &quot;load_duration&quot;: 6338219291,\n  &quot;prompt_eval_count&quot;: 26,\n  &quot;prompt_eval_duration&quot;: 130079000,\n  &quot;eval_count&quot;: 259,\n  &quot;eval_duration&quot;: 4232710000\n}\n</code></pre>\n\n<h4 id=\"request-no-streaming\">Request (No streaming)</h4>\n<h5 id=\"request_1\">Request</h5>\n<p>A response can be received in one reply when streaming is off.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,\n  &quot;stream&quot;: false\n}'\n</code></pre>\n\n<h5 id=\"response_1\">Response</h5>\n<p>If <code>stream</code> is set to <code>false</code>, the response will be a single JSON object:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,\n  &quot;response&quot;: &quot;The sky is blue because it is the color of the sky.&quot;,\n  &quot;done&quot;: true,\n  &quot;context&quot;: [1, 2, 3],\n  &quot;total_duration&quot;: 5043500667,\n  &quot;load_duration&quot;: 5025959,\n  &quot;prompt_eval_count&quot;: 26,\n  &quot;prompt_eval_duration&quot;: 325953000,\n  &quot;eval_count&quot;: 290,\n  &quot;eval_duration&quot;: 4709213000\n}\n</code></pre>\n\n<h4 id=\"request-with-suffix\">Request (with suffix)</h4>\n<h5 id=\"request_2\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;codellama:code&quot;,\n  &quot;prompt&quot;: &quot;def compute_gcd(a, b):&quot;,\n  &quot;suffix&quot;: &quot;    return result&quot;,\n  &quot;options&quot;: {\n    &quot;temperature&quot;: 0\n  },\n  &quot;stream&quot;: false\n}'\n</code></pre>\n\n<h5 id=\"response_2\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json5\">{\n  &quot;model&quot;: &quot;codellama:code&quot;,\n  &quot;created_at&quot;: &quot;2024-07-22T20:47:51.147561Z&quot;,\n  &quot;response&quot;: &quot;\\n  if a == 0:\\n    return b\\n  else:\\n    return compute_gcd(b % a, a)\\n\\ndef compute_lcm(a, b):\\n  result = (a * b) / compute_gcd(a, b)\\n&quot;,\n  &quot;done&quot;: true,\n  &quot;done_reason&quot;: &quot;stop&quot;,\n  &quot;context&quot;: [...],\n  &quot;total_duration&quot;: 1162761250,\n  &quot;load_duration&quot;: 6683708,\n  &quot;prompt_eval_count&quot;: 17,\n  &quot;prompt_eval_duration&quot;: 201222000,\n  &quot;eval_count&quot;: 63,\n  &quot;eval_duration&quot;: 953997000\n}\n</code></pre>\n\n<h4 id=\"request-structured-outputs\">Request (Structured outputs)</h4>\n<h5 id=\"request_3\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -X POST http://localhost:11434/api/generate -H &quot;Content-Type: application/json&quot; -d '{\n  &quot;model&quot;: &quot;llama3.1:8b&quot;,\n  &quot;prompt&quot;: &quot;Ollama is 22 years old and is busy saving the world. Respond using JSON&quot;,\n  &quot;stream&quot;: false,\n  &quot;format&quot;: {\n    &quot;type&quot;: &quot;object&quot;,\n    &quot;properties&quot;: {\n      &quot;age&quot;: {\n        &quot;type&quot;: &quot;integer&quot;\n      },\n      &quot;available&quot;: {\n        &quot;type&quot;: &quot;boolean&quot;\n      }\n    },\n    &quot;required&quot;: [\n      &quot;age&quot;,\n      &quot;available&quot;\n    ]\n  }\n}'\n</code></pre>\n\n<h5 id=\"response_3\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.1:8b&quot;,\n  &quot;created_at&quot;: &quot;2024-12-06T00:48:09.983619Z&quot;,\n  &quot;response&quot;: &quot;{\\n  \\&quot;age\\&quot;: 22,\\n  \\&quot;available\\&quot;: true\\n}&quot;,\n  &quot;done&quot;: true,\n  &quot;done_reason&quot;: &quot;stop&quot;,\n  &quot;context&quot;: [1, 2, 3],\n  &quot;total_duration&quot;: 1075509083,\n  &quot;load_duration&quot;: 567678166,\n  &quot;prompt_eval_count&quot;: 28,\n  &quot;prompt_eval_duration&quot;: 236000000,\n  &quot;eval_count&quot;: 16,\n  &quot;eval_duration&quot;: 269000000\n}\n</code></pre>\n\n<h4 id=\"request-json-mode\">Request (JSON mode)</h4>\n<blockquote>\n<p>[!IMPORTANT]\nWhen <code>format</code> is set to <code>json</code>, the output will always be a well-formed JSON object. It's important to also instruct the model to respond in JSON.</p>\n</blockquote>\n<h5 id=\"request_4\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;prompt&quot;: &quot;What color is the sky at different times of the day? Respond using JSON&quot;,\n  &quot;format&quot;: &quot;json&quot;,\n  &quot;stream&quot;: false\n}'\n</code></pre>\n\n<h5 id=\"response_4\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-11-09T21:07:55.186497Z&quot;,\n  &quot;response&quot;: &quot;{\\n\\&quot;morning\\&quot;: {\\n\\&quot;color\\&quot;: \\&quot;blue\\&quot;\\n},\\n\\&quot;noon\\&quot;: {\\n\\&quot;color\\&quot;: \\&quot;blue-gray\\&quot;\\n},\\n\\&quot;afternoon\\&quot;: {\\n\\&quot;color\\&quot;: \\&quot;warm gray\\&quot;\\n},\\n\\&quot;evening\\&quot;: {\\n\\&quot;color\\&quot;: \\&quot;orange\\&quot;\\n}\\n}\\n&quot;,\n  &quot;done&quot;: true,\n  &quot;context&quot;: [1, 2, 3],\n  &quot;total_duration&quot;: 4648158584,\n  &quot;load_duration&quot;: 4071084,\n  &quot;prompt_eval_count&quot;: 36,\n  &quot;prompt_eval_duration&quot;: 439038000,\n  &quot;eval_count&quot;: 180,\n  &quot;eval_duration&quot;: 4196918000\n}\n</code></pre>\n\n<p>The value of <code>response</code> will be a string containing JSON similar to:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;morning&quot;: {\n    &quot;color&quot;: &quot;blue&quot;\n  },\n  &quot;noon&quot;: {\n    &quot;color&quot;: &quot;blue-gray&quot;\n  },\n  &quot;afternoon&quot;: {\n    &quot;color&quot;: &quot;warm gray&quot;\n  },\n  &quot;evening&quot;: {\n    &quot;color&quot;: &quot;orange&quot;\n  }\n}\n</code></pre>\n\n<h4 id=\"request-with-images\">Request (with images)</h4>\n<p>To submit images to multimodal models such as <code>llava</code> or <code>bakllava</code>, provide a list of base64-encoded <code>images</code>:</p>\n<h4 id=\"request_5\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llava&quot;,\n  &quot;prompt&quot;:&quot;What is in this picture?&quot;,\n  &quot;stream&quot;: false,\n  &quot;images&quot;: [&quot;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&quot;]\n}'\n</code></pre>\n\n<h4 id=\"response_5\">Response</h4>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llava&quot;,\n  &quot;created_at&quot;: &quot;2023-11-03T15:36:02.583064Z&quot;,\n  &quot;response&quot;: &quot;A happy cartoon character, which is cute and cheerful.&quot;,\n  &quot;done&quot;: true,\n  &quot;context&quot;: [1, 2, 3],\n  &quot;total_duration&quot;: 2938432250,\n  &quot;load_duration&quot;: 2559292,\n  &quot;prompt_eval_count&quot;: 1,\n  &quot;prompt_eval_duration&quot;: 2195557000,\n  &quot;eval_count&quot;: 44,\n  &quot;eval_duration&quot;: 736432000\n}\n</code></pre>\n\n<h4 id=\"request-raw-mode\">Request (Raw Mode)</h4>\n<p>In some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the <code>raw</code> parameter to disable templating. Also note that raw mode will not return a context.</p>\n<h5 id=\"request_6\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;mistral&quot;,\n  &quot;prompt&quot;: &quot;[INST] why is the sky blue? [/INST]&quot;,\n  &quot;raw&quot;: true,\n  &quot;stream&quot;: false\n}'\n</code></pre>\n\n<h4 id=\"request-reproducible-outputs\">Request (Reproducible outputs)</h4>\n<p>For reproducible outputs, set <code>seed</code> to a number:</p>\n<h5 id=\"request_7\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;mistral&quot;,\n  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,\n  &quot;options&quot;: {\n    &quot;seed&quot;: 123\n  }\n}'\n</code></pre>\n\n<h5 id=\"response_6\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;mistral&quot;,\n  &quot;created_at&quot;: &quot;2023-11-03T15:36:02.583064Z&quot;,\n  &quot;response&quot;: &quot; The sky appears blue because of a phenomenon called Rayleigh scattering.&quot;,\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 8493852375,\n  &quot;load_duration&quot;: 6589624375,\n  &quot;prompt_eval_count&quot;: 14,\n  &quot;prompt_eval_duration&quot;: 119039000,\n  &quot;eval_count&quot;: 110,\n  &quot;eval_duration&quot;: 1779061000\n}\n</code></pre>\n\n<h4 id=\"generate-request-with-options\">Generate request (With options)</h4>\n<p>If you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the <code>options</code> parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.</p>\n<h5 id=\"request_8\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,\n  &quot;stream&quot;: false,\n  &quot;options&quot;: {\n    &quot;num_keep&quot;: 5,\n    &quot;seed&quot;: 42,\n    &quot;num_predict&quot;: 100,\n    &quot;top_k&quot;: 20,\n    &quot;top_p&quot;: 0.9,\n    &quot;min_p&quot;: 0.0,\n    &quot;typical_p&quot;: 0.7,\n    &quot;repeat_last_n&quot;: 33,\n    &quot;temperature&quot;: 0.8,\n    &quot;repeat_penalty&quot;: 1.2,\n    &quot;presence_penalty&quot;: 1.5,\n    &quot;frequency_penalty&quot;: 1.0,\n    &quot;penalize_newline&quot;: true,\n    &quot;stop&quot;: [&quot;\\n&quot;, &quot;user:&quot;],\n    &quot;numa&quot;: false,\n    &quot;num_ctx&quot;: 1024,\n    &quot;num_batch&quot;: 2,\n    &quot;num_gpu&quot;: 1,\n    &quot;main_gpu&quot;: 0,\n    &quot;use_mmap&quot;: true,\n    &quot;num_thread&quot;: 8\n  }\n}'\n</code></pre>\n\n<h5 id=\"response_7\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,\n  &quot;response&quot;: &quot;The sky is blue because it is the color of the sky.&quot;,\n  &quot;done&quot;: true,\n  &quot;context&quot;: [1, 2, 3],\n  &quot;total_duration&quot;: 4935886791,\n  &quot;load_duration&quot;: 534986708,\n  &quot;prompt_eval_count&quot;: 26,\n  &quot;prompt_eval_duration&quot;: 107345000,\n  &quot;eval_count&quot;: 237,\n  &quot;eval_duration&quot;: 4289432000\n}\n</code></pre>\n\n<h4 id=\"load-a-model\">Load a model</h4>\n<p>If an empty prompt is provided, the model will be loaded into memory.</p>\n<h5 id=\"request_9\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;\n}'\n</code></pre>\n\n<h5 id=\"response_8\">Response</h5>\n<p>A single JSON object is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-12-18T19:52:07.071755Z&quot;,\n  &quot;response&quot;: &quot;&quot;,\n  &quot;done&quot;: true\n}\n</code></pre>\n\n<h4 id=\"unload-a-model\">Unload a model</h4>\n<p>If an empty prompt is provided and the <code>keep_alive</code> parameter is set to <code>0</code>, a model will be unloaded from memory.</p>\n<h5 id=\"request_10\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;keep_alive&quot;: 0\n}'\n</code></pre>\n\n<h5 id=\"response_9\">Response</h5>\n<p>A single JSON object is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2024-09-12T03:54:03.516566Z&quot;,\n  &quot;response&quot;: &quot;&quot;,\n  &quot;done&quot;: true,\n  &quot;done_reason&quot;: &quot;unload&quot;\n}\n</code></pre>\n\n<h2 id=\"generate-a-chat-completion\">Generate a chat completion</h2>\n<pre class=\"codehilite\"><code>POST /api/chat\n</code></pre>\n\n<p>Generate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using <code>\"stream\": false</code>. The final response object will include statistics and additional data from the request.</p>\n<h3 id=\"parameters_1\">Parameters</h3>\n<ul>\n<li><code>model</code>: (required) the <a href=\"#model-names\">model name</a></li>\n<li><code>messages</code>: the messages of the chat, this can be used to keep a chat memory</li>\n<li><code>tools</code>: list of tools in JSON for the model to use if supported</li>\n<li><code>think</code>: (for thinking models) should the model think before responding?</li>\n</ul>\n<p>The <code>message</code> object has the following fields:</p>\n<ul>\n<li><code>role</code>: the role of the message, either <code>system</code>, <code>user</code>, <code>assistant</code>, or <code>tool</code></li>\n<li><code>content</code>: the content of the message</li>\n<li><code>thinking</code>: (for thinking models) the model's thinking process</li>\n<li><code>images</code> (optional): a list of images to include in the message (for multimodal models such as <code>llava</code>)</li>\n<li><code>tool_calls</code> (optional): a list of tools in JSON that the model wants to use</li>\n<li><code>tool_name</code> (optional): add the name of the tool that was executed to inform the model of the result</li>\n</ul>\n<p>Advanced parameters (optional):</p>\n<ul>\n<li><code>format</code>: the format to return a response in. Format can be <code>json</code> or a JSON schema.</li>\n<li><code>options</code>: additional model parameters listed in the documentation for the <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> such as <code>temperature</code></li>\n<li><code>stream</code>: if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>\n<li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li>\n</ul>\n<h3 id=\"tool-calling\">Tool calling</h3>\n<p>Tool calling is supported by providing a list of tools in the <code>tools</code> parameter. The model will generate a response that includes a list of tool calls. See the <a href=\"#chat-request-streaming-with-tools\">Chat request (Streaming with tools)</a> example below.</p>\n<p>Models can also explain the result of the tool call in the response. See the <a href=\"#chat-request-with-history-with-tools\">Chat request (With history, with tools)</a> example below.</p>\n<p><a href=\"https://ollama.com/search?c=tool\">See models with tool calling capabilities</a>.</p>\n<h3 id=\"structured-outputs_1\">Structured outputs</h3>\n<p>Structured outputs are supported by providing a JSON schema in the <code>format</code> parameter. The model will generate a response that matches the schema. See the <a href=\"#chat-request-structured-outputs\">Chat request (Structured outputs)</a> example below.</p>\n<h3 id=\"examples_1\">Examples</h3>\n<h4 id=\"chat-request-streaming\">Chat request (Streaming)</h4>\n<h5 id=\"request_11\">Request</h5>\n<p>Send a chat message with a streaming response.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;why is the sky blue?&quot;\n    }\n  ]\n}'\n</code></pre>\n\n<h5 id=\"response_10\">Response</h5>\n<p>A stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-08-04T08:52:19.385406455-07:00&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;The&quot;,\n    &quot;images&quot;: null\n  },\n  &quot;done&quot;: false\n}\n</code></pre>\n\n<p>Final response:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;&quot;\n  },\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 4883583458,\n  &quot;load_duration&quot;: 1334875,\n  &quot;prompt_eval_count&quot;: 26,\n  &quot;prompt_eval_duration&quot;: 342546000,\n  &quot;eval_count&quot;: 282,\n  &quot;eval_duration&quot;: 4535599000\n}\n</code></pre>\n\n<h4 id=\"chat-request-streaming-with-tools\">Chat request (Streaming with tools)</h4>\n<h5 id=\"request_12\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;what is the weather in tokyo?&quot;\n    }\n  ],\n  &quot;tools&quot;: [\n    {\n      &quot;type&quot;: &quot;function&quot;,\n      &quot;function&quot;: {\n        &quot;name&quot;: &quot;get_weather&quot;,\n        &quot;description&quot;: &quot;Get the weather in a given city&quot;,\n        &quot;parameters&quot;: {\n          &quot;type&quot;: &quot;object&quot;,\n          &quot;properties&quot;: {\n            &quot;city&quot;: {\n              &quot;type&quot;: &quot;string&quot;,\n              &quot;description&quot;: &quot;The city to get the weather for&quot;\n            }\n          },\n          &quot;required&quot;: [&quot;city&quot;]\n        }\n      }\n    }\n  ],\n  &quot;stream&quot;: true\n}'\n</code></pre>\n\n<h5 id=\"response_11\">Response</h5>\n<p>A stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;model&quot;: &quot;llama3.2&quot;,\n    &quot;created_at&quot;: &quot;2025-07-07T20:22:19.184789Z&quot;,\n    &quot;message&quot;: {\n        &quot;role&quot;: &quot;assistant&quot;,\n        &quot;content&quot;: &quot;&quot;,\n        &quot;tool_calls&quot;: [\n            {\n                &quot;function&quot;: {\n                    &quot;name&quot;: &quot;get_weather&quot;,\n                    &quot;arguments&quot;: {\n                        &quot;city&quot;: &quot;Tokyo&quot;\n                    }\n                },\n            }\n        ]\n    },\n    &quot;done&quot;: false\n}\n</code></pre>\n\n<p>Final response:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;:&quot;llama3.2&quot;,\n  &quot;created_at&quot;:&quot;2025-07-07T20:22:19.19314Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;&quot;\n  },\n  &quot;done_reason&quot;: &quot;stop&quot;,\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 182242375,\n  &quot;load_duration&quot;: 41295167,\n  &quot;prompt_eval_count&quot;: 169,\n  &quot;prompt_eval_duration&quot;: 24573166,\n  &quot;eval_count&quot;: 15,\n  &quot;eval_duration&quot;: 115959084\n}\n</code></pre>\n\n<h4 id=\"chat-request-no-streaming\">Chat request (No streaming)</h4>\n<h5 id=\"request_13\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;why is the sky blue?&quot;\n    }\n  ],\n  &quot;stream&quot;: false\n}'\n</code></pre>\n\n<h5 id=\"response_12\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-12-12T14:13:43.416799Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;Hello! How are you today?&quot;\n  },\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 5191566416,\n  &quot;load_duration&quot;: 2154458,\n  &quot;prompt_eval_count&quot;: 26,\n  &quot;prompt_eval_duration&quot;: 383809000,\n  &quot;eval_count&quot;: 298,\n  &quot;eval_duration&quot;: 4799921000\n}\n</code></pre>\n\n<h4 id=\"chat-request-no-streaming-with-tools\">Chat request (No streaming, with tools)</h4>\n<h5 id=\"request_14\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;what is the weather in tokyo?&quot;\n    }\n  ],\n  &quot;tools&quot;: [\n    {\n      &quot;type&quot;: &quot;function&quot;,\n      &quot;function&quot;: {\n        &quot;name&quot;: &quot;get_weather&quot;,\n        &quot;description&quot;: &quot;Get the weather in a given city&quot;,\n        &quot;parameters&quot;: {\n          &quot;type&quot;: &quot;object&quot;,\n          &quot;properties&quot;: {\n            &quot;city&quot;: {\n              &quot;type&quot;: &quot;string&quot;,\n              &quot;description&quot;: &quot;The city to get the weather for&quot;\n            }\n          },\n          &quot;required&quot;: [&quot;city&quot;]\n        }\n      }\n    }\n  ],\n  &quot;stream&quot;: false \n}'\n</code></pre>\n\n<h5 id=\"response_13\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2025-07-07T20:32:53.844124Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;&quot;,\n    &quot;tool_calls&quot;: [\n      {\n        &quot;function&quot;: {\n          &quot;name&quot;: &quot;get_weather&quot;,\n          &quot;arguments&quot;: {\n            &quot;city&quot;: &quot;Tokyo&quot;\n          }\n        },\n      }\n    ]\n  },\n  &quot;done_reason&quot;: &quot;stop&quot;,\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 3244883583,\n  &quot;load_duration&quot;: 2969184542,\n  &quot;prompt_eval_count&quot;: 169,\n  &quot;prompt_eval_duration&quot;: 141656333,\n  &quot;eval_count&quot;: 18,\n  &quot;eval_duration&quot;: 133293625\n}\n</code></pre>\n\n<h4 id=\"chat-request-structured-outputs\">Chat request (Structured outputs)</h4>\n<h5 id=\"request_15\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -X POST http://localhost:11434/api/chat -H &quot;Content-Type: application/json&quot; -d '{\n  &quot;model&quot;: &quot;llama3.1&quot;,\n  &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability.&quot;}],\n  &quot;stream&quot;: false,\n  &quot;format&quot;: {\n    &quot;type&quot;: &quot;object&quot;,\n    &quot;properties&quot;: {\n      &quot;age&quot;: {\n        &quot;type&quot;: &quot;integer&quot;\n      },\n      &quot;available&quot;: {\n        &quot;type&quot;: &quot;boolean&quot;\n      }\n    },\n    &quot;required&quot;: [\n      &quot;age&quot;,\n      &quot;available&quot;\n    ]\n  },\n  &quot;options&quot;: {\n    &quot;temperature&quot;: 0\n  }\n}'\n</code></pre>\n\n<h5 id=\"response_14\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.1&quot;,\n  &quot;created_at&quot;: &quot;2024-12-06T00:46:58.265747Z&quot;,\n  &quot;message&quot;: { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;{\\&quot;age\\&quot;: 22, \\&quot;available\\&quot;: false}&quot; },\n  &quot;done_reason&quot;: &quot;stop&quot;,\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 2254970291,\n  &quot;load_duration&quot;: 574751416,\n  &quot;prompt_eval_count&quot;: 34,\n  &quot;prompt_eval_duration&quot;: 1502000000,\n  &quot;eval_count&quot;: 12,\n  &quot;eval_duration&quot;: 175000000\n}\n</code></pre>\n\n<h4 id=\"chat-request-with-history\">Chat request (With History)</h4>\n<p>Send a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.</p>\n<h5 id=\"request_16\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;why is the sky blue?&quot;\n    },\n    {\n      &quot;role&quot;: &quot;assistant&quot;,\n      &quot;content&quot;: &quot;due to rayleigh scattering.&quot;\n    },\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;how is that different than mie scattering?&quot;\n    }\n  ]\n}'\n</code></pre>\n\n<h5 id=\"response_15\">Response</h5>\n<p>A stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-08-04T08:52:19.385406455-07:00&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;The&quot;\n  },\n  &quot;done&quot;: false\n}\n</code></pre>\n\n<p>Final response:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-08-04T19:22:45.499127Z&quot;,\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 8113331500,\n  &quot;load_duration&quot;: 6396458,\n  &quot;prompt_eval_count&quot;: 61,\n  &quot;prompt_eval_duration&quot;: 398801000,\n  &quot;eval_count&quot;: 468,\n  &quot;eval_duration&quot;: 7701267000\n}\n</code></pre>\n\n<h4 id=\"chat-request-with-history-with-tools\">Chat request (With history, with tools)</h4>\n<h5 id=\"request_17\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;what is the weather in Toronto?&quot;\n    },\n    // the message from the model appended to history\n    {\n      &quot;role&quot;: &quot;assistant&quot;,\n      &quot;content&quot;: &quot;&quot;,\n      &quot;tool_calls&quot;: [\n        {\n          &quot;function&quot;: {\n            &quot;name&quot;: &quot;get_temperature&quot;,\n            &quot;arguments&quot;: {\n              &quot;city&quot;: &quot;Toronto&quot;\n            }\n          },\n        }\n      ]\n    },\n    // the tool call result appended to history\n    {\n      &quot;role&quot;: &quot;tool&quot;,\n      &quot;content&quot;: &quot;11 degrees celsius&quot;,\n      &quot;tool_name&quot;: &quot;get_temperature&quot;,\n    }\n  ],\n  &quot;stream&quot;: false,\n  &quot;tools&quot;: [\n    {\n      &quot;type&quot;: &quot;function&quot;,\n      &quot;function&quot;: {\n        &quot;name&quot;: &quot;get_weather&quot;,\n        &quot;description&quot;: &quot;Get the weather in a given city&quot;,\n        &quot;parameters&quot;: {\n          &quot;type&quot;: &quot;object&quot;,\n          &quot;properties&quot;: {\n            &quot;city&quot;: {\n              &quot;type&quot;: &quot;string&quot;,\n              &quot;description&quot;: &quot;The city to get the weather for&quot;\n            }\n          },\n          &quot;required&quot;: [&quot;city&quot;]\n        }\n      }\n    }\n  ]\n}'\n</code></pre>\n\n<h5 id=\"response_16\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2025-07-07T20:43:37.688511Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;The current temperature in Toronto is 11Â°C.&quot;\n  },\n  &quot;done_reason&quot;: &quot;stop&quot;,\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 890771750,\n  &quot;load_duration&quot;: 707634750,\n  &quot;prompt_eval_count&quot;: 94,\n  &quot;prompt_eval_duration&quot;: 91703208,\n  &quot;eval_count&quot;: 11,\n  &quot;eval_duration&quot;: 90282125\n}\n</code></pre>\n\n<h4 id=\"chat-request-with-images\">Chat request (with images)</h4>\n<h5 id=\"request_18\">Request</h5>\n<p>Send a chat message with images. The images should be provided as an array, with the individual images encoded in Base64.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llava&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;what is in this image?&quot;,\n      &quot;images&quot;: [&quot;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&quot;]\n    }\n  ]\n}'\n</code></pre>\n\n<h5 id=\"response_17\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llava&quot;,\n  &quot;created_at&quot;: &quot;2023-12-13T22:42:50.203334Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot; The image features a cute, little pig with an angry facial expression. It's wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.&quot;,\n    &quot;images&quot;: null\n  },\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 1668506709,\n  &quot;load_duration&quot;: 1986209,\n  &quot;prompt_eval_count&quot;: 26,\n  &quot;prompt_eval_duration&quot;: 359682000,\n  &quot;eval_count&quot;: 83,\n  &quot;eval_duration&quot;: 1303285000\n}\n</code></pre>\n\n<h4 id=\"chat-request-reproducible-outputs\">Chat request (Reproducible outputs)</h4>\n<h5 id=\"request_19\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;Hello!&quot;\n    }\n  ],\n  &quot;options&quot;: {\n    &quot;seed&quot;: 101,\n    &quot;temperature&quot;: 0\n  }\n}'\n</code></pre>\n\n<h5 id=\"response_18\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2023-12-12T14:13:43.416799Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;Hello! How are you today?&quot;\n  },\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 5191566416,\n  &quot;load_duration&quot;: 2154458,\n  &quot;prompt_eval_count&quot;: 26,\n  &quot;prompt_eval_duration&quot;: 383809000,\n  &quot;eval_count&quot;: 298,\n  &quot;eval_duration&quot;: 4799921000\n}\n</code></pre>\n\n<h4 id=\"chat-request-with-tools\">Chat request (with tools)</h4>\n<h5 id=\"request_20\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [\n    {\n      &quot;role&quot;: &quot;user&quot;,\n      &quot;content&quot;: &quot;What is the weather today in Paris?&quot;\n    }\n  ],\n  &quot;stream&quot;: false,\n  &quot;tools&quot;: [\n    {\n      &quot;type&quot;: &quot;function&quot;,\n      &quot;function&quot;: {\n        &quot;name&quot;: &quot;get_current_weather&quot;,\n        &quot;description&quot;: &quot;Get the current weather for a location&quot;,\n        &quot;parameters&quot;: {\n          &quot;type&quot;: &quot;object&quot;,\n          &quot;properties&quot;: {\n            &quot;location&quot;: {\n              &quot;type&quot;: &quot;string&quot;,\n              &quot;description&quot;: &quot;The location to get the weather for, e.g. San Francisco, CA&quot;\n            },\n            &quot;format&quot;: {\n              &quot;type&quot;: &quot;string&quot;,\n              &quot;description&quot;: &quot;The format to return the weather in, e.g. 'celsius' or 'fahrenheit'&quot;,\n              &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;]\n            }\n          },\n          &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;]\n        }\n      }\n    }\n  ]\n}'\n</code></pre>\n\n<h5 id=\"response_19\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;: &quot;2024-07-22T20:33:28.123648Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;&quot;,\n    &quot;tool_calls&quot;: [\n      {\n        &quot;function&quot;: {\n          &quot;name&quot;: &quot;get_current_weather&quot;,\n          &quot;arguments&quot;: {\n            &quot;format&quot;: &quot;celsius&quot;,\n            &quot;location&quot;: &quot;Paris, FR&quot;\n          }\n        }\n      }\n    ]\n  },\n  &quot;done_reason&quot;: &quot;stop&quot;,\n  &quot;done&quot;: true,\n  &quot;total_duration&quot;: 885095291,\n  &quot;load_duration&quot;: 3753500,\n  &quot;prompt_eval_count&quot;: 122,\n  &quot;prompt_eval_duration&quot;: 328493000,\n  &quot;eval_count&quot;: 33,\n  &quot;eval_duration&quot;: 552222000\n}\n</code></pre>\n\n<h4 id=\"load-a-model_1\">Load a model</h4>\n<p>If the messages array is empty, the model will be loaded into memory.</p>\n<h5 id=\"request_21\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: []\n}'\n</code></pre>\n\n<h5 id=\"response_20\">Response</h5>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;:&quot;2024-09-12T21:17:29.110811Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;&quot;\n  },\n  &quot;done_reason&quot;: &quot;load&quot;,\n  &quot;done&quot;: true\n}\n</code></pre>\n\n<h4 id=\"unload-a-model_1\">Unload a model</h4>\n<p>If the messages array is empty and the <code>keep_alive</code> parameter is set to <code>0</code>, a model will be unloaded from memory.</p>\n<h5 id=\"request_22\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;messages&quot;: [],\n  &quot;keep_alive&quot;: 0\n}'\n</code></pre>\n\n<h5 id=\"response_21\">Response</h5>\n<p>A single JSON object is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;created_at&quot;:&quot;2024-09-12T21:33:17.547535Z&quot;,\n  &quot;message&quot;: {\n    &quot;role&quot;: &quot;assistant&quot;,\n    &quot;content&quot;: &quot;&quot;\n  },\n  &quot;done_reason&quot;: &quot;unload&quot;,\n  &quot;done&quot;: true\n}\n</code></pre>\n\n<h2 id=\"create-a-model\">Create a Model</h2>\n<pre class=\"codehilite\"><code>POST /api/create\n</code></pre>\n\n<p>Create a model from:\n * another model;\n * a safetensors directory; or\n * a GGUF file.</p>\n<p>If you are creating a model from a safetensors directory or from a GGUF file, you must <a href=\"#create-a-blob\">create a blob</a> for each of the files and then use the file name and SHA256 digest associated with each blob in the <code>files</code> field.</p>\n<h3 id=\"parameters_2\">Parameters</h3>\n<ul>\n<li><code>model</code>: name of the model to create</li>\n<li><code>from</code>: (optional) name of an existing model to create the new model from</li>\n<li><code>files</code>: (optional) a dictionary of file names to SHA256 digests of blobs to create the model from</li>\n<li><code>adapters</code>: (optional) a dictionary of file names to SHA256 digests of blobs for LORA adapters</li>\n<li><code>template</code>: (optional) the prompt template for the model</li>\n<li><code>license</code>: (optional) a string or list of strings containing the license or licenses for the model</li>\n<li><code>system</code>: (optional) a string containing the system prompt for the model</li>\n<li><code>parameters</code>: (optional) a dictionary of parameters for the model (see <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> for a list of parameters)</li>\n<li><code>messages</code>: (optional) a list of message objects used to create a conversation</li>\n<li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>\n<li><code>quantize</code> (optional): quantize a non-quantized (e.g. float16) model</li>\n</ul>\n<h4 id=\"quantization-types\">Quantization types</h4>\n<table>\n<thead>\n<tr>\n<th>Type</th>\n<th style=\"text-align: center;\">Recommended</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>q4_K_M</td>\n<td style=\"text-align: center;\">*</td>\n</tr>\n<tr>\n<td>q4_K_S</td>\n<td style=\"text-align: center;\"></td>\n</tr>\n<tr>\n<td>q8_0</td>\n<td style=\"text-align: center;\">*</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"examples_2\">Examples</h3>\n<h4 id=\"create-a-new-model\">Create a new model</h4>\n<p>Create a new model from an existing model.</p>\n<h5 id=\"request_23\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/create -d '{\n  &quot;model&quot;: &quot;mario&quot;,\n  &quot;from&quot;: &quot;llama3.2&quot;,\n  &quot;system&quot;: &quot;You are Mario from Super Mario Bros.&quot;\n}'\n</code></pre>\n\n<h5 id=\"response_22\">Response</h5>\n<p>A stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{&quot;status&quot;:&quot;reading model metadata&quot;}\n{&quot;status&quot;:&quot;creating system layer&quot;}\n{&quot;status&quot;:&quot;using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2&quot;}\n{&quot;status&quot;:&quot;using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b&quot;}\n{&quot;status&quot;:&quot;using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d&quot;}\n{&quot;status&quot;:&quot;using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988&quot;}\n{&quot;status&quot;:&quot;using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9&quot;}\n{&quot;status&quot;:&quot;writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42&quot;}\n{&quot;status&quot;:&quot;writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690&quot;}\n{&quot;status&quot;:&quot;writing manifest&quot;}\n{&quot;status&quot;:&quot;success&quot;}\n</code></pre>\n\n<h4 id=\"quantize-a-model\">Quantize a model</h4>\n<p>Quantize a non-quantized model.</p>\n<h5 id=\"request_24\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/create -d '{\n  &quot;model&quot;: &quot;llama3.2:quantized&quot;,\n  &quot;from&quot;: &quot;llama3.2:3b-instruct-fp16&quot;,\n  &quot;quantize&quot;: &quot;q4_K_M&quot;\n}'\n</code></pre>\n\n<h5 id=\"response_23\">Response</h5>\n<p>A stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{&quot;status&quot;:&quot;quantizing F16 model to Q4_K_M&quot;,&quot;digest&quot;:&quot;0&quot;,&quot;total&quot;:6433687776,&quot;completed&quot;:12302}\n{&quot;status&quot;:&quot;quantizing F16 model to Q4_K_M&quot;,&quot;digest&quot;:&quot;0&quot;,&quot;total&quot;:6433687776,&quot;completed&quot;:6433687552}\n{&quot;status&quot;:&quot;verifying conversion&quot;}\n{&quot;status&quot;:&quot;creating new layer sha256:fb7f4f211b89c6c4928ff4ddb73db9f9c0cfca3e000c3e40d6cf27ddc6ca72eb&quot;}\n{&quot;status&quot;:&quot;using existing layer sha256:966de95ca8a62200913e3f8bfbf84c8494536f1b94b49166851e76644e966396&quot;}\n{&quot;status&quot;:&quot;using existing layer sha256:fcc5a6bec9daf9b561a68827b67ab6088e1dba9d1fa2a50d7bbcc8384e0a265d&quot;}\n{&quot;status&quot;:&quot;using existing layer sha256:a70ff7e570d97baaf4e62ac6e6ad9975e04caa6d900d3742d37698494479e0cd&quot;}\n{&quot;status&quot;:&quot;using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb&quot;}\n{&quot;status&quot;:&quot;writing manifest&quot;}\n{&quot;status&quot;:&quot;success&quot;}\n</code></pre>\n\n<h4 id=\"create-a-model-from-gguf\">Create a model from GGUF</h4>\n<p>Create a model from a GGUF file. The <code>files</code> parameter should be filled out with the file name and SHA256 digest of the GGUF file you wish to use. Use <a href=\"#push-a-blob\">/api/blobs/:digest</a> to push the GGUF file to the server before calling this API.</p>\n<h5 id=\"request_25\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/create -d '{\n  &quot;model&quot;: &quot;my-gguf-model&quot;,\n  &quot;files&quot;: {\n    &quot;test.gguf&quot;: &quot;sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3&quot;\n  }\n}'\n</code></pre>\n\n<h5 id=\"response_24\">Response</h5>\n<p>A stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{&quot;status&quot;:&quot;parsing GGUF&quot;}\n{&quot;status&quot;:&quot;using existing layer sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3&quot;}\n{&quot;status&quot;:&quot;writing manifest&quot;}\n{&quot;status&quot;:&quot;success&quot;}\n</code></pre>\n\n<h4 id=\"create-a-model-from-a-safetensors-directory\">Create a model from a Safetensors directory</h4>\n<p>The <code>files</code> parameter should include a dictionary of files for the safetensors model which includes the file names and SHA256 digest of each file. Use <a href=\"#push-a-blob\">/api/blobs/:digest</a> to first push each of the files to the server before calling this API. Files will remain in the cache until the Ollama server is restarted.</p>\n<h5 id=\"request_26\">Request</h5>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/create -d '{\n  &quot;model&quot;: &quot;fred&quot;,\n  &quot;files&quot;: {\n    &quot;config.json&quot;: &quot;sha256:dd3443e529fb2290423a0c65c2d633e67b419d273f170259e27297219828e389&quot;,\n    &quot;generation_config.json&quot;: &quot;sha256:88effbb63300dbbc7390143fbbdd9d9fa50587b37e8bfd16c8c90d4970a74a36&quot;,\n    &quot;special_tokens_map.json&quot;: &quot;sha256:b7455f0e8f00539108837bfa586c4fbf424e31f8717819a6798be74bef813d05&quot;,\n    &quot;tokenizer.json&quot;: &quot;sha256:bbc1904d35169c542dffbe1f7589a5994ec7426d9e5b609d07bab876f32e97ab&quot;,\n    &quot;tokenizer_config.json&quot;: &quot;sha256:24e8a6dc2547164b7002e3125f10b415105644fcf02bf9ad8b674c87b1eaaed6&quot;,\n    &quot;model.safetensors&quot;: &quot;sha256:1ff795ff6a07e6a68085d206fb84417da2f083f68391c2843cd2b8ac6df8538f&quot;\n  }\n}'\n</code></pre>\n\n<h5 id=\"response_25\">Response</h5>\n<p>A stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">{&quot;status&quot;:&quot;converting model&quot;}\n{&quot;status&quot;:&quot;creating new layer sha256:05ca5b813af4a53d2c2922933936e398958855c44ee534858fcfd830940618b6&quot;}\n{&quot;status&quot;:&quot;using autodetected template llama3-instruct&quot;}\n{&quot;status&quot;:&quot;using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb&quot;}\n{&quot;status&quot;:&quot;writing manifest&quot;}\n{&quot;status&quot;:&quot;success&quot;}\n</code></pre>\n\n<h2 id=\"check-if-a-blob-exists\">Check if a Blob Exists</h2>\n<pre class=\"codehilite\"><code class=\"language-shell\">HEAD /api/blobs/:digest\n</code></pre>\n\n<p>Ensures that the file blob (Binary Large Object) used with create a model exists on the server. This checks your Ollama server and not ollama.com.</p>\n<h3 id=\"query-parameters\">Query Parameters</h3>\n<ul>\n<li><code>digest</code>: the SHA256 digest of the blob</li>\n</ul>\n<h3 id=\"examples_3\">Examples</h3>\n<h4 id=\"request_27\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n</code></pre>\n\n<h4 id=\"response_26\">Response</h4>\n<p>Return 200 OK if the blob exists, 404 Not Found if it does not.</p>\n<h2 id=\"push-a-blob\">Push a Blob</h2>\n<pre class=\"codehilite\"><code>POST /api/blobs/:digest\n</code></pre>\n\n<p>Push a file to the Ollama server to create a \"blob\" (Binary Large Object).</p>\n<h3 id=\"query-parameters_1\">Query Parameters</h3>\n<ul>\n<li><code>digest</code>: the expected SHA256 digest of the file</li>\n</ul>\n<h3 id=\"examples_4\">Examples</h3>\n<h4 id=\"request_28\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -T model.gguf -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n</code></pre>\n\n<h4 id=\"response_27\">Response</h4>\n<p>Return 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.</p>\n<h2 id=\"list-local-models\">List Local Models</h2>\n<pre class=\"codehilite\"><code>GET /api/tags\n</code></pre>\n\n<p>List models that are available locally.</p>\n<h3 id=\"examples_5\">Examples</h3>\n<h4 id=\"request_29\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/tags\n</code></pre>\n\n<h4 id=\"response_28\">Response</h4>\n<p>A single JSON object will be returned.</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;models&quot;: [\n    {\n      &quot;name&quot;: &quot;deepseek-r1:latest&quot;,\n      &quot;model&quot;: &quot;deepseek-r1:latest&quot;,\n      &quot;modified_at&quot;: &quot;2025-05-10T08:06:48.639712648-07:00&quot;,\n      &quot;size&quot;: 4683075271,\n      &quot;digest&quot;: &quot;0a8c266910232fd3291e71e5ba1e058cc5af9d411192cf88b6d30e92b6e73163&quot;,\n      &quot;details&quot;: {\n        &quot;parent_model&quot;: &quot;&quot;,\n        &quot;format&quot;: &quot;gguf&quot;,\n        &quot;family&quot;: &quot;qwen2&quot;,\n        &quot;families&quot;: [\n          &quot;qwen2&quot;\n        ],\n        &quot;parameter_size&quot;: &quot;7.6B&quot;,\n        &quot;quantization_level&quot;: &quot;Q4_K_M&quot;\n      }\n    },\n    {\n      &quot;name&quot;: &quot;llama3.2:latest&quot;,\n      &quot;model&quot;: &quot;llama3.2:latest&quot;,\n      &quot;modified_at&quot;: &quot;2025-05-04T17:37:44.706015396-07:00&quot;,\n      &quot;size&quot;: 2019393189,\n      &quot;digest&quot;: &quot;a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72&quot;,\n      &quot;details&quot;: {\n        &quot;parent_model&quot;: &quot;&quot;,\n        &quot;format&quot;: &quot;gguf&quot;,\n        &quot;family&quot;: &quot;llama&quot;,\n        &quot;families&quot;: [\n          &quot;llama&quot;\n        ],\n        &quot;parameter_size&quot;: &quot;3.2B&quot;,\n        &quot;quantization_level&quot;: &quot;Q4_K_M&quot;\n      }\n    }\n  ]\n}\n</code></pre>\n\n<h2 id=\"show-model-information\">Show Model Information</h2>\n<pre class=\"codehilite\"><code>POST /api/show\n</code></pre>\n\n<p>Show information about a model including details, modelfile, template, parameters, license, system prompt.</p>\n<h3 id=\"parameters_3\">Parameters</h3>\n<ul>\n<li><code>model</code>: name of the model to show</li>\n<li><code>verbose</code>: (optional) if set to <code>true</code>, returns full data for verbose response fields</li>\n</ul>\n<h3 id=\"examples_6\">Examples</h3>\n<h4 id=\"request_30\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/show -d '{\n  &quot;model&quot;: &quot;llava&quot;\n}'\n</code></pre>\n\n<h4 id=\"response_29\">Response</h4>\n<pre class=\"codehilite\"><code class=\"language-json5\">{\n  &quot;modelfile&quot;: &quot;# Modelfile generated by \\&quot;ollama show\\&quot;\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM llava:latest\\n\\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\\nTEMPLATE \\&quot;\\&quot;\\&quot;{{ .System }}\\nUSER: {{ .Prompt }}\\nASSISTANT: \\&quot;\\&quot;\\&quot;\\nPARAMETER num_ctx 4096\\nPARAMETER stop \\&quot;\\u003c/s\\u003e\\&quot;\\nPARAMETER stop \\&quot;USER:\\&quot;\\nPARAMETER stop \\&quot;ASSISTANT:\\&quot;&quot;,\n  &quot;parameters&quot;: &quot;num_keep                       24\\nstop                           \\&quot;&lt;|start_header_id|&gt;\\&quot;\\nstop                           \\&quot;&lt;|end_header_id|&gt;\\&quot;\\nstop                           \\&quot;&lt;|eot_id|&gt;\\&quot;&quot;,\n  &quot;template&quot;: &quot;{{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n{{ .System }}&lt;|eot_id|&gt;{{ end }}{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n{{ .Response }}&lt;|eot_id|&gt;&quot;,\n  &quot;details&quot;: {\n    &quot;parent_model&quot;: &quot;&quot;,\n    &quot;format&quot;: &quot;gguf&quot;,\n    &quot;family&quot;: &quot;llama&quot;,\n    &quot;families&quot;: [\n      &quot;llama&quot;\n    ],\n    &quot;parameter_size&quot;: &quot;8.0B&quot;,\n    &quot;quantization_level&quot;: &quot;Q4_0&quot;\n  },\n  &quot;model_info&quot;: {\n    &quot;general.architecture&quot;: &quot;llama&quot;,\n    &quot;general.file_type&quot;: 2,\n    &quot;general.parameter_count&quot;: 8030261248,\n    &quot;general.quantization_version&quot;: 2,\n    &quot;llama.attention.head_count&quot;: 32,\n    &quot;llama.attention.head_count_kv&quot;: 8,\n    &quot;llama.attention.layer_norm_rms_epsilon&quot;: 0.00001,\n    &quot;llama.block_count&quot;: 32,\n    &quot;llama.context_length&quot;: 8192,\n    &quot;llama.embedding_length&quot;: 4096,\n    &quot;llama.feed_forward_length&quot;: 14336,\n    &quot;llama.rope.dimension_count&quot;: 128,\n    &quot;llama.rope.freq_base&quot;: 500000,\n    &quot;llama.vocab_size&quot;: 128256,\n    &quot;tokenizer.ggml.bos_token_id&quot;: 128000,\n    &quot;tokenizer.ggml.eos_token_id&quot;: 128009,\n    &quot;tokenizer.ggml.merges&quot;: [],            // populates if `verbose=true`\n    &quot;tokenizer.ggml.model&quot;: &quot;gpt2&quot;,\n    &quot;tokenizer.ggml.pre&quot;: &quot;llama-bpe&quot;,\n    &quot;tokenizer.ggml.token_type&quot;: [],        // populates if `verbose=true`\n    &quot;tokenizer.ggml.tokens&quot;: []             // populates if `verbose=true`\n  },\n  &quot;capabilities&quot;: [\n    &quot;completion&quot;,\n    &quot;vision&quot;\n  ],\n}\n</code></pre>\n\n<h2 id=\"copy-a-model\">Copy a Model</h2>\n<pre class=\"codehilite\"><code>POST /api/copy\n</code></pre>\n\n<p>Copy a model. Creates a model with another name from an existing model.</p>\n<h3 id=\"examples_7\">Examples</h3>\n<h4 id=\"request_31\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/copy -d '{\n  &quot;source&quot;: &quot;llama3.2&quot;,\n  &quot;destination&quot;: &quot;llama3-backup&quot;\n}'\n</code></pre>\n\n<h4 id=\"response_30\">Response</h4>\n<p>Returns a 200 OK if successful, or a 404 Not Found if the source model doesn't exist.</p>\n<h2 id=\"delete-a-model\">Delete a Model</h2>\n<pre class=\"codehilite\"><code>DELETE /api/delete\n</code></pre>\n\n<p>Delete a model and its data.</p>\n<h3 id=\"parameters_4\">Parameters</h3>\n<ul>\n<li><code>model</code>: model name to delete</li>\n</ul>\n<h3 id=\"examples_8\">Examples</h3>\n<h4 id=\"request_32\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -X DELETE http://localhost:11434/api/delete -d '{\n  &quot;model&quot;: &quot;llama3:13b&quot;\n}'\n</code></pre>\n\n<h4 id=\"response_31\">Response</h4>\n<p>Returns a 200 OK if successful, 404 Not Found if the model to be deleted doesn't exist.</p>\n<h2 id=\"pull-a-model\">Pull a Model</h2>\n<pre class=\"codehilite\"><code>POST /api/pull\n</code></pre>\n\n<p>Download a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.</p>\n<h3 id=\"parameters_5\">Parameters</h3>\n<ul>\n<li><code>model</code>: name of the model to pull</li>\n<li><code>insecure</code>: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development.</li>\n<li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>\n</ul>\n<h3 id=\"examples_9\">Examples</h3>\n<h4 id=\"request_33\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/pull -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;\n}'\n</code></pre>\n\n<h4 id=\"response_32\">Response</h4>\n<p>If <code>stream</code> is not specified, or set to <code>true</code>, a stream of JSON objects is returned:</p>\n<p>The first object is the manifest:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;status&quot;: &quot;pulling manifest&quot;\n}\n</code></pre>\n\n<p>Then there is a series of downloading responses. Until any of the download is completed, the <code>completed</code> key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;status&quot;: &quot;downloading digestname&quot;,\n  &quot;digest&quot;: &quot;digestname&quot;,\n  &quot;total&quot;: 2142590208,\n  &quot;completed&quot;: 241970\n}\n</code></pre>\n\n<p>After all the files are downloaded, the final responses are:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n    &quot;status&quot;: &quot;verifying sha256 digest&quot;\n}\n{\n    &quot;status&quot;: &quot;writing manifest&quot;\n}\n{\n    &quot;status&quot;: &quot;removing any unused layers&quot;\n}\n{\n    &quot;status&quot;: &quot;success&quot;\n}\n</code></pre>\n\n<p>if <code>stream</code> is set to false, then the response is a single JSON object:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;status&quot;: &quot;success&quot;\n}\n</code></pre>\n\n<h2 id=\"push-a-model\">Push a Model</h2>\n<pre class=\"codehilite\"><code>POST /api/push\n</code></pre>\n\n<p>Upload a model to a model library. Requires registering for ollama.ai and adding a public key first.</p>\n<h3 id=\"parameters_6\">Parameters</h3>\n<ul>\n<li><code>model</code>: name of the model to push in the form of <code>&lt;namespace&gt;/&lt;model&gt;:&lt;tag&gt;</code></li>\n<li><code>insecure</code>: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development.</li>\n<li><code>stream</code>: (optional) if <code>false</code> the response will be returned as a single response object, rather than a stream of objects</li>\n</ul>\n<h3 id=\"examples_10\">Examples</h3>\n<h4 id=\"request_34\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/push -d '{\n  &quot;model&quot;: &quot;mattw/pygmalion:latest&quot;\n}'\n</code></pre>\n\n<h4 id=\"response_33\">Response</h4>\n<p>If <code>stream</code> is not specified, or set to <code>true</code>, a stream of JSON objects is returned:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{ &quot;status&quot;: &quot;retrieving manifest&quot; }\n</code></pre>\n\n<p>and then:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;status&quot;: &quot;starting upload&quot;,\n  &quot;digest&quot;: &quot;sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab&quot;,\n  &quot;total&quot;: 1928429856\n}\n</code></pre>\n\n<p>Then there is a series of uploading responses:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;status&quot;: &quot;starting upload&quot;,\n  &quot;digest&quot;: &quot;sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab&quot;,\n  &quot;total&quot;: 1928429856\n}\n</code></pre>\n\n<p>Finally, when the upload is complete:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{&quot;status&quot;:&quot;pushing manifest&quot;}\n{&quot;status&quot;:&quot;success&quot;}\n</code></pre>\n\n<p>If <code>stream</code> is set to <code>false</code>, then the response is a single JSON object:</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{ &quot;status&quot;: &quot;success&quot; }\n</code></pre>\n\n<h2 id=\"generate-embeddings\">Generate Embeddings</h2>\n<pre class=\"codehilite\"><code>POST /api/embed\n</code></pre>\n\n<p>Generate embeddings from a model</p>\n<h3 id=\"parameters_7\">Parameters</h3>\n<ul>\n<li><code>model</code>: name of model to generate embeddings from</li>\n<li><code>input</code>: text or list of text to generate embeddings for</li>\n</ul>\n<p>Advanced parameters:</p>\n<ul>\n<li><code>truncate</code>: truncates the end of each input to fit within context length. Returns error if <code>false</code> and context length is exceeded. Defaults to <code>true</code></li>\n<li><code>options</code>: additional model parameters listed in the documentation for the <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> such as <code>temperature</code></li>\n<li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li>\n</ul>\n<h3 id=\"examples_11\">Examples</h3>\n<h4 id=\"request_35\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/embed -d '{\n  &quot;model&quot;: &quot;all-minilm&quot;,\n  &quot;input&quot;: &quot;Why is the sky blue?&quot;\n}'\n</code></pre>\n\n<h4 id=\"response_34\">Response</h4>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;all-minilm&quot;,\n  &quot;embeddings&quot;: [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ]],\n  &quot;total_duration&quot;: 14143917,\n  &quot;load_duration&quot;: 1019500,\n  &quot;prompt_eval_count&quot;: 8\n}\n</code></pre>\n\n<h4 id=\"request-multiple-input\">Request (Multiple input)</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/embed -d '{\n  &quot;model&quot;: &quot;all-minilm&quot;,\n  &quot;input&quot;: [&quot;Why is the sky blue?&quot;, &quot;Why is the grass green?&quot;]\n}'\n</code></pre>\n\n<h4 id=\"response_35\">Response</h4>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;model&quot;: &quot;all-minilm&quot;,\n  &quot;embeddings&quot;: [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ],[\n    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,\n    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481\n  ]]\n}\n</code></pre>\n\n<h2 id=\"list-running-models\">List Running Models</h2>\n<pre class=\"codehilite\"><code>GET /api/ps\n</code></pre>\n\n<p>List models that are currently loaded into memory.</p>\n<h4 id=\"examples_12\">Examples</h4>\n<h3 id=\"request_36\">Request</h3>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/ps\n</code></pre>\n\n<h4 id=\"response_36\">Response</h4>\n<p>A single JSON object will be returned.</p>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;models&quot;: [\n    {\n      &quot;name&quot;: &quot;mistral:latest&quot;,\n      &quot;model&quot;: &quot;mistral:latest&quot;,\n      &quot;size&quot;: 5137025024,\n      &quot;digest&quot;: &quot;2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8&quot;,\n      &quot;details&quot;: {\n        &quot;parent_model&quot;: &quot;&quot;,\n        &quot;format&quot;: &quot;gguf&quot;,\n        &quot;family&quot;: &quot;llama&quot;,\n        &quot;families&quot;: [\n          &quot;llama&quot;\n        ],\n        &quot;parameter_size&quot;: &quot;7.2B&quot;,\n        &quot;quantization_level&quot;: &quot;Q4_0&quot;\n      },\n      &quot;expires_at&quot;: &quot;2024-06-04T14:38:31.83753-07:00&quot;,\n      &quot;size_vram&quot;: 5137025024\n    }\n  ]\n}\n</code></pre>\n\n<h2 id=\"generate-embedding\">Generate Embedding</h2>\n<blockquote>\n<p>Note: this endpoint has been superseded by <code>/api/embed</code></p>\n</blockquote>\n<pre class=\"codehilite\"><code>POST /api/embeddings\n</code></pre>\n\n<p>Generate embeddings from a model</p>\n<h3 id=\"parameters_8\">Parameters</h3>\n<ul>\n<li><code>model</code>: name of model to generate embeddings from</li>\n<li><code>prompt</code>: text to generate embeddings for</li>\n</ul>\n<p>Advanced parameters:</p>\n<ul>\n<li><code>options</code>: additional model parameters listed in the documentation for the <a href=\"./modelfile.md#valid-parameters-and-values\">Modelfile</a> such as <code>temperature</code></li>\n<li><code>keep_alive</code>: controls how long the model will stay loaded into memory following the request (default: <code>5m</code>)</li>\n</ul>\n<h3 id=\"examples_13\">Examples</h3>\n<h4 id=\"request_37\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/embeddings -d '{\n  &quot;model&quot;: &quot;all-minilm&quot;,\n  &quot;prompt&quot;: &quot;Here is an article about llamas...&quot;\n}'\n</code></pre>\n\n<h4 id=\"response_37\">Response</h4>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;embedding&quot;: [\n    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,\n    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281\n  ]\n}\n</code></pre>\n\n<h2 id=\"version\">Version</h2>\n<pre class=\"codehilite\"><code>GET /api/version\n</code></pre>\n\n<p>Retrieve the Ollama version</p>\n<h3 id=\"examples_14\">Examples</h3>\n<h4 id=\"request_38\">Request</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/version\n</code></pre>\n\n<h4 id=\"response_38\">Response</h4>\n<pre class=\"codehilite\"><code class=\"language-json\">{\n  &quot;version&quot;: &quot;0.5.1&quot;\n}\n</code></pre>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# API\n\n## Endpoints\n\n- [Generate a completion](#generate-a-completion)\n- [Generate a chat completion](#generate-a-chat-completion)\n- [Create a Model](#create-a-model)\n- [List Local Models](#list-local-models)\n- [Show Model Information](#show-model-information)\n- [Copy a Model](#copy-a-model)\n- [Delete a Model](#delete-a-model)\n- [Pull a Model](#pull-a-model)\n- [Push a Model](#push-a-model)\n- [Generate Embeddings](#generate-embeddings)\n- [List Running Models](#list-running-models)\n- [Version](#version)\n\n## Conventions\n\n### Model names\n\nModel names follow a `model:tag` format, where `model` can have an optional namespace such as `example/model`. Some examples are `orca-mini:3b-q8_0` and `llama3:70b`. The tag is optional and, if not provided, will default to `latest`. The tag is used to identify a specific version.\n\n### Durations\n\nAll durations are returned in nanoseconds.\n\n### Streaming responses\n\nCertain endpoints stream responses as JSON objects. Streaming can be disabled by providing `{\"stream\": false}` for these endpoints.\n\n## Generate a completion\n\n```\nPOST /api/generate\n```\n\nGenerate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\n\n### Parameters\n\n- `model`: (required) the [model name](#model-names)\n- `prompt`: the prompt to generate a response for\n- `suffix`: the text after the model response\n- `images`: (optional) a list of base64-encoded images (for multimodal models such as `llava`)\n- `think`: (for thinking models) should the model think before responding?\n\nAdvanced parameters (optional):\n\n- `format`: the format to return a response in. Format can be `json` or a JSON schema\n- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`\n- `system`: system message to (overrides what is defined in the `Modelfile`)\n- `template`: the prompt template to use (overrides what is defined in the `Modelfile`)\n- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects\n- `raw`: if `true` no formatting will be applied to the prompt. You may choose to use the `raw` parameter if you are specifying a full templated prompt in your request to the API\n- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)\n- `context` (deprecated): the context parameter returned from a previous request to `/generate`, this can be used to keep a short conversational memory\n\n#### Structured outputs\n\nStructured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [structured outputs](#request-structured-outputs) example below.\n\n#### JSON mode\n\nEnable JSON mode by setting the `format` parameter to `json`. This will structure the response as a valid JSON object. See the JSON mode [example](#request-json-mode) below.\n\n> [!IMPORTANT]\n> It's important to instruct the model to use JSON in the `prompt`. Otherwise, the model may generate large amounts whitespace.\n\n### Examples\n\n#### Generate request (Streaming)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\"\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"response\": \"The\",\n  \"done\": false\n}\n```\n\nThe final response in the stream also includes additional data about the generation:\n\n- `total_duration`: time spent generating the response\n- `load_duration`: time spent in nanoseconds loading the model\n- `prompt_eval_count`: number of tokens in the prompt\n- `prompt_eval_duration`: time spent in nanoseconds evaluating the prompt\n- `eval_count`: number of tokens in the response\n- `eval_duration`: time in nanoseconds spent generating the response\n- `context`: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory\n- `response`: empty if the response was streamed, if not streamed, this will contain the full response\n\nTo calculate how fast the response is generated in tokens per second (token/s), divide `eval_count` / `eval_duration` * `10^9`.\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 10706818083,\n  \"load_duration\": 6338219291,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 130079000,\n  \"eval_count\": 259,\n  \"eval_duration\": 4232710000\n}\n```\n\n#### Request (No streaming)\n\n##### Request\n\nA response can be received in one reply when streaming is off.\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false\n}'\n```\n\n##### Response\n\nIf `stream` is set to `false`, the response will be a single JSON object:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"The sky is blue because it is the color of the sky.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 5043500667,\n  \"load_duration\": 5025959,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 325953000,\n  \"eval_count\": 290,\n  \"eval_duration\": 4709213000\n}\n```\n\n#### Request (with suffix)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"codellama:code\",\n  \"prompt\": \"def compute_gcd(a, b):\",\n  \"suffix\": \"    return result\",\n  \"options\": {\n    \"temperature\": 0\n  },\n  \"stream\": false\n}'\n```\n\n##### Response\n\n```json5\n{\n  \"model\": \"codellama:code\",\n  \"created_at\": \"2024-07-22T20:47:51.147561Z\",\n  \"response\": \"\\n  if a == 0:\\n    return b\\n  else:\\n    return compute_gcd(b % a, a)\\n\\ndef compute_lcm(a, b):\\n  result = (a * b) / compute_gcd(a, b)\\n\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"context\": [...],\n  \"total_duration\": 1162761250,\n  \"load_duration\": 6683708,\n  \"prompt_eval_count\": 17,\n  \"prompt_eval_duration\": 201222000,\n  \"eval_count\": 63,\n  \"eval_duration\": 953997000\n}\n```\n\n#### Request (Structured outputs)\n\n##### Request\n\n```shell\ncurl -X POST http://localhost:11434/api/generate -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1:8b\",\n  \"prompt\": \"Ollama is 22 years old and is busy saving the world. Respond using JSON\",\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"available\": {\n        \"type\": \"boolean\"\n      }\n    },\n    \"required\": [\n      \"age\",\n      \"available\"\n    ]\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.1:8b\",\n  \"created_at\": \"2024-12-06T00:48:09.983619Z\",\n  \"response\": \"{\\n  \\\"age\\\": 22,\\n  \\\"available\\\": true\\n}\",\n  \"done\": true,\n  \"done_reason\": \"stop\",\n  \"context\": [1, 2, 3],\n  \"total_duration\": 1075509083,\n  \"load_duration\": 567678166,\n  \"prompt_eval_count\": 28,\n  \"prompt_eval_duration\": 236000000,\n  \"eval_count\": 16,\n  \"eval_duration\": 269000000\n}\n```\n\n#### Request (JSON mode)\n\n> [!IMPORTANT]\n> When `format` is set to `json`, the output will always be a well-formed JSON object. It's important to also instruct the model to respond in JSON.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"What color is the sky at different times of the day? Respond using JSON\",\n  \"format\": \"json\",\n  \"stream\": false\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-11-09T21:07:55.186497Z\",\n  \"response\": \"{\\n\\\"morning\\\": {\\n\\\"color\\\": \\\"blue\\\"\\n},\\n\\\"noon\\\": {\\n\\\"color\\\": \\\"blue-gray\\\"\\n},\\n\\\"afternoon\\\": {\\n\\\"color\\\": \\\"warm gray\\\"\\n},\\n\\\"evening\\\": {\\n\\\"color\\\": \\\"orange\\\"\\n}\\n}\\n\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 4648158584,\n  \"load_duration\": 4071084,\n  \"prompt_eval_count\": 36,\n  \"prompt_eval_duration\": 439038000,\n  \"eval_count\": 180,\n  \"eval_duration\": 4196918000\n}\n```\n\nThe value of `response` will be a string containing JSON similar to:\n\n```json\n{\n  \"morning\": {\n    \"color\": \"blue\"\n  },\n  \"noon\": {\n    \"color\": \"blue-gray\"\n  },\n  \"afternoon\": {\n    \"color\": \"warm gray\"\n  },\n  \"evening\": {\n    \"color\": \"orange\"\n  }\n}\n```\n\n#### Request (with images)\n\nTo submit images to multimodal models such as `llava` or `bakllava`, provide a list of base64-encoded `images`:\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llava\",\n  \"prompt\":\"What is in this picture?\",\n  \"stream\": false,\n  \"images\": [\"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\"]\n}'\n```\n\n#### Response\n\n```json\n{\n  \"model\": \"llava\",\n  \"created_at\": \"2023-11-03T15:36:02.583064Z\",\n  \"response\": \"A happy cartoon character, which is cute and cheerful.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 2938432250,\n  \"load_duration\": 2559292,\n  \"prompt_eval_count\": 1,\n  \"prompt_eval_duration\": 2195557000,\n  \"eval_count\": 44,\n  \"eval_duration\": 736432000\n}\n```\n\n#### Request (Raw Mode)\n\nIn some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the `raw` parameter to disable templating. Also note that raw mode will not return a context.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"[INST] why is the sky blue? [/INST]\",\n  \"raw\": true,\n  \"stream\": false\n}'\n```\n\n#### Request (Reproducible outputs)\n\nFor reproducible outputs, set `seed` to a number:\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"mistral\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"seed\": 123\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"mistral\",\n  \"created_at\": \"2023-11-03T15:36:02.583064Z\",\n  \"response\": \" The sky appears blue because of a phenomenon called Rayleigh scattering.\",\n  \"done\": true,\n  \"total_duration\": 8493852375,\n  \"load_duration\": 6589624375,\n  \"prompt_eval_count\": 14,\n  \"prompt_eval_duration\": 119039000,\n  \"eval_count\": 110,\n  \"eval_duration\": 1779061000\n}\n```\n\n#### Generate request (With options)\n\nIf you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the `options` parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"stream\": false,\n  \"options\": {\n    \"num_keep\": 5,\n    \"seed\": 42,\n    \"num_predict\": 100,\n    \"top_k\": 20,\n    \"top_p\": 0.9,\n    \"min_p\": 0.0,\n    \"typical_p\": 0.7,\n    \"repeat_last_n\": 33,\n    \"temperature\": 0.8,\n    \"repeat_penalty\": 1.2,\n    \"presence_penalty\": 1.5,\n    \"frequency_penalty\": 1.0,\n    \"penalize_newline\": true,\n    \"stop\": [\"\\n\", \"user:\"],\n    \"numa\": false,\n    \"num_ctx\": 1024,\n    \"num_batch\": 2,\n    \"num_gpu\": 1,\n    \"main_gpu\": 0,\n    \"use_mmap\": true,\n    \"num_thread\": 8\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"response\": \"The sky is blue because it is the color of the sky.\",\n  \"done\": true,\n  \"context\": [1, 2, 3],\n  \"total_duration\": 4935886791,\n  \"load_duration\": 534986708,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 107345000,\n  \"eval_count\": 237,\n  \"eval_duration\": 4289432000\n}\n```\n\n#### Load a model\n\nIf an empty prompt is provided, the model will be loaded into memory.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\"\n}'\n```\n\n##### Response\n\nA single JSON object is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-18T19:52:07.071755Z\",\n  \"response\": \"\",\n  \"done\": true\n}\n```\n\n#### Unload a model\n\nIf an empty prompt is provided and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"keep_alive\": 0\n}'\n```\n\n##### Response\n\nA single JSON object is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2024-09-12T03:54:03.516566Z\",\n  \"response\": \"\",\n  \"done\": true,\n  \"done_reason\": \"unload\"\n}\n```\n\n## Generate a chat completion\n\n```\nPOST /api/chat\n```\n\nGenerate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using `\"stream\": false`. The final response object will include statistics and additional data from the request.\n\n### Parameters\n\n- `model`: (required) the [model name](#model-names)\n- `messages`: the messages of the chat, this can be used to keep a chat memory\n- `tools`: list of tools in JSON for the model to use if supported\n- `think`: (for thinking models) should the model think before responding?\n\nThe `message` object has the following fields:\n\n- `role`: the role of the message, either `system`, `user`, `assistant`, or `tool`\n- `content`: the content of the message\n- `thinking`: (for thinking models) the model's thinking process\n- `images` (optional): a list of images to include in the message (for multimodal models such as `llava`)\n- `tool_calls` (optional): a list of tools in JSON that the model wants to use\n- `tool_name` (optional): add the name of the tool that was executed to inform the model of the result\n\nAdvanced parameters (optional):\n\n- `format`: the format to return a response in. Format can be `json` or a JSON schema.\n- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`\n- `stream`: if `false` the response will be returned as a single response object, rather than a stream of objects\n- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)\n\n### Tool calling\n\nTool calling is supported by providing a list of tools in the `tools` parameter. The model will generate a response that includes a list of tool calls. See the [Chat request (Streaming with tools)](#chat-request-streaming-with-tools) example below.\n\nModels can also explain the result of the tool call in the response. See the [Chat request (With history, with tools)](#chat-request-with-history-with-tools) example below.\n\n[See models with tool calling capabilities](https://ollama.com/search?c=tool).\n\n### Structured outputs\n\nStructured outputs are supported by providing a JSON schema in the `format` parameter. The model will generate a response that matches the schema. See the [Chat request (Structured outputs)](#chat-request-structured-outputs) example below.\n\n### Examples\n\n#### Chat request (Streaming)\n\n##### Request\n\nSend a chat message with a streaming response.\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    }\n  ]\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The\",\n    \"images\": null\n  },\n  \"done\": false\n}\n```\n\nFinal response:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done\": true,\n  \"total_duration\": 4883583458,\n  \"load_duration\": 1334875,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 342546000,\n  \"eval_count\": 282,\n  \"eval_duration\": 4535599000\n}\n```\n\n#### Chat request (Streaming with tools)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is the weather in tokyo?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather in a given city\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"city\": {\n              \"type\": \"string\",\n              \"description\": \"The city to get the weather for\"\n            }\n          },\n          \"required\": [\"city\"]\n        }\n      }\n    }\n  ],\n  \"stream\": true\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n```json\n{\n    \"model\": \"llama3.2\",\n    \"created_at\": \"2025-07-07T20:22:19.184789Z\",\n    \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"\",\n        \"tool_calls\": [\n            {\n                \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": {\n                        \"city\": \"Tokyo\"\n                    }\n                },\n            }\n        ]\n    },\n    \"done\": false\n}\n```\n\nFinal response:\n\n```json\n{\n  \"model\":\"llama3.2\",\n  \"created_at\":\"2025-07-07T20:22:19.19314Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 182242375,\n  \"load_duration\": 41295167,\n  \"prompt_eval_count\": 169,\n  \"prompt_eval_duration\": 24573166,\n  \"eval_count\": 15,\n  \"eval_duration\": 115959084\n}\n```\n\n#### Chat request (No streaming)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    }\n  ],\n  \"stream\": false\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-12T14:13:43.416799Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Hello! How are you today?\"\n  },\n  \"done\": true,\n  \"total_duration\": 5191566416,\n  \"load_duration\": 2154458,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 383809000,\n  \"eval_count\": 298,\n  \"eval_duration\": 4799921000\n}\n```\n\n#### Chat request (No streaming, with tools)\n\n##### Request\n\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is the weather in tokyo?\"\n    }\n  ],\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather in a given city\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"city\": {\n              \"type\": \"string\",\n              \"description\": \"The city to get the weather for\"\n            }\n          },\n          \"required\": [\"city\"]\n        }\n      }\n    }\n  ],\n  \"stream\": false \n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2025-07-07T20:32:53.844124Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\",\n    \"tool_calls\": [\n      {\n        \"function\": {\n          \"name\": \"get_weather\",\n          \"arguments\": {\n            \"city\": \"Tokyo\"\n          }\n        },\n      }\n    ]\n  },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 3244883583,\n  \"load_duration\": 2969184542,\n  \"prompt_eval_count\": 169,\n  \"prompt_eval_duration\": 141656333,\n  \"eval_count\": 18,\n  \"eval_duration\": 133293625\n}\n```\n\n#### Chat request (Structured outputs)\n\n##### Request\n\n```shell\ncurl -X POST http://localhost:11434/api/chat -H \"Content-Type: application/json\" -d '{\n  \"model\": \"llama3.1\",\n  \"messages\": [{\"role\": \"user\", \"content\": \"Ollama is 22 years old and busy saving the world. Return a JSON object with the age and availability.\"}],\n  \"stream\": false,\n  \"format\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"age\": {\n        \"type\": \"integer\"\n      },\n      \"available\": {\n        \"type\": \"boolean\"\n      }\n    },\n    \"required\": [\n      \"age\",\n      \"available\"\n    ]\n  },\n  \"options\": {\n    \"temperature\": 0\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.1\",\n  \"created_at\": \"2024-12-06T00:46:58.265747Z\",\n  \"message\": { \"role\": \"assistant\", \"content\": \"{\\\"age\\\": 22, \\\"available\\\": false}\" },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 2254970291,\n  \"load_duration\": 574751416,\n  \"prompt_eval_count\": 34,\n  \"prompt_eval_duration\": 1502000000,\n  \"eval_count\": 12,\n  \"eval_duration\": 175000000\n}\n```\n\n#### Chat request (With History)\n\nSend a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"why is the sky blue?\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"due to rayleigh scattering.\"\n    },\n    {\n      \"role\": \"user\",\n      \"content\": \"how is that different than mie scattering?\"\n    }\n  ]\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T08:52:19.385406455-07:00\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The\"\n  },\n  \"done\": false\n}\n```\n\nFinal response:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-08-04T19:22:45.499127Z\",\n  \"done\": true,\n  \"total_duration\": 8113331500,\n  \"load_duration\": 6396458,\n  \"prompt_eval_count\": 61,\n  \"prompt_eval_duration\": 398801000,\n  \"eval_count\": 468,\n  \"eval_duration\": 7701267000\n}\n```\n\n\n#### Chat request (With history, with tools)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is the weather in Toronto?\"\n    },\n    // the message from the model appended to history\n    {\n      \"role\": \"assistant\",\n      \"content\": \"\",\n      \"tool_calls\": [\n        {\n          \"function\": {\n            \"name\": \"get_temperature\",\n            \"arguments\": {\n              \"city\": \"Toronto\"\n            }\n          },\n        }\n      ]\n    },\n    // the tool call result appended to history\n    {\n      \"role\": \"tool\",\n      \"content\": \"11 degrees celsius\",\n      \"tool_name\": \"get_temperature\",\n    }\n  ],\n  \"stream\": false,\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the weather in a given city\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"city\": {\n              \"type\": \"string\",\n              \"description\": \"The city to get the weather for\"\n            }\n          },\n          \"required\": [\"city\"]\n        }\n      }\n    }\n  ]\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2025-07-07T20:43:37.688511Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"The current temperature in Toronto is 11Â°C.\"\n  },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 890771750,\n  \"load_duration\": 707634750,\n  \"prompt_eval_count\": 94,\n  \"prompt_eval_duration\": 91703208,\n  \"eval_count\": 11,\n  \"eval_duration\": 90282125\n}\n\n```\n\n\n#### Chat request (with images)\n\n##### Request\n\nSend a chat message with images. The images should be provided as an array, with the individual images encoded in Base64.\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llava\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"what is in this image?\",\n      \"images\": [\"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\"]\n    }\n  ]\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llava\",\n  \"created_at\": \"2023-12-13T22:42:50.203334Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \" The image features a cute, little pig with an angry facial expression. It's wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.\",\n    \"images\": null\n  },\n  \"done\": true,\n  \"total_duration\": 1668506709,\n  \"load_duration\": 1986209,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 359682000,\n  \"eval_count\": 83,\n  \"eval_duration\": 1303285000\n}\n```\n\n#### Chat request (Reproducible outputs)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"Hello!\"\n    }\n  ],\n  \"options\": {\n    \"seed\": 101,\n    \"temperature\": 0\n  }\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2023-12-12T14:13:43.416799Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"Hello! How are you today?\"\n  },\n  \"done\": true,\n  \"total_duration\": 5191566416,\n  \"load_duration\": 2154458,\n  \"prompt_eval_count\": 26,\n  \"prompt_eval_duration\": 383809000,\n  \"eval_count\": 298,\n  \"eval_duration\": 4799921000\n}\n```\n\n#### Chat request (with tools)\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"What is the weather today in Paris?\"\n    }\n  ],\n  \"stream\": false,\n  \"tools\": [\n    {\n      \"type\": \"function\",\n      \"function\": {\n        \"name\": \"get_current_weather\",\n        \"description\": \"Get the current weather for a location\",\n        \"parameters\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"location\": {\n              \"type\": \"string\",\n              \"description\": \"The location to get the weather for, e.g. San Francisco, CA\"\n            },\n            \"format\": {\n              \"type\": \"string\",\n              \"description\": \"The format to return the weather in, e.g. 'celsius' or 'fahrenheit'\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n            }\n          },\n          \"required\": [\"location\", \"format\"]\n        }\n      }\n    }\n  ]\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\": \"2024-07-22T20:33:28.123648Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\",\n    \"tool_calls\": [\n      {\n        \"function\": {\n          \"name\": \"get_current_weather\",\n          \"arguments\": {\n            \"format\": \"celsius\",\n            \"location\": \"Paris, FR\"\n          }\n        }\n      }\n    ]\n  },\n  \"done_reason\": \"stop\",\n  \"done\": true,\n  \"total_duration\": 885095291,\n  \"load_duration\": 3753500,\n  \"prompt_eval_count\": 122,\n  \"prompt_eval_duration\": 328493000,\n  \"eval_count\": 33,\n  \"eval_duration\": 552222000\n}\n```\n\n#### Load a model\n\nIf the messages array is empty, the model will be loaded into memory.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": []\n}'\n```\n\n##### Response\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\":\"2024-09-12T21:17:29.110811Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done_reason\": \"load\",\n  \"done\": true\n}\n```\n\n#### Unload a model\n\nIf the messages array is empty and the `keep_alive` parameter is set to `0`, a model will be unloaded from memory.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\n  \"model\": \"llama3.2\",\n  \"messages\": [],\n  \"keep_alive\": 0\n}'\n```\n\n##### Response\n\nA single JSON object is returned:\n\n```json\n{\n  \"model\": \"llama3.2\",\n  \"created_at\":\"2024-09-12T21:33:17.547535Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"\"\n  },\n  \"done_reason\": \"unload\",\n  \"done\": true\n}\n```\n\n## Create a Model\n\n```\nPOST /api/create\n```\n\nCreate a model from:\n * another model;\n * a safetensors directory; or\n * a GGUF file.\n\nIf you are creating a model from a safetensors directory or from a GGUF file, you must [create a blob](#create-a-blob) for each of the files and then use the file name and SHA256 digest associated with each blob in the `files` field.\n\n### Parameters\n\n- `model`: name of the model to create\n- `from`: (optional) name of an existing model to create the new model from\n- `files`: (optional) a dictionary of file names to SHA256 digests of blobs to create the model from\n- `adapters`: (optional) a dictionary of file names to SHA256 digests of blobs for LORA adapters\n- `template`: (optional) the prompt template for the model\n- `license`: (optional) a string or list of strings containing the license or licenses for the model\n- `system`: (optional) a string containing the system prompt for the model\n- `parameters`: (optional) a dictionary of parameters for the model (see [Modelfile](./modelfile.md#valid-parameters-and-values) for a list of parameters)\n- `messages`: (optional) a list of message objects used to create a conversation\n- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects\n- `quantize` (optional): quantize a non-quantized (e.g. float16) model\n\n#### Quantization types\n\n| Type | Recommended |\n| --- | :-: |\n| q4_K_M | * |\n| q4_K_S | |\n| q8_0 | * |\n\n### Examples\n\n#### Create a new model\n\nCreate a new model from an existing model.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"mario\",\n  \"from\": \"llama3.2\",\n  \"system\": \"You are Mario from Super Mario Bros.\"\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\"status\":\"reading model metadata\"}\n{\"status\":\"creating system layer\"}\n{\"status\":\"using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2\"}\n{\"status\":\"using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b\"}\n{\"status\":\"using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d\"}\n{\"status\":\"using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988\"}\n{\"status\":\"using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9\"}\n{\"status\":\"writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42\"}\n{\"status\":\"writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n```\n\n#### Quantize a model\n\nQuantize a non-quantized model.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"llama3.2:quantized\",\n  \"from\": \"llama3.2:3b-instruct-fp16\",\n  \"quantize\": \"q4_K_M\"\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\"status\":\"quantizing F16 model to Q4_K_M\",\"digest\":\"0\",\"total\":6433687776,\"completed\":12302}\n{\"status\":\"quantizing F16 model to Q4_K_M\",\"digest\":\"0\",\"total\":6433687776,\"completed\":6433687552}\n{\"status\":\"verifying conversion\"}\n{\"status\":\"creating new layer sha256:fb7f4f211b89c6c4928ff4ddb73db9f9c0cfca3e000c3e40d6cf27ddc6ca72eb\"}\n{\"status\":\"using existing layer sha256:966de95ca8a62200913e3f8bfbf84c8494536f1b94b49166851e76644e966396\"}\n{\"status\":\"using existing layer sha256:fcc5a6bec9daf9b561a68827b67ab6088e1dba9d1fa2a50d7bbcc8384e0a265d\"}\n{\"status\":\"using existing layer sha256:a70ff7e570d97baaf4e62ac6e6ad9975e04caa6d900d3742d37698494479e0cd\"}\n{\"status\":\"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n```\n\n#### Create a model from GGUF\n\nCreate a model from a GGUF file. The `files` parameter should be filled out with the file name and SHA256 digest of the GGUF file you wish to use. Use [/api/blobs/:digest](#push-a-blob) to push the GGUF file to the server before calling this API.\n\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"my-gguf-model\",\n  \"files\": {\n    \"test.gguf\": \"sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3\"\n  }\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```json\n{\"status\":\"parsing GGUF\"}\n{\"status\":\"using existing layer sha256:432f310a77f4650a88d0fd59ecdd7cebed8d684bafea53cbff0473542964f0c3\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n```\n\n\n#### Create a model from a Safetensors directory\n\nThe `files` parameter should include a dictionary of files for the safetensors model which includes the file names and SHA256 digest of each file. Use [/api/blobs/:digest](#push-a-blob) to first push each of the files to the server before calling this API. Files will remain in the cache until the Ollama server is restarted.\n\n##### Request\n\n```shell\ncurl http://localhost:11434/api/create -d '{\n  \"model\": \"fred\",\n  \"files\": {\n    \"config.json\": \"sha256:dd3443e529fb2290423a0c65c2d633e67b419d273f170259e27297219828e389\",\n    \"generation_config.json\": \"sha256:88effbb63300dbbc7390143fbbdd9d9fa50587b37e8bfd16c8c90d4970a74a36\",\n    \"special_tokens_map.json\": \"sha256:b7455f0e8f00539108837bfa586c4fbf424e31f8717819a6798be74bef813d05\",\n    \"tokenizer.json\": \"sha256:bbc1904d35169c542dffbe1f7589a5994ec7426d9e5b609d07bab876f32e97ab\",\n    \"tokenizer_config.json\": \"sha256:24e8a6dc2547164b7002e3125f10b415105644fcf02bf9ad8b674c87b1eaaed6\",\n    \"model.safetensors\": \"sha256:1ff795ff6a07e6a68085d206fb84417da2f083f68391c2843cd2b8ac6df8538f\"\n  }\n}'\n```\n\n##### Response\n\nA stream of JSON objects is returned:\n\n```shell\n{\"status\":\"converting model\"}\n{\"status\":\"creating new layer sha256:05ca5b813af4a53d2c2922933936e398958855c44ee534858fcfd830940618b6\"}\n{\"status\":\"using autodetected template llama3-instruct\"}\n{\"status\":\"using existing layer sha256:56bb8bd477a519ffa694fc449c2413c6f0e1d3b1c88fa7e3c9d88d3ae49d4dcb\"}\n{\"status\":\"writing manifest\"}\n{\"status\":\"success\"}\n```\n\n## Check if a Blob Exists\n\n```shell\nHEAD /api/blobs/:digest\n```\n\nEnsures that the file blob (Binary Large Object) used with create a model exists on the server. This checks your Ollama server and not ollama.com.\n\n### Query Parameters\n\n- `digest`: the SHA256 digest of the blob\n\n### Examples\n\n#### Request\n\n```shell\ncurl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n```\n\n#### Response\n\nReturn 200 OK if the blob exists, 404 Not Found if it does not.\n\n## Push a Blob\n\n```\nPOST /api/blobs/:digest\n```\n\nPush a file to the Ollama server to create a \"blob\" (Binary Large Object).\n\n### Query Parameters\n\n- `digest`: the expected SHA256 digest of the file\n\n### Examples\n\n#### Request\n\n```shell\ncurl -T model.gguf -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2\n```\n\n#### Response\n\nReturn 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.\n\n## List Local Models\n\n```\nGET /api/tags\n```\n\nList models that are available locally.\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/tags\n```\n\n#### Response\n\nA single JSON object will be returned.\n\n```json\n{\n  \"models\": [\n    {\n      \"name\": \"deepseek-r1:latest\",\n      \"model\": \"deepseek-r1:latest\",\n      \"modified_at\": \"2025-05-10T08:06:48.639712648-07:00\",\n      \"size\": 4683075271,\n      \"digest\": \"0a8c266910232fd3291e71e5ba1e058cc5af9d411192cf88b6d30e92b6e73163\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"qwen2\",\n        \"families\": [\n          \"qwen2\"\n        ],\n        \"parameter_size\": \"7.6B\",\n        \"quantization_level\": \"Q4_K_M\"\n      }\n    },\n    {\n      \"name\": \"llama3.2:latest\",\n      \"model\": \"llama3.2:latest\",\n      \"modified_at\": \"2025-05-04T17:37:44.706015396-07:00\",\n      \"size\": 2019393189,\n      \"digest\": \"a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": [\n          \"llama\"\n        ],\n        \"parameter_size\": \"3.2B\",\n        \"quantization_level\": \"Q4_K_M\"\n      }\n    }\n  ]\n}\n```\n\n## Show Model Information\n\n```\nPOST /api/show\n```\n\nShow information about a model including details, modelfile, template, parameters, license, system prompt.\n\n### Parameters\n\n- `model`: name of the model to show\n- `verbose`: (optional) if set to `true`, returns full data for verbose response fields\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/show -d '{\n  \"model\": \"llava\"\n}'\n```\n\n#### Response\n\n```json5\n{\n  \"modelfile\": \"# Modelfile generated by \\\"ollama show\\\"\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM llava:latest\\n\\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\\nTEMPLATE \\\"\\\"\\\"{{ .System }}\\nUSER: {{ .Prompt }}\\nASSISTANT: \\\"\\\"\\\"\\nPARAMETER num_ctx 4096\\nPARAMETER stop \\\"\\u003c/s\\u003e\\\"\\nPARAMETER stop \\\"USER:\\\"\\nPARAMETER stop \\\"ASSISTANT:\\\"\",\n  \"parameters\": \"num_keep                       24\\nstop                           \\\"<|start_header_id|>\\\"\\nstop                           \\\"<|end_header_id|>\\\"\\nstop                           \\\"<|eot_id|>\\\"\",\n  \"template\": \"{{ if .System }}<|start_header_id|>system<|end_header_id|>\\n\\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\\n\\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\\n\\n{{ .Response }}<|eot_id|>\",\n  \"details\": {\n    \"parent_model\": \"\",\n    \"format\": \"gguf\",\n    \"family\": \"llama\",\n    \"families\": [\n      \"llama\"\n    ],\n    \"parameter_size\": \"8.0B\",\n    \"quantization_level\": \"Q4_0\"\n  },\n  \"model_info\": {\n    \"general.architecture\": \"llama\",\n    \"general.file_type\": 2,\n    \"general.parameter_count\": 8030261248,\n    \"general.quantization_version\": 2,\n    \"llama.attention.head_count\": 32,\n    \"llama.attention.head_count_kv\": 8,\n    \"llama.attention.layer_norm_rms_epsilon\": 0.00001,\n    \"llama.block_count\": 32,\n    \"llama.context_length\": 8192,\n    \"llama.embedding_length\": 4096,\n    \"llama.feed_forward_length\": 14336,\n    \"llama.rope.dimension_count\": 128,\n    \"llama.rope.freq_base\": 500000,\n    \"llama.vocab_size\": 128256,\n    \"tokenizer.ggml.bos_token_id\": 128000,\n    \"tokenizer.ggml.eos_token_id\": 128009,\n    \"tokenizer.ggml.merges\": [],            // populates if `verbose=true`\n    \"tokenizer.ggml.model\": \"gpt2\",\n    \"tokenizer.ggml.pre\": \"llama-bpe\",\n    \"tokenizer.ggml.token_type\": [],        // populates if `verbose=true`\n    \"tokenizer.ggml.tokens\": []             // populates if `verbose=true`\n  },\n  \"capabilities\": [\n    \"completion\",\n    \"vision\"\n  ],\n}\n```\n\n## Copy a Model\n\n```\nPOST /api/copy\n```\n\nCopy a model. Creates a model with another name from an existing model.\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/copy -d '{\n  \"source\": \"llama3.2\",\n  \"destination\": \"llama3-backup\"\n}'\n```\n\n#### Response\n\nReturns a 200 OK if successful, or a 404 Not Found if the source model doesn't exist.\n\n## Delete a Model\n\n```\nDELETE /api/delete\n```\n\nDelete a model and its data.\n\n### Parameters\n\n- `model`: model name to delete\n\n### Examples\n\n#### Request\n\n```shell\ncurl -X DELETE http://localhost:11434/api/delete -d '{\n  \"model\": \"llama3:13b\"\n}'\n```\n\n#### Response\n\nReturns a 200 OK if successful, 404 Not Found if the model to be deleted doesn't exist.\n\n## Pull a Model\n\n```\nPOST /api/pull\n```\n\nDownload a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.\n\n### Parameters\n\n- `model`: name of the model to pull\n- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development.\n- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/pull -d '{\n  \"model\": \"llama3.2\"\n}'\n```\n\n#### Response\n\nIf `stream` is not specified, or set to `true`, a stream of JSON objects is returned:\n\nThe first object is the manifest:\n\n```json\n{\n  \"status\": \"pulling manifest\"\n}\n```\n\nThen there is a series of downloading responses. Until any of the download is completed, the `completed` key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.\n\n```json\n{\n  \"status\": \"downloading digestname\",\n  \"digest\": \"digestname\",\n  \"total\": 2142590208,\n  \"completed\": 241970\n}\n```\n\nAfter all the files are downloaded, the final responses are:\n\n```json\n{\n    \"status\": \"verifying sha256 digest\"\n}\n{\n    \"status\": \"writing manifest\"\n}\n{\n    \"status\": \"removing any unused layers\"\n}\n{\n    \"status\": \"success\"\n}\n```\n\nif `stream` is set to false, then the response is a single JSON object:\n\n```json\n{\n  \"status\": \"success\"\n}\n```\n\n## Push a Model\n\n```\nPOST /api/push\n```\n\nUpload a model to a model library. Requires registering for ollama.ai and adding a public key first.\n\n### Parameters\n\n- `model`: name of the model to push in the form of `<namespace>/<model>:<tag>`\n- `insecure`: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development.\n- `stream`: (optional) if `false` the response will be returned as a single response object, rather than a stream of objects\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/push -d '{\n  \"model\": \"mattw/pygmalion:latest\"\n}'\n```\n\n#### Response\n\nIf `stream` is not specified, or set to `true`, a stream of JSON objects is returned:\n\n```json\n{ \"status\": \"retrieving manifest\" }\n```\n\nand then:\n\n```json\n{\n  \"status\": \"starting upload\",\n  \"digest\": \"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\",\n  \"total\": 1928429856\n}\n```\n\nThen there is a series of uploading responses:\n\n```json\n{\n  \"status\": \"starting upload\",\n  \"digest\": \"sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\",\n  \"total\": 1928429856\n}\n```\n\nFinally, when the upload is complete:\n\n```json\n{\"status\":\"pushing manifest\"}\n{\"status\":\"success\"}\n```\n\nIf `stream` is set to `false`, then the response is a single JSON object:\n\n```json\n{ \"status\": \"success\" }\n```\n\n## Generate Embeddings\n\n```\nPOST /api/embed\n```\n\nGenerate embeddings from a model\n\n### Parameters\n\n- `model`: name of model to generate embeddings from\n- `input`: text or list of text to generate embeddings for\n\nAdvanced parameters:\n\n- `truncate`: truncates the end of each input to fit within context length. Returns error if `false` and context length is exceeded. Defaults to `true`\n- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`\n- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/embed -d '{\n  \"model\": \"all-minilm\",\n  \"input\": \"Why is the sky blue?\"\n}'\n```\n\n#### Response\n\n```json\n{\n  \"model\": \"all-minilm\",\n  \"embeddings\": [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ]],\n  \"total_duration\": 14143917,\n  \"load_duration\": 1019500,\n  \"prompt_eval_count\": 8\n}\n```\n\n#### Request (Multiple input)\n\n```shell\ncurl http://localhost:11434/api/embed -d '{\n  \"model\": \"all-minilm\",\n  \"input\": [\"Why is the sky blue?\", \"Why is the grass green?\"]\n}'\n```\n\n#### Response\n\n```json\n{\n  \"model\": \"all-minilm\",\n  \"embeddings\": [[\n    0.010071029, -0.0017594862, 0.05007221, 0.04692972, 0.054916814,\n    0.008599704, 0.105441414, -0.025878139, 0.12958129, 0.031952348\n  ],[\n    -0.0098027075, 0.06042469, 0.025257962, -0.006364387, 0.07272725,\n    0.017194884, 0.09032035, -0.051705178, 0.09951512, 0.09072481\n  ]]\n}\n```\n\n## List Running Models\n```\nGET /api/ps\n```\n\nList models that are currently loaded into memory.\n\n#### Examples\n\n### Request\n\n```shell\ncurl http://localhost:11434/api/ps\n```\n\n#### Response\n\nA single JSON object will be returned.\n\n```json\n{\n  \"models\": [\n    {\n      \"name\": \"mistral:latest\",\n      \"model\": \"mistral:latest\",\n      \"size\": 5137025024,\n      \"digest\": \"2ae6f6dd7a3dd734790bbbf58b8909a606e0e7e97e94b7604e0aa7ae4490e6d8\",\n      \"details\": {\n        \"parent_model\": \"\",\n        \"format\": \"gguf\",\n        \"family\": \"llama\",\n        \"families\": [\n          \"llama\"\n        ],\n        \"parameter_size\": \"7.2B\",\n        \"quantization_level\": \"Q4_0\"\n      },\n      \"expires_at\": \"2024-06-04T14:38:31.83753-07:00\",\n      \"size_vram\": 5137025024\n    }\n  ]\n}\n```\n\n## Generate Embedding\n\n> Note: this endpoint has been superseded by `/api/embed`\n\n```\nPOST /api/embeddings\n```\n\nGenerate embeddings from a model\n\n### Parameters\n\n- `model`: name of model to generate embeddings from\n- `prompt`: text to generate embeddings for\n\nAdvanced parameters:\n\n- `options`: additional model parameters listed in the documentation for the [Modelfile](./modelfile.md#valid-parameters-and-values) such as `temperature`\n- `keep_alive`: controls how long the model will stay loaded into memory following the request (default: `5m`)\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/embeddings -d '{\n  \"model\": \"all-minilm\",\n  \"prompt\": \"Here is an article about llamas...\"\n}'\n```\n\n#### Response\n\n```json\n{\n  \"embedding\": [\n    0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313,\n    0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281\n  ]\n}\n```\n\n## Version\n\n```\nGET /api/version\n```\n\nRetrieve the Ollama version\n\n### Examples\n\n#### Request\n\n```shell\ncurl http://localhost:11434/api/version\n```\n\n#### Response\n\n```json\n{\n  \"version\": \"0.5.1\"\n}\n```\n\n\n"
  },
  {
    "name": "development.md",
    "relative_path": "development.md",
    "path": "docs/development.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/development.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>development.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>development.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/development.md\">docs/development.md</a></em></p>\n                <hr>\n                <h1 id=\"development\">Development</h1>\n<p>Install prerequisites:</p>\n<ul>\n<li><a href=\"https://go.dev/doc/install\">Go</a></li>\n<li>C/C++ Compiler e.g. Clang on macOS, <a href=\"https://github.com/jmeubank/tdm-gcc/releases/latest\">TDM-GCC</a> (Windows amd64) or <a href=\"https://github.com/mstorsjo/llvm-mingw\">llvm-mingw</a> (Windows arm64), GCC/Clang on Linux.</li>\n</ul>\n<p>Then build and run Ollama from the root directory of the repository:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">go run . serve\n</code></pre>\n\n<h2 id=\"macos-apple-silicon\">macOS (Apple Silicon)</h2>\n<p>macOS Apple Silicon supports Metal which is built-in to the Ollama binary. No additional steps are required.</p>\n<h2 id=\"macos-intel\">macOS (Intel)</h2>\n<p>Install prerequisites:</p>\n<ul>\n<li><a href=\"https://cmake.org/download/\">CMake</a> or <code>brew install cmake</code></li>\n</ul>\n<p>Then, configure and build the project:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">cmake -B build\ncmake --build build\n</code></pre>\n\n<p>Lastly, run Ollama:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">go run . serve\n</code></pre>\n\n<h2 id=\"windows\">Windows</h2>\n<p>Install prerequisites:</p>\n<ul>\n<li><a href=\"https://cmake.org/download/\">CMake</a></li>\n<li><a href=\"https://visualstudio.microsoft.com/downloads/\">Visual Studio 2022</a> including the Native Desktop Workload</li>\n<li>(Optional) AMD GPU support<ul>\n<li><a href=\"https://rocm.docs.amd.com/en/latest/\">ROCm</a></li>\n<li><a href=\"https://github.com/ninja-build/ninja/releases\">Ninja</a></li>\n</ul>\n</li>\n<li>(Optional) NVIDIA GPU support<ul>\n<li><a href=\"https://developer.nvidia.com/cuda-downloads?target_os=Windows&amp;target_arch=x86_64&amp;target_version=11&amp;target_type=exe_network\">CUDA SDK</a></li>\n</ul>\n</li>\n</ul>\n<p>Then, configure and build the project:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">cmake -B build\ncmake --build build --config Release\n</code></pre>\n\n<blockquote>\n<p>[!IMPORTANT]\nBuilding for ROCm requires additional flags:\n<code>cmake -B build -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\ncmake --build build --config Release</code></p>\n</blockquote>\n<p>Lastly, run Ollama:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">go run . serve\n</code></pre>\n\n<h2 id=\"windows-arm\">Windows (ARM)</h2>\n<p>Windows ARM does not support additional acceleration libraries at this time.  Do not use cmake, simply <code>go run</code> or <code>go build</code>.</p>\n<h2 id=\"linux\">Linux</h2>\n<p>Install prerequisites:</p>\n<ul>\n<li><a href=\"https://cmake.org/download/\">CMake</a> or <code>sudo apt install cmake</code> or <code>sudo dnf install cmake</code></li>\n<li>(Optional) AMD GPU support<ul>\n<li><a href=\"https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html\">ROCm</a></li>\n</ul>\n</li>\n<li>(Optional) NVIDIA GPU support<ul>\n<li><a href=\"https://developer.nvidia.com/cuda-downloads\">CUDA SDK</a></li>\n</ul>\n</li>\n</ul>\n<blockquote>\n<p>[!IMPORTANT]\nEnsure prerequisites are in <code>PATH</code> before running CMake.</p>\n</blockquote>\n<p>Then, configure and build the project:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">cmake -B build\ncmake --build build\n</code></pre>\n\n<p>Lastly, run Ollama:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">go run . serve\n</code></pre>\n\n<h2 id=\"docker\">Docker</h2>\n<pre class=\"codehilite\"><code class=\"language-shell\">docker build .\n</code></pre>\n\n<h3 id=\"rocm\">ROCm</h3>\n<pre class=\"codehilite\"><code class=\"language-shell\">docker build --build-arg FLAVOR=rocm .\n</code></pre>\n\n<h2 id=\"running-tests\">Running tests</h2>\n<p>To run tests, use <code>go test</code>:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">go test ./...\n</code></pre>\n\n<blockquote>\n<p>NOTE: In rare circumstances, you may need to change a package using the new\n\"synctest\" package in go1.24.</p>\n<p>If you do not have the \"synctest\" package enabled, you will not see build or\ntest failures resulting from your change(s), if any, locally, but CI will\nbreak.</p>\n<p>If you see failures in CI, you can either keep pushing changes to see if the\nCI build passes, or you can enable the \"synctest\" package locally to see the\nfailures before pushing.</p>\n<p>To enable the \"synctest\" package for testing, run the following command:</p>\n<p><code>shell\nGOEXPERIMENT=synctest go test ./...</code></p>\n<p>If you wish to enable synctest for all go commands, you can set the\n<code>GOEXPERIMENT</code> environment variable in your shell profile or by using:</p>\n<p><code>shell\ngo env -w GOEXPERIMENT=synctest</code></p>\n<p>Which will enable the \"synctest\" package for all go commands without needing\nto set it for all shell sessions.</p>\n<p>The synctest package is not required for production builds.</p>\n</blockquote>\n<h2 id=\"library-detection\">Library detection</h2>\n<p>Ollama looks for acceleration libraries in the following paths relative to the <code>ollama</code> executable:</p>\n<ul>\n<li><code>./lib/ollama</code> (Windows)</li>\n<li><code>../lib/ollama</code> (Linux)</li>\n<li><code>.</code> (macOS)</li>\n<li><code>build/lib/ollama</code> (for development)</li>\n</ul>\n<p>If the libraries are not found, Ollama will not run with any acceleration libraries.</p>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Development\n\nInstall prerequisites:\n\n- [Go](https://go.dev/doc/install)\n- C/C++ Compiler e.g. Clang on macOS, [TDM-GCC](https://github.com/jmeubank/tdm-gcc/releases/latest) (Windows amd64) or [llvm-mingw](https://github.com/mstorsjo/llvm-mingw) (Windows arm64), GCC/Clang on Linux.\n\nThen build and run Ollama from the root directory of the repository:\n\n```shell\ngo run . serve\n```\n\n## macOS (Apple Silicon)\n\nmacOS Apple Silicon supports Metal which is built-in to the Ollama binary. No additional steps are required.\n\n## macOS (Intel)\n\nInstall prerequisites:\n\n- [CMake](https://cmake.org/download/) or `brew install cmake`\n\nThen, configure and build the project:\n\n```shell\ncmake -B build\ncmake --build build\n```\n\nLastly, run Ollama:\n\n```shell\ngo run . serve\n```\n\n## Windows\n\nInstall prerequisites:\n\n- [CMake](https://cmake.org/download/)\n- [Visual Studio 2022](https://visualstudio.microsoft.com/downloads/) including the Native Desktop Workload\n- (Optional) AMD GPU support\n    - [ROCm](https://rocm.docs.amd.com/en/latest/)\n    - [Ninja](https://github.com/ninja-build/ninja/releases)\n- (Optional) NVIDIA GPU support\n    - [CUDA SDK](https://developer.nvidia.com/cuda-downloads?target_os=Windows&target_arch=x86_64&target_version=11&target_type=exe_network)\n\nThen, configure and build the project:\n\n```shell\ncmake -B build\ncmake --build build --config Release\n```\n\n> [!IMPORTANT]\n> Building for ROCm requires additional flags:\n> ```\n> cmake -B build -G Ninja -DCMAKE_C_COMPILER=clang -DCMAKE_CXX_COMPILER=clang++\n> cmake --build build --config Release\n> ```\n\n\nLastly, run Ollama:\n\n```shell\ngo run . serve\n```\n\n## Windows (ARM)\n\nWindows ARM does not support additional acceleration libraries at this time.  Do not use cmake, simply `go run` or `go build`.\n\n## Linux\n\nInstall prerequisites:\n\n- [CMake](https://cmake.org/download/) or `sudo apt install cmake` or `sudo dnf install cmake`\n- (Optional) AMD GPU support\n    - [ROCm](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/quick-start.html)\n- (Optional) NVIDIA GPU support\n    - [CUDA SDK](https://developer.nvidia.com/cuda-downloads)\n\n> [!IMPORTANT]\n> Ensure prerequisites are in `PATH` before running CMake.\n\n\nThen, configure and build the project:\n\n```shell\ncmake -B build\ncmake --build build\n```\n\nLastly, run Ollama:\n\n```shell\ngo run . serve\n```\n\n## Docker\n\n```shell\ndocker build .\n```\n\n### ROCm\n\n```shell\ndocker build --build-arg FLAVOR=rocm .\n```\n\n## Running tests\n\nTo run tests, use `go test`:\n\n```shell\ngo test ./...\n```\n\n> NOTE: In rare circumstances, you may need to change a package using the new\n> \"synctest\" package in go1.24.\n>\n> If you do not have the \"synctest\" package enabled, you will not see build or\n> test failures resulting from your change(s), if any, locally, but CI will\n> break.\n>\n> If you see failures in CI, you can either keep pushing changes to see if the\n> CI build passes, or you can enable the \"synctest\" package locally to see the\n> failures before pushing.\n>\n> To enable the \"synctest\" package for testing, run the following command:\n>\n> ```shell\n> GOEXPERIMENT=synctest go test ./...\n> ```\n>\n> If you wish to enable synctest for all go commands, you can set the\n> `GOEXPERIMENT` environment variable in your shell profile or by using:\n>\n> ```shell\n> go env -w GOEXPERIMENT=synctest\n> ```\n>\n> Which will enable the \"synctest\" package for all go commands without needing\n> to set it for all shell sessions.\n>\n> The synctest package is not required for production builds.\n\n## Library detection\n\nOllama looks for acceleration libraries in the following paths relative to the `ollama` executable:\n\n* `./lib/ollama` (Windows)\n* `../lib/ollama` (Linux)\n* `.` (macOS)\n* `build/lib/ollama` (for development)\n\nIf the libraries are not found, Ollama will not run with any acceleration libraries.\n"
  },
  {
    "name": "docker.md",
    "relative_path": "docker.md",
    "path": "docs/docker.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/docker.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>docker.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>docker.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/docker.md\">docs/docker.md</a></em></p>\n                <hr>\n                <h1 id=\"ollama-docker-image\">Ollama Docker image</h1>\n<h3 id=\"cpu-only\">CPU only</h3>\n<pre class=\"codehilite\"><code class=\"language-shell\">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre>\n\n<h3 id=\"nvidia-gpu\">Nvidia GPU</h3>\n<p>Install the <a href=\"https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation\">NVIDIA Container Toolkit</a>.</p>\n<h4 id=\"install-with-apt\">Install with Apt</h4>\n<ol>\n<li>\n<p>Configure the repository</p>\n<p><code>shell\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n    | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n    | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n    | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\nsudo apt-get update</code></p>\n</li>\n<li>\n<p>Install the NVIDIA Container Toolkit packages</p>\n<p><code>shell\nsudo apt-get install -y nvidia-container-toolkit</code></p>\n</li>\n</ol>\n<h4 id=\"install-with-yum-or-dnf\">Install with Yum or Dnf</h4>\n<ol>\n<li>\n<p>Configure the repository</p>\n<p><code>shell\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo \\\n    | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo</code></p>\n</li>\n<li>\n<p>Install the NVIDIA Container Toolkit packages</p>\n<p><code>shell\nsudo yum install -y nvidia-container-toolkit</code></p>\n</li>\n</ol>\n<h4 id=\"configure-docker-to-use-nvidia-driver\">Configure Docker to use Nvidia driver</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n</code></pre>\n\n<h4 id=\"start-the-container\">Start the container</h4>\n<pre class=\"codehilite\"><code class=\"language-shell\">docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n</code></pre>\n\n<blockquote>\n<p>[!NOTE]<br />\nIf you're running on an NVIDIA JetPack system, Ollama can't automatically discover the correct JetPack version. Pass the environment variable JETSON_JETPACK=5 or JETSON_JETPACK=6 to the container to select version 5 or 6.</p>\n</blockquote>\n<h3 id=\"amd-gpu\">AMD GPU</h3>\n<p>To run Ollama using Docker with AMD GPUs, use the <code>rocm</code> tag and the following command:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">docker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm\n</code></pre>\n\n<h3 id=\"run-model-locally\">Run model locally</h3>\n<p>Now you can run a model:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">docker exec -it ollama ollama run llama3.2\n</code></pre>\n\n<h3 id=\"try-different-models\">Try different models</h3>\n<p>More models can be found on the <a href=\"https://ollama.com/library\">Ollama library</a>.</p>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Ollama Docker image\n\n### CPU only\n\n```shell\ndocker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n### Nvidia GPU\nInstall the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installation).\n\n#### Install with Apt\n1.  Configure the repository\n\n    ```shell\n    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey \\\n        | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\n    curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list \\\n        | sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' \\\n        | sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n    sudo apt-get update\n    ```\n\n2.  Install the NVIDIA Container Toolkit packages\n\n    ```shell\n    sudo apt-get install -y nvidia-container-toolkit\n    ```\n\n#### Install with Yum or Dnf\n1.  Configure the repository\n\n    ```shell\n    curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo \\\n        | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo\n    ```\n\n2. Install the NVIDIA Container Toolkit packages\n\n    ```shell\n    sudo yum install -y nvidia-container-toolkit\n    ```\n\n#### Configure Docker to use Nvidia driver\n\n```shell\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n```\n\n#### Start the container\n\n```shell\ndocker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama\n```\n\n> [!NOTE]  \n> If you're running on an NVIDIA JetPack system, Ollama can't automatically discover the correct JetPack version. Pass the environment variable JETSON_JETPACK=5 or JETSON_JETPACK=6 to the container to select version 5 or 6.\n\n### AMD GPU\n\nTo run Ollama using Docker with AMD GPUs, use the `rocm` tag and the following command:\n\n```shell\ndocker run -d --device /dev/kfd --device /dev/dri -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama:rocm\n```\n\n### Run model locally\n\nNow you can run a model:\n\n```shell\ndocker exec -it ollama ollama run llama3.2\n```\n\n### Try different models\n\nMore models can be found on the [Ollama library](https://ollama.com/library).\n"
  },
  {
    "name": "examples.md",
    "relative_path": "examples.md",
    "path": "docs/examples.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/examples.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>examples.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>examples.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/examples.md\">docs/examples.md</a></em></p>\n                <hr>\n                <h1 id=\"examples\">Examples</h1>\n<p>This directory contains different examples of using Ollama.</p>\n<h2 id=\"python-examples\">Python examples</h2>\n<p>Ollama Python examples at <a href=\"https://github.com/ollama/ollama-python/tree/main/examples\">ollama-python/examples</a></p>\n<h2 id=\"javascript-examples\">JavaScript examples</h2>\n<p>Ollama JavaScript examples at <a href=\"https://github.com/ollama/ollama-js/tree/main/examples\">ollama-js/examples</a></p>\n<h2 id=\"openai-compatibility-examples\">OpenAI compatibility examples</h2>\n<p>Ollama OpenAI compatibility examples at <a href=\"../docs/openai.md\">ollama/examples/openai</a></p>\n<h2 id=\"community-examples\">Community examples</h2>\n<ul>\n<li><a href=\"https://python.langchain.com/docs/integrations/chat/ollama/\">LangChain Ollama Python</a></li>\n<li><a href=\"https://js.langchain.com/docs/integrations/chat/ollama/\">LangChain Ollama JS</a></li>\n</ul>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Examples\n\nThis directory contains different examples of using Ollama.\n\n## Python examples\nOllama Python examples at [ollama-python/examples](https://github.com/ollama/ollama-python/tree/main/examples)\n\n\n## JavaScript examples\nOllama JavaScript examples at [ollama-js/examples](https://github.com/ollama/ollama-js/tree/main/examples)\n\n\n## OpenAI compatibility examples\nOllama OpenAI compatibility examples at [ollama/examples/openai](../docs/openai.md)\n\n\n## Community examples\n\n- [LangChain Ollama Python](https://python.langchain.com/docs/integrations/chat/ollama/)\n- [LangChain Ollama JS](https://js.langchain.com/docs/integrations/chat/ollama/)\n"
  },
  {
    "name": "faq.md",
    "relative_path": "faq.md",
    "path": "docs/faq.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/faq.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>faq.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>faq.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/faq.md\">docs/faq.md</a></em></p>\n                <hr>\n                <h1 id=\"faq\">FAQ</h1>\n<h2 id=\"how-can-i-upgrade-ollama\">How can I upgrade Ollama?</h2>\n<p>Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click \"Restart to update\" to apply the update. Updates can also be installed by downloading the latest version <a href=\"https://ollama.com/download/\">manually</a>.</p>\n<p>On Linux, re-run the install script:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>\n\n<h2 id=\"how-can-i-view-the-logs\">How can I view the logs?</h2>\n<p>Review the <a href=\"./troubleshooting.md\">Troubleshooting</a> docs for more about using logs.</p>\n<h2 id=\"is-my-gpu-compatible-with-ollama\">Is my GPU compatible with Ollama?</h2>\n<p>Please refer to the <a href=\"./gpu.md\">GPU docs</a>.</p>\n<h2 id=\"how-can-i-specify-the-context-window-size\">How can I specify the context window size?</h2>\n<p>By default, Ollama uses a context window size of 4096 tokens. </p>\n<p>This can be overridden with the <code>OLLAMA_CONTEXT_LENGTH</code> environment variable. For example, to set the default context window to 8K, use: </p>\n<pre class=\"codehilite\"><code class=\"language-shell\">OLLAMA_CONTEXT_LENGTH=8192 ollama serve\n</code></pre>\n\n<p>To change this when using <code>ollama run</code>, use <code>/set parameter</code>:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">/set parameter num_ctx 4096\n</code></pre>\n\n<p>When using the API, specify the <code>num_ctx</code> parameter:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{\n  &quot;model&quot;: &quot;llama3.2&quot;,\n  &quot;prompt&quot;: &quot;Why is the sky blue?&quot;,\n  &quot;options&quot;: {\n    &quot;num_ctx&quot;: 4096\n  }\n}'\n</code></pre>\n\n<h2 id=\"how-can-i-tell-if-my-model-was-loaded-onto-the-gpu\">How can I tell if my model was loaded onto the GPU?</h2>\n<p>Use the <code>ollama ps</code> command to see what models are currently loaded into memory.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama ps\n</code></pre>\n\n<blockquote>\n<p><strong>Output</strong>:</p>\n<p><code>NAME          ID              SIZE    PROCESSOR   UNTIL\nllama3:70b    bcfb190ca3a7    42 GB   100% GPU    4 minutes from now</code></p>\n</blockquote>\n<p>The <code>Processor</code> column will show which memory the model was loaded in to:\n* <code>100% GPU</code> means the model was loaded entirely into the GPU\n* <code>100% CPU</code> means the model was loaded entirely in system memory\n* <code>48%/52% CPU/GPU</code> means the model was loaded partially onto both the GPU and into system memory</p>\n<h2 id=\"how-do-i-configure-ollama-server\">How do I configure Ollama server?</h2>\n<p>Ollama server can be configured with environment variables.</p>\n<h3 id=\"setting-environment-variables-on-mac\">Setting environment variables on Mac</h3>\n<p>If Ollama is run as a macOS application, environment variables should be set using <code>launchctl</code>:</p>\n<ol>\n<li>\n<p>For each environment variable, call <code>launchctl setenv</code>.</p>\n<p><code>bash\nlaunchctl setenv OLLAMA_HOST \"0.0.0.0:11434\"</code></p>\n</li>\n<li>\n<p>Restart Ollama application.</p>\n</li>\n</ol>\n<h3 id=\"setting-environment-variables-on-linux\">Setting environment variables on Linux</h3>\n<p>If Ollama is run as a systemd service, environment variables should be set using <code>systemctl</code>:</p>\n<ol>\n<li>\n<p>Edit the systemd service by calling <code>systemctl edit ollama.service</code>. This will open an editor.</p>\n</li>\n<li>\n<p>For each environment variable, add a line <code>Environment</code> under section <code>[Service]</code>:</p>\n<p><code>ini\n[Service]\nEnvironment=\"OLLAMA_HOST=0.0.0.0:11434\"</code></p>\n</li>\n<li>\n<p>Save and exit.</p>\n</li>\n<li>\n<p>Reload <code>systemd</code> and restart Ollama:</p>\n</li>\n</ol>\n<p><code>shell\n   systemctl daemon-reload\n   systemctl restart ollama</code></p>\n<h3 id=\"setting-environment-variables-on-windows\">Setting environment variables on Windows</h3>\n<p>On Windows, Ollama inherits your user and system environment variables.</p>\n<ol>\n<li>\n<p>First Quit Ollama by clicking on it in the task bar.</p>\n</li>\n<li>\n<p>Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for <em>environment variables</em>.</p>\n</li>\n<li>\n<p>Click on <em>Edit environment variables for your account</em>.</p>\n</li>\n<li>\n<p>Edit or create a new variable for your user account for <code>OLLAMA_HOST</code>, <code>OLLAMA_MODELS</code>, etc.</p>\n</li>\n<li>\n<p>Click OK/Apply to save.</p>\n</li>\n<li>\n<p>Start the Ollama application from the Windows Start menu.</p>\n</li>\n</ol>\n<h2 id=\"how-do-i-use-ollama-behind-a-proxy\">How do I use Ollama behind a proxy?</h2>\n<p>Ollama pulls models from the Internet and may require a proxy server to access the models. Use <code>HTTPS_PROXY</code> to redirect outbound requests through the proxy. Ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.</p>\n<blockquote>\n<p>[!NOTE]\nAvoid setting <code>HTTP_PROXY</code>. Ollama does not use HTTP for model pulls, only HTTPS. Setting <code>HTTP_PROXY</code> may interrupt client connections to the server.</p>\n</blockquote>\n<h3 id=\"how-do-i-use-ollama-behind-a-proxy-in-docker\">How do I use Ollama behind a proxy in Docker?</h3>\n<p>The Ollama Docker container image can be configured to use a proxy by passing <code>-e HTTPS_PROXY=https://proxy.example.com</code> when starting the container.</p>\n<p>Alternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on <a href=\"https://docs.docker.com/desktop/settings/mac/#proxies\">macOS</a>, <a href=\"https://docs.docker.com/desktop/settings/windows/#proxies\">Windows</a>, and <a href=\"https://docs.docker.com/desktop/settings/linux/#proxies\">Linux</a>, and Docker <a href=\"https://docs.docker.com/config/daemon/systemd/#httphttps-proxy\">daemon with systemd</a>.</p>\n<p>Ensure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.</p>\n<pre class=\"codehilite\"><code class=\"language-dockerfile\">FROM ollama/ollama\nCOPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt\nRUN update-ca-certificates\n</code></pre>\n\n<p>Build and run this image:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">docker build -t ollama-with-ca .\ndocker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca\n</code></pre>\n\n<h2 id=\"does-ollama-send-my-prompts-and-answers-back-to-ollamacom\">Does Ollama send my prompts and answers back to ollama.com?</h2>\n<p>No. Ollama runs locally, and conversation data does not leave your machine.</p>\n<h2 id=\"how-can-i-expose-ollama-on-my-network\">How can I expose Ollama on my network?</h2>\n<p>Ollama binds 127.0.0.1 port 11434 by default. Change the bind address with the <code>OLLAMA_HOST</code> environment variable.</p>\n<p>Refer to the section <a href=\"#how-do-i-configure-ollama-server\">above</a> for how to set environment variables on your platform.</p>\n<h2 id=\"how-can-i-use-ollama-with-a-proxy-server\">How can I use Ollama with a proxy server?</h2>\n<p>Ollama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:</p>\n<pre class=\"codehilite\"><code class=\"language-nginx\">server {\n    listen 80;\n    server_name example.com;  # Replace with your domain or IP\n    location / {\n        proxy_pass http://localhost:11434;\n        proxy_set_header Host localhost:11434;\n    }\n}\n</code></pre>\n\n<h2 id=\"how-can-i-use-ollama-with-ngrok\">How can I use Ollama with ngrok?</h2>\n<p>Ollama can be accessed using a range of tools for tunneling tools. For example with Ngrok:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ngrok http 11434 --host-header=&quot;localhost:11434&quot;\n</code></pre>\n\n<h2 id=\"how-can-i-use-ollama-with-cloudflare-tunnel\">How can I use Ollama with Cloudflare Tunnel?</h2>\n<p>To use Ollama with Cloudflare Tunnel, use the <code>--url</code> and <code>--http-host-header</code> flags:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">cloudflared tunnel --url http://localhost:11434 --http-host-header=&quot;localhost:11434&quot;\n</code></pre>\n\n<h2 id=\"how-can-i-allow-additional-web-origins-to-access-ollama\">How can I allow additional web origins to access Ollama?</h2>\n<p>Ollama allows cross-origin requests from <code>127.0.0.1</code> and <code>0.0.0.0</code> by default. Additional origins can be configured with <code>OLLAMA_ORIGINS</code>.</p>\n<p>For browser extensions, you'll need to explicitly allow the extension's origin pattern. Set <code>OLLAMA_ORIGINS</code> to include <code>chrome-extension://*</code>, <code>moz-extension://*</code>, and <code>safari-web-extension://*</code> if you wish to allow all browser extensions access, or specific extensions as needed:</p>\n<pre class=\"codehilite\"><code># Allow all Chrome, Firefox, and Safari extensions\nOLLAMA_ORIGINS=chrome-extension://*,moz-extension://*,safari-web-extension://* ollama serve\n</code></pre>\n\n<p>Refer to the section <a href=\"#how-do-i-configure-ollama-server\">above</a> for how to set environment variables on your platform.</p>\n<h2 id=\"where-are-models-stored\">Where are models stored?</h2>\n<ul>\n<li>macOS: <code>~/.ollama/models</code></li>\n<li>Linux: <code>/usr/share/ollama/.ollama/models</code></li>\n<li>Windows: <code>C:\\Users\\%username%\\.ollama\\models</code></li>\n</ul>\n<h3 id=\"how-do-i-set-them-to-a-different-location\">How do I set them to a different location?</h3>\n<p>If a different directory needs to be used, set the environment variable <code>OLLAMA_MODELS</code> to the chosen directory.</p>\n<blockquote>\n<p>Note: on Linux using the standard installer, the <code>ollama</code> user needs read and write access to the specified directory. To assign the directory to the <code>ollama</code> user run <code>sudo chown -R ollama:ollama &lt;directory&gt;</code>.</p>\n</blockquote>\n<p>Refer to the section <a href=\"#how-do-i-configure-ollama-server\">above</a> for how to set environment variables on your platform.</p>\n<h2 id=\"how-can-i-use-ollama-in-visual-studio-code\">How can I use Ollama in Visual Studio Code?</h2>\n<p>There is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of <a href=\"https://github.com/ollama/ollama#extensions--plugins\">extensions &amp; plugins</a> at the bottom of the main repository readme.</p>\n<h2 id=\"how-do-i-use-ollama-with-gpu-acceleration-in-docker\">How do I use Ollama with GPU acceleration in Docker?</h2>\n<p>The Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the <a href=\"https://github.com/NVIDIA/nvidia-container-toolkit\">nvidia-container-toolkit</a>. See <a href=\"https://hub.docker.com/r/ollama/ollama\">ollama/ollama</a> for more details.</p>\n<p>GPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.</p>\n<h2 id=\"why-is-networking-slow-in-wsl2-on-windows-10\">Why is networking slow in WSL2 on Windows 10?</h2>\n<p>This can impact both installing Ollama, as well as downloading models.</p>\n<p>Open <code>Control Panel &gt; Networking and Internet &gt; View network status and tasks</code> and click on <code>Change adapter settings</code> on the left panel. Find the <code>vEthernel (WSL)</code> adapter, right click and select <code>Properties</code>.\nClick on <code>Configure</code> and open the <code>Advanced</code> tab. Search through each of the properties until you find <code>Large Send Offload Version 2 (IPv4)</code> and <code>Large Send Offload Version 2 (IPv6)</code>. <em>Disable</em> both of these\nproperties.</p>\n<h2 id=\"how-can-i-preload-a-model-into-ollama-to-get-faster-response-times\">How can I preload a model into Ollama to get faster response times?</h2>\n<p>If you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the <code>/api/generate</code> and <code>/api/chat</code> API endpoints.</p>\n<p>To preload the mistral model using the generate endpoint, use:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{&quot;model&quot;: &quot;mistral&quot;}'\n</code></pre>\n\n<p>To use the chat completions endpoint, use:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/chat -d '{&quot;model&quot;: &quot;mistral&quot;}'\n</code></pre>\n\n<p>To preload a model using the CLI, use the command:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama run llama3.2 &quot;&quot;\n</code></pre>\n\n<h2 id=\"how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately\">How do I keep a model loaded in memory or make it unload immediately?</h2>\n<p>By default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you're making numerous requests to the LLM. If you want to immediately unload a model from memory, use the <code>ollama stop</code> command:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama stop llama3.2\n</code></pre>\n\n<p>If you're using the API, use the <code>keep_alive</code> parameter with the <code>/api/generate</code> and <code>/api/chat</code> endpoints to set the amount of time that a model stays in memory. The <code>keep_alive</code> parameter can be set to:\n* a duration string (such as \"10m\" or \"24h\")\n* a number in seconds (such as 3600)\n* any negative number which will keep the model loaded in memory (e.g. -1 or \"-1m\")\n* '0' which will unload the model immediately after generating a response</p>\n<p>For example, to preload a model and leave it in memory use:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{&quot;model&quot;: &quot;llama3.2&quot;, &quot;keep_alive&quot;: -1}'\n</code></pre>\n\n<p>To unload the model and free up memory use:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/api/generate -d '{&quot;model&quot;: &quot;llama3.2&quot;, &quot;keep_alive&quot;: 0}'\n</code></pre>\n\n<p>Alternatively, you can change the amount of time all models are loaded into memory by setting the <code>OLLAMA_KEEP_ALIVE</code> environment variable when starting the Ollama server. The <code>OLLAMA_KEEP_ALIVE</code> variable uses the same parameter types as the <code>keep_alive</code> parameter types mentioned above. Refer to the section explaining <a href=\"#how-do-i-configure-ollama-server\">how to configure the Ollama server</a> to correctly set the environment variable.</p>\n<p>The <code>keep_alive</code> API parameter with the <code>/api/generate</code> and <code>/api/chat</code> API endpoints will override the <code>OLLAMA_KEEP_ALIVE</code> setting.</p>\n<h2 id=\"how-do-i-manage-the-maximum-number-of-requests-the-ollama-server-can-queue\">How do I manage the maximum number of requests the Ollama server can queue?</h2>\n<p>If too many requests are sent to the server, it will respond with a 503 error indicating the server is overloaded.  You can adjust how many requests may be queue by setting <code>OLLAMA_MAX_QUEUE</code>.</p>\n<h2 id=\"how-does-ollama-handle-concurrent-requests\">How does Ollama handle concurrent requests?</h2>\n<p>Ollama supports two levels of concurrent processing.  If your system has sufficient available memory (system memory when using CPU inference, or VRAM for GPU inference) then multiple models can be loaded at the same time.  For a given model, if there is sufficient available memory when the model is loaded, it can be configured to allow parallel request processing.</p>\n<p>If there is insufficient available memory to load a new model request while one or more models are already loaded, all new requests will be queued until the new model can be loaded.  As prior models become idle, one or more will be unloaded to make room for the new model.  Queued requests will be processed in order.  When using GPU inference new models must be able to completely fit in VRAM to allow concurrent model loads.</p>\n<p>Parallel request processing for a given model results in increasing the context size by the number of parallel requests.  For example, a 2K context with 4 parallel requests will result in an 8K context and additional memory allocation.</p>\n<p>The following server settings may be used to adjust how Ollama handles concurrent requests on most platforms:</p>\n<ul>\n<li><code>OLLAMA_MAX_LOADED_MODELS</code> - The maximum number of models that can be loaded concurrently provided they fit in available memory.  The default is 3 * the number of GPUs or 3 for CPU inference.</li>\n<li><code>OLLAMA_NUM_PARALLEL</code> - The maximum number of parallel requests each model will process at the same time.  The default is 1, and will handle 1 request per model at a time.</li>\n<li><code>OLLAMA_MAX_QUEUE</code> - The maximum number of requests Ollama will queue when busy before rejecting additional requests. The default is 512</li>\n</ul>\n<p>Note: Windows with Radeon GPUs currently default to 1 model maximum due to limitations in ROCm v5.7 for available VRAM reporting.  Once ROCm v6.2 is available, Windows Radeon will follow the defaults above.  You may enable concurrent model loads on Radeon on Windows, but ensure you don't load more models than will fit into your GPUs VRAM.</p>\n<h2 id=\"how-does-ollama-load-models-on-multiple-gpus\">How does Ollama load models on multiple GPUs?</h2>\n<p>When loading a new model, Ollama evaluates the required VRAM for the model against what is currently available.  If the model will entirely fit on any single GPU, Ollama will load the model on that GPU.  This typically provides the best performance as it reduces the amount of data transferring across the PCI bus during inference.  If the model does not fit entirely on one GPU, then it will be spread across all the available GPUs.</p>\n<h2 id=\"how-can-i-enable-flash-attention\">How can I enable Flash Attention?</h2>\n<p>Flash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows.  To enable Flash Attention, set the <code>OLLAMA_FLASH_ATTENTION</code> environment variable to <code>1</code> when starting the Ollama server.</p>\n<h2 id=\"how-can-i-set-the-quantization-type-for-the-kv-cache\">How can I set the quantization type for the K/V cache?</h2>\n<p>The K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.</p>\n<p>To use quantized K/V cache with Ollama you can set the following environment variable:</p>\n<ul>\n<li><code>OLLAMA_KV_CACHE_TYPE</code> - The quantization type for the K/V cache.  Default is <code>f16</code>.</li>\n</ul>\n<blockquote>\n<p>Note: Currently this is a global option - meaning all models will run with the specified quantization type.</p>\n</blockquote>\n<p>The currently available K/V cache quantization types are:</p>\n<ul>\n<li><code>f16</code> - high precision and memory usage (default).</li>\n<li><code>q8_0</code> - 8-bit quantization, uses approximately 1/2 the memory of <code>f16</code> with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).</li>\n<li><code>q4_0</code> - 4-bit quantization, uses approximately 1/4 the memory of <code>f16</code> with a small-medium loss in precision that may be more noticeable at higher context sizes.</li>\n</ul>\n<p>How much the cache quantization impacts the model's response quality will depend on the model and the task.  Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.</p>\n<p>You may need to experiment with different quantization types to find the best balance between memory usage and quality.</p>\n<h2 id=\"how-can-i-stop-ollama-from-starting-when-i-login-to-my-computer\">How can I stop Ollama from starting when I login to my computer</h2>\n<p>Ollama for Windows and macOS register as a login item during installation.  You can disable this if you prefer not to have Ollama automatically start.  Ollama will respect this setting across upgrades, unless you uninstall the application.</p>\n<p><strong>Windows</strong>\n- Remove <code>%APPDATA%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\Ollama.lnk</code></p>\n<p><strong>MacOS Monterey (v12)</strong>\n- Open <code>Settings</code> -&gt; <code>Users &amp; Groups</code> -&gt; <code>Login Items</code> and find the <code>Ollama</code> entry, then click the <code>-</code> (minus) to remove</p>\n<p><strong>MacOS Ventura (v13) and later</strong>\n- Open <code>Settings</code> and search for \"Login Items\", find the <code>Ollama</code> entry under \"Allow in the Background`, then click the slider to disable.</p>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# FAQ\n\n## How can I upgrade Ollama?\n\nOllama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click \"Restart to update\" to apply the update. Updates can also be installed by downloading the latest version [manually](https://ollama.com/download/).\n\nOn Linux, re-run the install script:\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n## How can I view the logs?\n\nReview the [Troubleshooting](./troubleshooting.md) docs for more about using logs.\n\n## Is my GPU compatible with Ollama?\n\nPlease refer to the [GPU docs](./gpu.md).\n\n## How can I specify the context window size?\n\nBy default, Ollama uses a context window size of 4096 tokens. \n\nThis can be overridden with the `OLLAMA_CONTEXT_LENGTH` environment variable. For example, to set the default context window to 8K, use: \n\n```shell\nOLLAMA_CONTEXT_LENGTH=8192 ollama serve\n```\n\nTo change this when using `ollama run`, use `/set parameter`:\n\n```shell\n/set parameter num_ctx 4096\n```\n\nWhen using the API, specify the `num_ctx` parameter:\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\n  \"model\": \"llama3.2\",\n  \"prompt\": \"Why is the sky blue?\",\n  \"options\": {\n    \"num_ctx\": 4096\n  }\n}'\n```\n\n## How can I tell if my model was loaded onto the GPU?\n\nUse the `ollama ps` command to see what models are currently loaded into memory.\n\n```shell\nollama ps\n```\n\n> **Output**:\n>\n> ```\n> NAME      \tID          \tSIZE \tPROCESSOR\tUNTIL\n> llama3:70b\tbcfb190ca3a7\t42 GB\t100% GPU \t4 minutes from now\n> ```\n\nThe `Processor` column will show which memory the model was loaded in to:\n* `100% GPU` means the model was loaded entirely into the GPU\n* `100% CPU` means the model was loaded entirely in system memory\n* `48%/52% CPU/GPU` means the model was loaded partially onto both the GPU and into system memory\n\n## How do I configure Ollama server?\n\nOllama server can be configured with environment variables.\n\n### Setting environment variables on Mac\n\nIf Ollama is run as a macOS application, environment variables should be set using `launchctl`:\n\n1. For each environment variable, call `launchctl setenv`.\n\n    ```bash\n    launchctl setenv OLLAMA_HOST \"0.0.0.0:11434\"\n    ```\n\n2. Restart Ollama application.\n\n### Setting environment variables on Linux\n\nIf Ollama is run as a systemd service, environment variables should be set using `systemctl`:\n\n1. Edit the systemd service by calling `systemctl edit ollama.service`. This will open an editor.\n\n2. For each environment variable, add a line `Environment` under section `[Service]`:\n\n    ```ini\n    [Service]\n    Environment=\"OLLAMA_HOST=0.0.0.0:11434\"\n    ```\n\n3. Save and exit.\n\n4. Reload `systemd` and restart Ollama:\n\n   ```shell\n   systemctl daemon-reload\n   systemctl restart ollama\n   ```\n\n### Setting environment variables on Windows\n\nOn Windows, Ollama inherits your user and system environment variables.\n\n1. First Quit Ollama by clicking on it in the task bar.\n\n2. Start the Settings (Windows 11) or Control Panel (Windows 10) application and search for _environment variables_.\n\n3. Click on _Edit environment variables for your account_.\n\n4. Edit or create a new variable for your user account for `OLLAMA_HOST`, `OLLAMA_MODELS`, etc.\n\n5. Click OK/Apply to save.\n\n6. Start the Ollama application from the Windows Start menu.\n\n## How do I use Ollama behind a proxy?\n\nOllama pulls models from the Internet and may require a proxy server to access the models. Use `HTTPS_PROXY` to redirect outbound requests through the proxy. Ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.\n\n> [!NOTE]\n> Avoid setting `HTTP_PROXY`. Ollama does not use HTTP for model pulls, only HTTPS. Setting `HTTP_PROXY` may interrupt client connections to the server.\n\n### How do I use Ollama behind a proxy in Docker?\n\nThe Ollama Docker container image can be configured to use a proxy by passing `-e HTTPS_PROXY=https://proxy.example.com` when starting the container.\n\nAlternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on [macOS](https://docs.docker.com/desktop/settings/mac/#proxies), [Windows](https://docs.docker.com/desktop/settings/windows/#proxies), and [Linux](https://docs.docker.com/desktop/settings/linux/#proxies), and Docker [daemon with systemd](https://docs.docker.com/config/daemon/systemd/#httphttps-proxy).\n\nEnsure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.\n\n```dockerfile\nFROM ollama/ollama\nCOPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt\nRUN update-ca-certificates\n```\n\nBuild and run this image:\n\n```shell\ndocker build -t ollama-with-ca .\ndocker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca\n```\n\n## Does Ollama send my prompts and answers back to ollama.com?\n\nNo. Ollama runs locally, and conversation data does not leave your machine.\n\n## How can I expose Ollama on my network?\n\nOllama binds 127.0.0.1 port 11434 by default. Change the bind address with the `OLLAMA_HOST` environment variable.\n\nRefer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.\n\n## How can I use Ollama with a proxy server?\n\nOllama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:\n\n```nginx\nserver {\n    listen 80;\n    server_name example.com;  # Replace with your domain or IP\n    location / {\n        proxy_pass http://localhost:11434;\n        proxy_set_header Host localhost:11434;\n    }\n}\n```\n\n## How can I use Ollama with ngrok?\n\nOllama can be accessed using a range of tools for tunneling tools. For example with Ngrok:\n\n```shell\nngrok http 11434 --host-header=\"localhost:11434\"\n```\n\n## How can I use Ollama with Cloudflare Tunnel?\n\nTo use Ollama with Cloudflare Tunnel, use the `--url` and `--http-host-header` flags:\n\n```shell\ncloudflared tunnel --url http://localhost:11434 --http-host-header=\"localhost:11434\"\n```\n\n## How can I allow additional web origins to access Ollama?\n\nOllama allows cross-origin requests from `127.0.0.1` and `0.0.0.0` by default. Additional origins can be configured with `OLLAMA_ORIGINS`.\n\nFor browser extensions, you'll need to explicitly allow the extension's origin pattern. Set `OLLAMA_ORIGINS` to include `chrome-extension://*`, `moz-extension://*`, and `safari-web-extension://*` if you wish to allow all browser extensions access, or specific extensions as needed:\n\n```\n# Allow all Chrome, Firefox, and Safari extensions\nOLLAMA_ORIGINS=chrome-extension://*,moz-extension://*,safari-web-extension://* ollama serve\n```\n\nRefer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.\n\n## Where are models stored?\n\n- macOS: `~/.ollama/models`\n- Linux: `/usr/share/ollama/.ollama/models`\n- Windows: `C:\\Users\\%username%\\.ollama\\models`\n\n### How do I set them to a different location?\n\nIf a different directory needs to be used, set the environment variable `OLLAMA_MODELS` to the chosen directory.\n\n> Note: on Linux using the standard installer, the `ollama` user needs read and write access to the specified directory. To assign the directory to the `ollama` user run `sudo chown -R ollama:ollama <directory>`.\n\nRefer to the section [above](#how-do-i-configure-ollama-server) for how to set environment variables on your platform.\n\n## How can I use Ollama in Visual Studio Code?\n\nThere is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of [extensions & plugins](https://github.com/ollama/ollama#extensions--plugins) at the bottom of the main repository readme.\n\n## How do I use Ollama with GPU acceleration in Docker?\n\nThe Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the [nvidia-container-toolkit](https://github.com/NVIDIA/nvidia-container-toolkit). See [ollama/ollama](https://hub.docker.com/r/ollama/ollama) for more details.\n\nGPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.\n\n## Why is networking slow in WSL2 on Windows 10?\n\nThis can impact both installing Ollama, as well as downloading models.\n\nOpen `Control Panel > Networking and Internet > View network status and tasks` and click on `Change adapter settings` on the left panel. Find the `vEthernel (WSL)` adapter, right click and select `Properties`.\nClick on `Configure` and open the `Advanced` tab. Search through each of the properties until you find `Large Send Offload Version 2 (IPv4)` and `Large Send Offload Version 2 (IPv6)`. *Disable* both of these\nproperties.\n\n## How can I preload a model into Ollama to get faster response times?\n\nIf you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the `/api/generate` and `/api/chat` API endpoints.\n\nTo preload the mistral model using the generate endpoint, use:\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\"model\": \"mistral\"}'\n```\n\nTo use the chat completions endpoint, use:\n\n```shell\ncurl http://localhost:11434/api/chat -d '{\"model\": \"mistral\"}'\n```\n\nTo preload a model using the CLI, use the command:\n\n```shell\nollama run llama3.2 \"\"\n```\n\n## How do I keep a model loaded in memory or make it unload immediately?\n\nBy default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you're making numerous requests to the LLM. If you want to immediately unload a model from memory, use the `ollama stop` command:\n\n```shell\nollama stop llama3.2\n```\n\nIf you're using the API, use the `keep_alive` parameter with the `/api/generate` and `/api/chat` endpoints to set the amount of time that a model stays in memory. The `keep_alive` parameter can be set to:\n* a duration string (such as \"10m\" or \"24h\")\n* a number in seconds (such as 3600)\n* any negative number which will keep the model loaded in memory (e.g. -1 or \"-1m\")\n* '0' which will unload the model immediately after generating a response\n\nFor example, to preload a model and leave it in memory use:\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": -1}'\n```\n\nTo unload the model and free up memory use:\n\n```shell\ncurl http://localhost:11434/api/generate -d '{\"model\": \"llama3.2\", \"keep_alive\": 0}'\n```\n\nAlternatively, you can change the amount of time all models are loaded into memory by setting the `OLLAMA_KEEP_ALIVE` environment variable when starting the Ollama server. The `OLLAMA_KEEP_ALIVE` variable uses the same parameter types as the `keep_alive` parameter types mentioned above. Refer to the section explaining [how to configure the Ollama server](#how-do-i-configure-ollama-server) to correctly set the environment variable.\n\nThe `keep_alive` API parameter with the `/api/generate` and `/api/chat` API endpoints will override the `OLLAMA_KEEP_ALIVE` setting.\n\n## How do I manage the maximum number of requests the Ollama server can queue?\n\nIf too many requests are sent to the server, it will respond with a 503 error indicating the server is overloaded.  You can adjust how many requests may be queue by setting `OLLAMA_MAX_QUEUE`.\n\n## How does Ollama handle concurrent requests?\n\nOllama supports two levels of concurrent processing.  If your system has sufficient available memory (system memory when using CPU inference, or VRAM for GPU inference) then multiple models can be loaded at the same time.  For a given model, if there is sufficient available memory when the model is loaded, it can be configured to allow parallel request processing.\n\nIf there is insufficient available memory to load a new model request while one or more models are already loaded, all new requests will be queued until the new model can be loaded.  As prior models become idle, one or more will be unloaded to make room for the new model.  Queued requests will be processed in order.  When using GPU inference new models must be able to completely fit in VRAM to allow concurrent model loads.\n\nParallel request processing for a given model results in increasing the context size by the number of parallel requests.  For example, a 2K context with 4 parallel requests will result in an 8K context and additional memory allocation.\n\nThe following server settings may be used to adjust how Ollama handles concurrent requests on most platforms:\n\n- `OLLAMA_MAX_LOADED_MODELS` - The maximum number of models that can be loaded concurrently provided they fit in available memory.  The default is 3 * the number of GPUs or 3 for CPU inference.\n- `OLLAMA_NUM_PARALLEL` - The maximum number of parallel requests each model will process at the same time.  The default is 1, and will handle 1 request per model at a time.\n- `OLLAMA_MAX_QUEUE` - The maximum number of requests Ollama will queue when busy before rejecting additional requests. The default is 512\n\nNote: Windows with Radeon GPUs currently default to 1 model maximum due to limitations in ROCm v5.7 for available VRAM reporting.  Once ROCm v6.2 is available, Windows Radeon will follow the defaults above.  You may enable concurrent model loads on Radeon on Windows, but ensure you don't load more models than will fit into your GPUs VRAM.\n\n## How does Ollama load models on multiple GPUs?\n\nWhen loading a new model, Ollama evaluates the required VRAM for the model against what is currently available.  If the model will entirely fit on any single GPU, Ollama will load the model on that GPU.  This typically provides the best performance as it reduces the amount of data transferring across the PCI bus during inference.  If the model does not fit entirely on one GPU, then it will be spread across all the available GPUs.\n\n## How can I enable Flash Attention?\n\nFlash Attention is a feature of most modern models that can significantly reduce memory usage as the context size grows.  To enable Flash Attention, set the `OLLAMA_FLASH_ATTENTION` environment variable to `1` when starting the Ollama server.\n\n## How can I set the quantization type for the K/V cache?\n\nThe K/V context cache can be quantized to significantly reduce memory usage when Flash Attention is enabled.\n\nTo use quantized K/V cache with Ollama you can set the following environment variable:\n\n- `OLLAMA_KV_CACHE_TYPE` - The quantization type for the K/V cache.  Default is `f16`.\n\n> Note: Currently this is a global option - meaning all models will run with the specified quantization type.\n\nThe currently available K/V cache quantization types are:\n\n- `f16` - high precision and memory usage (default).\n- `q8_0` - 8-bit quantization, uses approximately 1/2 the memory of `f16` with a very small loss in precision, this usually has no noticeable impact on the model's quality (recommended if not using f16).\n- `q4_0` - 4-bit quantization, uses approximately 1/4 the memory of `f16` with a small-medium loss in precision that may be more noticeable at higher context sizes.\n\nHow much the cache quantization impacts the model's response quality will depend on the model and the task.  Models that have a high GQA count (e.g. Qwen2) may see a larger impact on precision from quantization than models with a low GQA count.\n\nYou may need to experiment with different quantization types to find the best balance between memory usage and quality.\n\n## How can I stop Ollama from starting when I login to my computer\n\nOllama for Windows and macOS register as a login item during installation.  You can disable this if you prefer not to have Ollama automatically start.  Ollama will respect this setting across upgrades, unless you uninstall the application.\n\n**Windows**\n- Remove `%APPDATA%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\Ollama.lnk`\n\n**MacOS Monterey (v12)**\n- Open `Settings` -> `Users & Groups` -> `Login Items` and find the `Ollama` entry, then click the `-` (minus) to remove\n\n**MacOS Ventura (v13) and later**\n- Open `Settings` and search for \"Login Items\", find the `Ollama` entry under \"Allow in the Background`, then click the slider to disable."
  },
  {
    "name": "gpu.md",
    "relative_path": "gpu.md",
    "path": "docs/gpu.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/gpu.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>gpu.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>gpu.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/gpu.md\">docs/gpu.md</a></em></p>\n                <hr>\n                <h1 id=\"gpu\">GPU</h1>\n<h2 id=\"nvidia\">Nvidia</h2>\n<p>Ollama supports Nvidia GPUs with compute capability 5.0+ and driver version 531 and newer.</p>\n<p>Check your compute compatibility to see if your card is supported:\n<a href=\"https://developer.nvidia.com/cuda-gpus\">https://developer.nvidia.com/cuda-gpus</a></p>\n<table>\n<thead>\n<tr>\n<th>Compute Capability</th>\n<th>Family</th>\n<th>Cards</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>12.0</td>\n<td>GeForce RTX 50xx</td>\n<td><code>RTX 5060</code> <code>RTX 5060 Ti</code> <code>RTX 5070</code> <code>RTX 5070 Ti</code> <code>RTX 5080</code> <code>RTX 5090</code></td>\n</tr>\n<tr>\n<td></td>\n<td>NVIDIA Professioal</td>\n<td><code>RTX PRO 4000 Blackwell</code> <code>RTX PRO 4500 Blackwell</code> <code>RTX PRO 5000 Blackwell</code> <code>RTX PRO 6000 Blackwell</code></td>\n</tr>\n<tr>\n<td>9.0</td>\n<td>NVIDIA</td>\n<td><code>H200</code> <code>H100</code></td>\n</tr>\n<tr>\n<td>8.9</td>\n<td>GeForce RTX 40xx</td>\n<td><code>RTX 4090</code> <code>RTX 4080 SUPER</code> <code>RTX 4080</code> <code>RTX 4070 Ti SUPER</code> <code>RTX 4070 Ti</code> <code>RTX 4070 SUPER</code> <code>RTX 4070</code> <code>RTX 4060 Ti</code> <code>RTX 4060</code></td>\n</tr>\n<tr>\n<td></td>\n<td>NVIDIA Professional</td>\n<td><code>L4</code> <code>L40</code> <code>RTX 6000</code></td>\n</tr>\n<tr>\n<td>8.6</td>\n<td>GeForce RTX 30xx</td>\n<td><code>RTX 3090 Ti</code> <code>RTX 3090</code> <code>RTX 3080 Ti</code> <code>RTX 3080</code> <code>RTX 3070 Ti</code> <code>RTX 3070</code> <code>RTX 3060 Ti</code> <code>RTX 3060</code> <code>RTX 3050 Ti</code> <code>RTX 3050</code></td>\n</tr>\n<tr>\n<td></td>\n<td>NVIDIA Professional</td>\n<td><code>A40</code> <code>RTX A6000</code> <code>RTX A5000</code> <code>RTX A4000</code> <code>RTX A3000</code> <code>RTX A2000</code> <code>A10</code> <code>A16</code> <code>A2</code></td>\n</tr>\n<tr>\n<td>8.0</td>\n<td>NVIDIA</td>\n<td><code>A100</code> <code>A30</code></td>\n</tr>\n<tr>\n<td>7.5</td>\n<td>GeForce GTX/RTX</td>\n<td><code>GTX 1650 Ti</code> <code>TITAN RTX</code> <code>RTX 2080 Ti</code> <code>RTX 2080</code> <code>RTX 2070</code> <code>RTX 2060</code></td>\n</tr>\n<tr>\n<td></td>\n<td>NVIDIA Professional</td>\n<td><code>T4</code> <code>RTX 5000</code> <code>RTX 4000</code> <code>RTX 3000</code> <code>T2000</code> <code>T1200</code> <code>T1000</code> <code>T600</code> <code>T500</code></td>\n</tr>\n<tr>\n<td></td>\n<td>Quadro</td>\n<td><code>RTX 8000</code> <code>RTX 6000</code> <code>RTX 5000</code> <code>RTX 4000</code></td>\n</tr>\n<tr>\n<td>7.0</td>\n<td>NVIDIA</td>\n<td><code>TITAN V</code> <code>V100</code> <code>Quadro GV100</code></td>\n</tr>\n<tr>\n<td>6.1</td>\n<td>NVIDIA TITAN</td>\n<td><code>TITAN Xp</code> <code>TITAN X</code></td>\n</tr>\n<tr>\n<td></td>\n<td>GeForce GTX</td>\n<td><code>GTX 1080 Ti</code> <code>GTX 1080</code> <code>GTX 1070 Ti</code> <code>GTX 1070</code> <code>GTX 1060</code> <code>GTX 1050 Ti</code> <code>GTX 1050</code></td>\n</tr>\n<tr>\n<td></td>\n<td>Quadro</td>\n<td><code>P6000</code> <code>P5200</code> <code>P4200</code> <code>P3200</code> <code>P5000</code> <code>P4000</code> <code>P3000</code> <code>P2200</code> <code>P2000</code> <code>P1000</code> <code>P620</code> <code>P600</code> <code>P500</code> <code>P520</code></td>\n</tr>\n<tr>\n<td></td>\n<td>Tesla</td>\n<td><code>P40</code> <code>P4</code></td>\n</tr>\n<tr>\n<td>6.0</td>\n<td>NVIDIA</td>\n<td><code>Tesla P100</code> <code>Quadro GP100</code></td>\n</tr>\n<tr>\n<td>5.2</td>\n<td>GeForce GTX</td>\n<td><code>GTX TITAN X</code> <code>GTX 980 Ti</code> <code>GTX 980</code> <code>GTX 970</code> <code>GTX 960</code> <code>GTX 950</code></td>\n</tr>\n<tr>\n<td></td>\n<td>Quadro</td>\n<td><code>M6000 24GB</code> <code>M6000</code> <code>M5000</code> <code>M5500M</code> <code>M4000</code> <code>M2200</code> <code>M2000</code> <code>M620</code></td>\n</tr>\n<tr>\n<td></td>\n<td>Tesla</td>\n<td><code>M60</code> <code>M40</code></td>\n</tr>\n<tr>\n<td>5.0</td>\n<td>GeForce GTX</td>\n<td><code>GTX 750 Ti</code> <code>GTX 750</code> <code>NVS 810</code></td>\n</tr>\n<tr>\n<td></td>\n<td>Quadro</td>\n<td><code>K2200</code> <code>K1200</code> <code>K620</code> <code>M1200</code> <code>M520</code> <code>M5000M</code> <code>M4000M</code> <code>M3000M</code> <code>M2000M</code> <code>M1000M</code> <code>K620M</code> <code>M600M</code> <code>M500M</code></td>\n</tr>\n</tbody>\n</table>\n<p>For building locally to support older GPUs, see <a href=\"./development.md#linux-cuda-nvidia\">developer.md</a></p>\n<h3 id=\"gpu-selection\">GPU Selection</h3>\n<p>If you have multiple NVIDIA GPUs in your system and want to limit Ollama to use\na subset, you can set <code>CUDA_VISIBLE_DEVICES</code> to a comma separated list of GPUs.\nNumeric IDs may be used, however ordering may vary, so UUIDs are more reliable.\nYou can discover the UUID of your GPUs by running <code>nvidia-smi -L</code> If you want to\nignore the GPUs and force CPU usage, use an invalid GPU ID (e.g., \"-1\")</p>\n<h3 id=\"linux-suspend-resume\">Linux Suspend Resume</h3>\n<p>On linux, after a suspend/resume cycle, sometimes Ollama will fail to discover\nyour NVIDIA GPU, and fallback to running on the CPU.  You can workaround this\ndriver bug by reloading the NVIDIA UVM driver with <code>sudo rmmod nvidia_uvm &amp;&amp;\nsudo modprobe nvidia_uvm</code></p>\n<h2 id=\"amd-radeon\">AMD Radeon</h2>\n<p>Ollama supports the following AMD GPUs:</p>\n<h3 id=\"linux-support\">Linux Support</h3>\n<table>\n<thead>\n<tr>\n<th>Family</th>\n<th>Cards and accelerators</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AMD Radeon RX</td>\n<td><code>7900 XTX</code> <code>7900 XT</code> <code>7900 GRE</code> <code>7800 XT</code> <code>7700 XT</code> <code>7600 XT</code> <code>7600</code> <code>6950 XT</code> <code>6900 XTX</code> <code>6900XT</code> <code>6800 XT</code> <code>6800</code> <code>Vega 64</code> <code>Vega 56</code></td>\n</tr>\n<tr>\n<td>AMD Radeon PRO</td>\n<td><code>W7900</code> <code>W7800</code> <code>W7700</code> <code>W7600</code> <code>W7500</code> <code>W6900X</code> <code>W6800X Duo</code> <code>W6800X</code> <code>W6800</code> <code>V620</code> <code>V420</code> <code>V340</code> <code>V320</code> <code>Vega II Duo</code> <code>Vega II</code> <code>VII</code> <code>SSG</code></td>\n</tr>\n<tr>\n<td>AMD Instinct</td>\n<td><code>MI300X</code> <code>MI300A</code> <code>MI300</code> <code>MI250X</code> <code>MI250</code> <code>MI210</code> <code>MI200</code> <code>MI100</code> <code>MI60</code> <code>MI50</code></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"windows-support\">Windows Support</h3>\n<p>With ROCm v6.1, the following GPUs are supported on Windows.</p>\n<table>\n<thead>\n<tr>\n<th>Family</th>\n<th>Cards and accelerators</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>AMD Radeon RX</td>\n<td><code>7900 XTX</code> <code>7900 XT</code> <code>7900 GRE</code> <code>7800 XT</code> <code>7700 XT</code> <code>7600 XT</code> <code>7600</code> <code>6950 XT</code> <code>6900 XTX</code> <code>6900XT</code> <code>6800 XT</code> <code>6800</code></td>\n</tr>\n<tr>\n<td>AMD Radeon PRO</td>\n<td><code>W7900</code> <code>W7800</code> <code>W7700</code> <code>W7600</code> <code>W7500</code> <code>W6900X</code> <code>W6800X Duo</code> <code>W6800X</code> <code>W6800</code> <code>V620</code></td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"overrides-on-linux\">Overrides on Linux</h3>\n<p>Ollama leverages the AMD ROCm library, which does not support all AMD GPUs. In\nsome cases you can force the system to try to use a similar LLVM target that is\nclose.  For example The Radeon RX 5400 is <code>gfx1034</code> (also known as 10.3.4)\nhowever, ROCm does not currently support this target. The closest support is\n<code>gfx1030</code>.  You can use the environment variable <code>HSA_OVERRIDE_GFX_VERSION</code> with\n<code>x.y.z</code> syntax.  So for example, to force the system to run on the RX 5400, you\nwould set <code>HSA_OVERRIDE_GFX_VERSION=\"10.3.0\"</code> as an environment variable for the\nserver.  If you have an unsupported AMD GPU you can experiment using the list of\nsupported types below.</p>\n<p>If you have multiple GPUs with different GFX versions, append the numeric device\nnumber to the environment variable to set them individually.  For example,\n<code>HSA_OVERRIDE_GFX_VERSION_0=10.3.0</code> and  <code>HSA_OVERRIDE_GFX_VERSION_1=11.0.0</code></p>\n<p>At this time, the known supported GPU types on linux are the following LLVM Targets.\nThis table shows some example GPUs that map to these LLVM targets:\n| <strong>LLVM Target</strong> | <strong>An Example GPU</strong> |\n|-----------------|---------------------|\n| gfx900 | Radeon RX Vega 56 |\n| gfx906 | Radeon Instinct MI50 |\n| gfx908 | Radeon Instinct MI100 |\n| gfx90a | Radeon Instinct MI210 |\n| gfx940 | Radeon Instinct MI300 |\n| gfx941 | |\n| gfx942 | |\n| gfx1030 | Radeon PRO V620 |\n| gfx1100 | Radeon PRO W7900 |\n| gfx1101 | Radeon PRO W7700 |\n| gfx1102 | Radeon RX 7600 |</p>\n<p>AMD is working on enhancing ROCm v6 to broaden support for families of GPUs in a\nfuture release which should increase support for more GPUs.</p>\n<p>Reach out on <a href=\"https://discord.gg/ollama\">Discord</a> or file an\n<a href=\"https://github.com/ollama/ollama/issues\">issue</a> for additional help.</p>\n<h3 id=\"gpu-selection_1\">GPU Selection</h3>\n<p>If you have multiple AMD GPUs in your system and want to limit Ollama to use a\nsubset, you can set <code>ROCR_VISIBLE_DEVICES</code> to a comma separated list of GPUs.\nYou can see the list of devices with <code>rocminfo</code>.  If you want to ignore the GPUs\nand force CPU usage, use an invalid GPU ID (e.g., \"-1\").  When available, use the\n<code>Uuid</code> to uniquely identify the device instead of numeric value.</p>\n<h3 id=\"container-permission\">Container Permission</h3>\n<p>In some Linux distributions, SELinux can prevent containers from\naccessing the AMD GPU devices.  On the host system you can run \n<code>sudo setsebool container_use_devices=1</code> to allow containers to use devices.</p>\n<h3 id=\"metal-apple-gpus\">Metal (Apple GPUs)</h3>\n<p>Ollama supports GPU acceleration on Apple devices via the Metal API.</p>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# GPU\n## Nvidia\nOllama supports Nvidia GPUs with compute capability 5.0+ and driver version 531 and newer.\n\nCheck your compute compatibility to see if your card is supported:\n[https://developer.nvidia.com/cuda-gpus](https://developer.nvidia.com/cuda-gpus)\n\n| Compute Capability | Family              | Cards                                                                                                       |\n| ------------------ | ------------------- | ----------------------------------------------------------------------------------------------------------- |\n| 12.0               | GeForce RTX 50xx    | `RTX 5060` `RTX 5060 Ti` `RTX 5070` `RTX 5070 Ti` `RTX 5080` `RTX 5090`                                     |\n|                    | NVIDIA Professioal  | `RTX PRO 4000 Blackwell` `RTX PRO 4500 Blackwell` `RTX PRO 5000 Blackwell` `RTX PRO 6000 Blackwell`         |\n| 9.0                | NVIDIA              | `H200` `H100`                                                                                               |\n| 8.9                | GeForce RTX 40xx    | `RTX 4090` `RTX 4080 SUPER` `RTX 4080` `RTX 4070 Ti SUPER` `RTX 4070 Ti` `RTX 4070 SUPER` `RTX 4070` `RTX 4060 Ti` `RTX 4060`  |\n|                    | NVIDIA Professional | `L4` `L40` `RTX 6000`                                                                                       |\n| 8.6                | GeForce RTX 30xx    | `RTX 3090 Ti` `RTX 3090` `RTX 3080 Ti` `RTX 3080` `RTX 3070 Ti` `RTX 3070` `RTX 3060 Ti` `RTX 3060` `RTX 3050 Ti` `RTX 3050`   |\n|                    | NVIDIA Professional | `A40` `RTX A6000` `RTX A5000` `RTX A4000` `RTX A3000` `RTX A2000` `A10` `A16` `A2`                          |\n| 8.0                | NVIDIA              | `A100` `A30`                                                                                                |\n| 7.5                | GeForce GTX/RTX     | `GTX 1650 Ti` `TITAN RTX` `RTX 2080 Ti` `RTX 2080` `RTX 2070` `RTX 2060`                                    |\n|                    | NVIDIA Professional | `T4` `RTX 5000` `RTX 4000` `RTX 3000` `T2000` `T1200` `T1000` `T600` `T500`                                 |\n|                    | Quadro              | `RTX 8000` `RTX 6000` `RTX 5000` `RTX 4000`                                                                 |\n| 7.0                | NVIDIA              | `TITAN V` `V100` `Quadro GV100`                                                                             |\n| 6.1                | NVIDIA TITAN        | `TITAN Xp` `TITAN X`                                                                                        |\n|                    | GeForce GTX         | `GTX 1080 Ti` `GTX 1080` `GTX 1070 Ti` `GTX 1070` `GTX 1060` `GTX 1050 Ti` `GTX 1050`                       |\n|                    | Quadro              | `P6000` `P5200` `P4200` `P3200` `P5000` `P4000` `P3000` `P2200` `P2000` `P1000` `P620` `P600` `P500` `P520` |\n|                    | Tesla               | `P40` `P4`                                                                                                  |\n| 6.0                | NVIDIA              | `Tesla P100` `Quadro GP100`                                                                                 |\n| 5.2                | GeForce GTX         | `GTX TITAN X` `GTX 980 Ti` `GTX 980` `GTX 970` `GTX 960` `GTX 950`                                          |\n|                    | Quadro              | `M6000 24GB` `M6000` `M5000` `M5500M` `M4000` `M2200` `M2000` `M620`                                        |\n|                    | Tesla               | `M60` `M40`                                                                                                 |\n| 5.0                | GeForce GTX         | `GTX 750 Ti` `GTX 750` `NVS 810`                                                                            |\n|                    | Quadro              | `K2200` `K1200` `K620` `M1200` `M520` `M5000M` `M4000M` `M3000M` `M2000M` `M1000M` `K620M` `M600M` `M500M`  |\n\nFor building locally to support older GPUs, see [developer.md](./development.md#linux-cuda-nvidia)\n\n### GPU Selection\n\nIf you have multiple NVIDIA GPUs in your system and want to limit Ollama to use\na subset, you can set `CUDA_VISIBLE_DEVICES` to a comma separated list of GPUs.\nNumeric IDs may be used, however ordering may vary, so UUIDs are more reliable.\nYou can discover the UUID of your GPUs by running `nvidia-smi -L` If you want to\nignore the GPUs and force CPU usage, use an invalid GPU ID (e.g., \"-1\")\n\n### Linux Suspend Resume\n\nOn linux, after a suspend/resume cycle, sometimes Ollama will fail to discover\nyour NVIDIA GPU, and fallback to running on the CPU.  You can workaround this\ndriver bug by reloading the NVIDIA UVM driver with `sudo rmmod nvidia_uvm &&\nsudo modprobe nvidia_uvm`\n\n## AMD Radeon\nOllama supports the following AMD GPUs:\n\n### Linux Support\n| Family         | Cards and accelerators                                                                                                               |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| AMD Radeon RX  | `7900 XTX` `7900 XT` `7900 GRE` `7800 XT` `7700 XT` `7600 XT` `7600` `6950 XT` `6900 XTX` `6900XT` `6800 XT` `6800` `Vega 64` `Vega 56`    |\n| AMD Radeon PRO | `W7900` `W7800` `W7700` `W7600` `W7500` `W6900X` `W6800X Duo` `W6800X` `W6800` `V620` `V420` `V340` `V320` `Vega II Duo` `Vega II` `VII` `SSG` |\n| AMD Instinct   | `MI300X` `MI300A` `MI300` `MI250X` `MI250` `MI210` `MI200` `MI100` `MI60` `MI50`                                                               |\n\n### Windows Support\nWith ROCm v6.1, the following GPUs are supported on Windows.\n\n| Family         | Cards and accelerators                                                                                                               |\n| -------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n| AMD Radeon RX  | `7900 XTX` `7900 XT` `7900 GRE` `7800 XT` `7700 XT` `7600 XT` `7600` `6950 XT` `6900 XTX` `6900XT` `6800 XT` `6800`    |\n| AMD Radeon PRO | `W7900` `W7800` `W7700` `W7600` `W7500` `W6900X` `W6800X Duo` `W6800X` `W6800` `V620` |\n\n\n### Overrides on Linux\nOllama leverages the AMD ROCm library, which does not support all AMD GPUs. In\nsome cases you can force the system to try to use a similar LLVM target that is\nclose.  For example The Radeon RX 5400 is `gfx1034` (also known as 10.3.4)\nhowever, ROCm does not currently support this target. The closest support is\n`gfx1030`.  You can use the environment variable `HSA_OVERRIDE_GFX_VERSION` with\n`x.y.z` syntax.  So for example, to force the system to run on the RX 5400, you\nwould set `HSA_OVERRIDE_GFX_VERSION=\"10.3.0\"` as an environment variable for the\nserver.  If you have an unsupported AMD GPU you can experiment using the list of\nsupported types below.\n\nIf you have multiple GPUs with different GFX versions, append the numeric device\nnumber to the environment variable to set them individually.  For example,\n`HSA_OVERRIDE_GFX_VERSION_0=10.3.0` and  `HSA_OVERRIDE_GFX_VERSION_1=11.0.0`\n\nAt this time, the known supported GPU types on linux are the following LLVM Targets.\nThis table shows some example GPUs that map to these LLVM targets:\n| **LLVM Target** | **An Example GPU** |\n|-----------------|---------------------|\n| gfx900 | Radeon RX Vega 56 |\n| gfx906 | Radeon Instinct MI50 |\n| gfx908 | Radeon Instinct MI100 |\n| gfx90a | Radeon Instinct MI210 |\n| gfx940 | Radeon Instinct MI300 |\n| gfx941 | |\n| gfx942 | |\n| gfx1030 | Radeon PRO V620 |\n| gfx1100 | Radeon PRO W7900 |\n| gfx1101 | Radeon PRO W7700 |\n| gfx1102 | Radeon RX 7600 |\n\nAMD is working on enhancing ROCm v6 to broaden support for families of GPUs in a\nfuture release which should increase support for more GPUs.\n\nReach out on [Discord](https://discord.gg/ollama) or file an\n[issue](https://github.com/ollama/ollama/issues) for additional help.\n\n### GPU Selection\n\nIf you have multiple AMD GPUs in your system and want to limit Ollama to use a\nsubset, you can set `ROCR_VISIBLE_DEVICES` to a comma separated list of GPUs.\nYou can see the list of devices with `rocminfo`.  If you want to ignore the GPUs\nand force CPU usage, use an invalid GPU ID (e.g., \"-1\").  When available, use the\n`Uuid` to uniquely identify the device instead of numeric value.\n\n### Container Permission\n\nIn some Linux distributions, SELinux can prevent containers from\naccessing the AMD GPU devices.  On the host system you can run \n`sudo setsebool container_use_devices=1` to allow containers to use devices.\n\n### Metal (Apple GPUs)\nOllama supports GPU acceleration on Apple devices via the Metal API.\n"
  },
  {
    "name": "import.md",
    "relative_path": "import.md",
    "path": "docs/import.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/import.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>import.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>import.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/import.md\">docs/import.md</a></em></p>\n                <hr>\n                <h1 id=\"importing-a-model\">Importing a model</h1>\n<h2 id=\"table-of-contents\">Table of Contents</h2>\n<ul>\n<li><a href=\"#Importing-a-fine-tuned-adapter-from-Safetensors-weights\">Importing a Safetensors adapter</a></li>\n<li><a href=\"#Importing-a-model-from-Safetensors-weights\">Importing a Safetensors model</a></li>\n<li><a href=\"#Importing-a-GGUF-based-model-or-adapter\">Importing a GGUF file</a></li>\n<li><a href=\"#Sharing-your-model-on-ollamacom\">Sharing models on ollama.com</a></li>\n</ul>\n<h2 id=\"importing-a-fine-tuned-adapter-from-safetensors-weights\">Importing a fine tuned adapter from Safetensors weights</h2>\n<p>First, create a <code>Modelfile</code> with a <code>FROM</code> command pointing at the base model you used for fine tuning, and an <code>ADAPTER</code> command which points to the directory with your Safetensors adapter:</p>\n<pre class=\"codehilite\"><code class=\"language-dockerfile\">FROM &lt;base model name&gt;\nADAPTER /path/to/safetensors/adapter/directory\n</code></pre>\n\n<p>Make sure that you use the same base model in the <code>FROM</code> command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your <code>Modelfile</code>, use <code>ADAPTER .</code> to specify the adapter path.</p>\n<p>Now run <code>ollama create</code> from the directory where the <code>Modelfile</code> was created:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama create my-model\n</code></pre>\n\n<p>Lastly, test the model:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama run my-model\n</code></pre>\n\n<p>Ollama supports importing adapters based on several different model architectures including:</p>\n<ul>\n<li>Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);</li>\n<li>Mistral (including Mistral 1, Mistral 2, and Mixtral); and</li>\n<li>Gemma (including Gemma 1 and Gemma 2)</li>\n</ul>\n<p>You can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:</p>\n<ul>\n<li>Hugging Face <a href=\"https://huggingface.co/docs/transformers/en/training\">fine tuning framework</a></li>\n<li><a href=\"https://github.com/unslothai/unsloth\">Unsloth</a></li>\n<li><a href=\"https://github.com/ml-explore/mlx\">MLX</a></li>\n</ul>\n<h2 id=\"importing-a-model-from-safetensors-weights\">Importing a model from Safetensors weights</h2>\n<p>First, create a <code>Modelfile</code> with a <code>FROM</code> command which points to the directory containing your Safetensors weights:</p>\n<pre class=\"codehilite\"><code class=\"language-dockerfile\">FROM /path/to/safetensors/directory\n</code></pre>\n\n<p>If you create the Modelfile in the same directory as the weights, you can use the command <code>FROM .</code>.</p>\n<p>If you do not create the Modelfile, ollama will act as if there was a Modelfile with the command <code>FROM .</code>.</p>\n<p>Now run the <code>ollama create</code> command from the directory where you created the <code>Modelfile</code>:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama create my-model\n</code></pre>\n\n<p>Lastly, test the model:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama run my-model\n</code></pre>\n\n<p>Ollama supports importing models for several different architectures including:</p>\n<ul>\n<li>Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);</li>\n<li>Mistral (including Mistral 1, Mistral 2, and Mixtral);</li>\n<li>Gemma (including Gemma 1 and Gemma 2); and</li>\n<li>Phi3</li>\n</ul>\n<p>This includes importing foundation models as well as any fine tuned models which have been <em>fused</em> with a foundation model.</p>\n<h2 id=\"importing-a-gguf-based-model-or-adapter\">Importing a GGUF based model or adapter</h2>\n<p>If you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:</p>\n<ul>\n<li>converting a Safetensors model with the <code>convert_hf_to_gguf.py</code> from Llama.cpp; </li>\n<li>converting a Safetensors adapter with the <code>convert_lora_to_gguf.py</code> from Llama.cpp; or</li>\n<li>downloading a model or adapter from a place such as HuggingFace</li>\n</ul>\n<p>To import a GGUF model, create a <code>Modelfile</code> containing:</p>\n<pre class=\"codehilite\"><code class=\"language-dockerfile\">FROM /path/to/file.gguf\n</code></pre>\n\n<p>For a GGUF adapter, create the <code>Modelfile</code> with:</p>\n<pre class=\"codehilite\"><code class=\"language-dockerfile\">FROM &lt;model name&gt;\nADAPTER /path/to/file.gguf\n</code></pre>\n\n<p>When importing a GGUF adapter, it's important to use the same base model as the base model that the adapter was created with. You can use:</p>\n<ul>\n<li>a model from Ollama</li>\n<li>a GGUF file</li>\n<li>a Safetensors based model </li>\n</ul>\n<p>Once you have created your <code>Modelfile</code>, use the <code>ollama create</code> command to build the model.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama create my-model\n</code></pre>\n\n<h2 id=\"quantizing-a-model\">Quantizing a Model</h2>\n<p>Quantizing a model allows you to run models faster and with less memory consumption but at reduced accuracy. This allows you to run a model on more modest hardware.</p>\n<p>Ollama can quantize FP16 and FP32 based models into different quantization levels using the <code>-q/--quantize</code> flag with the <code>ollama create</code> command.</p>\n<p>First, create a Modelfile with the FP16 or FP32 based model you wish to quantize.</p>\n<pre class=\"codehilite\"><code class=\"language-dockerfile\">FROM /path/to/my/gemma/f16/model\n</code></pre>\n\n<p>Use <code>ollama create</code> to then create the quantized model.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">$ ollama create --quantize q4_K_M mymodel\ntransferring model data\nquantizing F16 model to Q4_K_M\ncreating new layer sha256:735e246cc1abfd06e9cdcf95504d6789a6cd1ad7577108a70d9902fef503c1bd\ncreating new layer sha256:0853f0ad24e5865173bbf9ffcc7b0f5d56b66fd690ab1009867e45e7d2c4db0f\nwriting manifest\nsuccess\n</code></pre>\n\n<h3 id=\"supported-quantizations\">Supported Quantizations</h3>\n<ul>\n<li><code>q8_0</code></li>\n</ul>\n<h4 id=\"k-means-quantizations\">K-means Quantizations</h4>\n<ul>\n<li><code>q4_K_S</code></li>\n<li><code>q4_K_M</code></li>\n</ul>\n<h2 id=\"sharing-your-model-on-ollamacom\">Sharing your model on ollama.com</h2>\n<p>You can share any model you have created by pushing it to <a href=\"https://ollama.com\">ollama.com</a> so that other users can try it out.</p>\n<p>First, use your browser to go to the <a href=\"https://ollama.com/signup\">Ollama Sign-Up</a> page. If you already have an account, you can skip this step.</p>\n<p><img src=\"images/signup.png\" alt=\"Sign-Up\" width=\"40%\"></p>\n<p>The <code>Username</code> field will be used as part of your model's name (e.g. <code>jmorganca/mymodel</code>), so make sure you are comfortable with the username that you have selected.</p>\n<p>Now that you have created an account and are signed-in, go to the <a href=\"https://ollama.com/settings/keys\">Ollama Keys Settings</a> page.</p>\n<p>Follow the directions on the page to determine where your Ollama Public Key is located.</p>\n<p><img src=\"images/ollama-keys.png\" alt=\"Ollama Keys\" width=\"80%\"></p>\n<p>Click on the <code>Add Ollama Public Key</code> button, and copy and paste the contents of your Ollama Public Key into the text field.</p>\n<p>To push a model to <a href=\"https://ollama.com\">ollama.com</a>, first make sure that it is named correctly with your username. You may have to use the <code>ollama cp</code> command to copy\nyour model to give it the correct name. Once you're happy with your model's name, use the <code>ollama push</code> command to push it to <a href=\"https://ollama.com\">ollama.com</a>.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama cp mymodel myuser/mymodel\nollama push myuser/mymodel\n</code></pre>\n\n<p>Once your model has been pushed, other users can pull and run it by using the command:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama run myuser/mymodel\n</code></pre>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Importing a model\n\n## Table of Contents\n\n  * [Importing a Safetensors adapter](#Importing-a-fine-tuned-adapter-from-Safetensors-weights)\n  * [Importing a Safetensors model](#Importing-a-model-from-Safetensors-weights)\n  * [Importing a GGUF file](#Importing-a-GGUF-based-model-or-adapter)\n  * [Sharing models on ollama.com](#Sharing-your-model-on-ollamacom)\n\n## Importing a fine tuned adapter from Safetensors weights\n\nFirst, create a `Modelfile` with a `FROM` command pointing at the base model you used for fine tuning, and an `ADAPTER` command which points to the directory with your Safetensors adapter:\n\n```dockerfile\nFROM <base model name>\nADAPTER /path/to/safetensors/adapter/directory\n```\n\nMake sure that you use the same base model in the `FROM` command as you used to create the adapter otherwise you will get erratic results. Most frameworks use different quantization methods, so it's best to use non-quantized (i.e. non-QLoRA) adapters. If your adapter is in the same directory as your `Modelfile`, use `ADAPTER .` to specify the adapter path.\n\nNow run `ollama create` from the directory where the `Modelfile` was created:\n\n```shell\nollama create my-model\n```\n\nLastly, test the model:\n\n```shell\nollama run my-model\n```\n\nOllama supports importing adapters based on several different model architectures including:\n\n  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral); and\n  * Gemma (including Gemma 1 and Gemma 2)\n\nYou can create the adapter using a fine tuning framework or tool which can output adapters in the Safetensors format, such as:\n\n  * Hugging Face [fine tuning framework](https://huggingface.co/docs/transformers/en/training)\n  * [Unsloth](https://github.com/unslothai/unsloth)\n  * [MLX](https://github.com/ml-explore/mlx)\n\n\n## Importing a model from Safetensors weights\n\nFirst, create a `Modelfile` with a `FROM` command which points to the directory containing your Safetensors weights:\n\n```dockerfile\nFROM /path/to/safetensors/directory\n```\n\nIf you create the Modelfile in the same directory as the weights, you can use the command `FROM .`.\n\nIf you do not create the Modelfile, ollama will act as if there was a Modelfile with the command `FROM .`.\n\nNow run the `ollama create` command from the directory where you created the `Modelfile`:\n\n```shell\nollama create my-model\n```\n\nLastly, test the model:\n\n```shell\nollama run my-model\n```\n\nOllama supports importing models for several different architectures including:\n\n  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2);\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral);\n  * Gemma (including Gemma 1 and Gemma 2); and\n  * Phi3\n\nThis includes importing foundation models as well as any fine tuned models which have been _fused_ with a foundation model.\n## Importing a GGUF based model or adapter\n\nIf you have a GGUF based model or adapter it is possible to import it into Ollama. You can obtain a GGUF model or adapter by:\n\n  * converting a Safetensors model with the `convert_hf_to_gguf.py` from Llama.cpp; \n  * converting a Safetensors adapter with the `convert_lora_to_gguf.py` from Llama.cpp; or\n  * downloading a model or adapter from a place such as HuggingFace\n\nTo import a GGUF model, create a `Modelfile` containing:\n\n```dockerfile\nFROM /path/to/file.gguf\n```\n\nFor a GGUF adapter, create the `Modelfile` with:\n\n```dockerfile\nFROM <model name>\nADAPTER /path/to/file.gguf\n```\n\nWhen importing a GGUF adapter, it's important to use the same base model as the base model that the adapter was created with. You can use:\n\n * a model from Ollama\n * a GGUF file\n * a Safetensors based model \n\nOnce you have created your `Modelfile`, use the `ollama create` command to build the model.\n\n```shell\nollama create my-model\n```\n\n## Quantizing a Model\n\nQuantizing a model allows you to run models faster and with less memory consumption but at reduced accuracy. This allows you to run a model on more modest hardware.\n\nOllama can quantize FP16 and FP32 based models into different quantization levels using the `-q/--quantize` flag with the `ollama create` command.\n\nFirst, create a Modelfile with the FP16 or FP32 based model you wish to quantize.\n\n```dockerfile\nFROM /path/to/my/gemma/f16/model\n```\n\nUse `ollama create` to then create the quantized model.\n\n```shell\n$ ollama create --quantize q4_K_M mymodel\ntransferring model data\nquantizing F16 model to Q4_K_M\ncreating new layer sha256:735e246cc1abfd06e9cdcf95504d6789a6cd1ad7577108a70d9902fef503c1bd\ncreating new layer sha256:0853f0ad24e5865173bbf9ffcc7b0f5d56b66fd690ab1009867e45e7d2c4db0f\nwriting manifest\nsuccess\n```\n\n### Supported Quantizations\n\n- `q8_0`\n\n#### K-means Quantizations\n\n- `q4_K_S`\n- `q4_K_M`\n\n\n## Sharing your model on ollama.com\n\nYou can share any model you have created by pushing it to [ollama.com](https://ollama.com) so that other users can try it out.\n\nFirst, use your browser to go to the [Ollama Sign-Up](https://ollama.com/signup) page. If you already have an account, you can skip this step.\n\n<img src=\"images/signup.png\" alt=\"Sign-Up\" width=\"40%\">\n\nThe `Username` field will be used as part of your model's name (e.g. `jmorganca/mymodel`), so make sure you are comfortable with the username that you have selected.\n\nNow that you have created an account and are signed-in, go to the [Ollama Keys Settings](https://ollama.com/settings/keys) page.\n\nFollow the directions on the page to determine where your Ollama Public Key is located.\n\n<img src=\"images/ollama-keys.png\" alt=\"Ollama Keys\" width=\"80%\">\n\nClick on the `Add Ollama Public Key` button, and copy and paste the contents of your Ollama Public Key into the text field.\n\nTo push a model to [ollama.com](https://ollama.com), first make sure that it is named correctly with your username. You may have to use the `ollama cp` command to copy\nyour model to give it the correct name. Once you're happy with your model's name, use the `ollama push` command to push it to [ollama.com](https://ollama.com).\n\n```shell\nollama cp mymodel myuser/mymodel\nollama push myuser/mymodel\n```\n\nOnce your model has been pushed, other users can pull and run it by using the command:\n\n```shell\nollama run myuser/mymodel\n```\n\n"
  },
  {
    "name": "linux.md",
    "relative_path": "linux.md",
    "path": "docs/linux.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/linux.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>linux.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>linux.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/linux.md\">docs/linux.md</a></em></p>\n                <hr>\n                <h1 id=\"linux\">Linux</h1>\n<h2 id=\"install\">Install</h2>\n<p>To install Ollama, run the following command:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>\n\n<h2 id=\"manual-install\">Manual install</h2>\n<blockquote>\n<p>[!NOTE]\nIf you are upgrading from a prior version, you should remove the old libraries with <code>sudo rm -rf /usr/lib/ollama</code> first.</p>\n</blockquote>\n<p>Download and extract the package:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -LO https://ollama.com/download/ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n</code></pre>\n\n<p>Start Ollama:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama serve\n</code></pre>\n\n<p>In another terminal, verify that Ollama is running:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama -v\n</code></pre>\n\n<h3 id=\"amd-gpu-install\">AMD GPU install</h3>\n<p>If you have an AMD GPU, also download and extract the additional ROCm package:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -L https://ollama.com/download/ollama-linux-amd64-rocm.tgz -o ollama-linux-amd64-rocm.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64-rocm.tgz\n</code></pre>\n\n<h3 id=\"arm64-install\">ARM64 install</h3>\n<p>Download and extract the ARM64-specific package:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -L https://ollama.com/download/ollama-linux-arm64.tgz -o ollama-linux-arm64.tgz\nsudo tar -C /usr -xzf ollama-linux-arm64.tgz\n</code></pre>\n\n<h3 id=\"adding-ollama-as-a-startup-service-recommended\">Adding Ollama as a startup service (recommended)</h3>\n<p>Create a user and group for Ollama:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama\nsudo usermod -a -G ollama $(whoami)\n</code></pre>\n\n<p>Create a service file in <code>/etc/systemd/system/ollama.service</code>:</p>\n<pre class=\"codehilite\"><code class=\"language-ini\">[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=&quot;PATH=$PATH&quot;\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n\n<p>Then start the service:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo systemctl daemon-reload\nsudo systemctl enable ollama\n</code></pre>\n\n<h3 id=\"install-cuda-drivers-optional\">Install CUDA drivers (optional)</h3>\n<p><a href=\"https://developer.nvidia.com/cuda-downloads\">Download and install</a> CUDA.</p>\n<p>Verify that the drivers are installed by running the following command, which should print details about your GPU:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">nvidia-smi\n</code></pre>\n\n<h3 id=\"install-amd-rocm-drivers-optional\">Install AMD ROCm drivers (optional)</h3>\n<p><a href=\"https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html\">Download and Install</a> ROCm v6.</p>\n<h3 id=\"start-ollama\">Start Ollama</h3>\n<p>Start Ollama and verify it is running:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo systemctl start ollama\nsudo systemctl status ollama\n</code></pre>\n\n<blockquote>\n<p>[!NOTE]\nWhile AMD has contributed the <code>amdgpu</code> driver upstream to the official linux\nkernel source, the version is older and may not support all ROCm features. We\nrecommend you install the latest driver from\n<a href=\"https://www.amd.com/en/support/download/linux-drivers.html\">AMD</a> for best support\nof your Radeon GPU.</p>\n</blockquote>\n<h2 id=\"customizing\">Customizing</h2>\n<p>To customize the installation of Ollama, you can edit the systemd service file or the environment variables by running:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo systemctl edit ollama\n</code></pre>\n\n<p>Alternatively, create an override file manually in <code>/etc/systemd/system/ollama.service.d/override.conf</code>:</p>\n<pre class=\"codehilite\"><code class=\"language-ini\">[Service]\nEnvironment=&quot;OLLAMA_DEBUG=1&quot;\n</code></pre>\n\n<h2 id=\"updating\">Updating</h2>\n<p>Update Ollama by running the install script again:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -fsSL https://ollama.com/install.sh | sh\n</code></pre>\n\n<p>Or by re-downloading Ollama:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n</code></pre>\n\n<h2 id=\"installing-specific-versions\">Installing specific versions</h2>\n<p>Use <code>OLLAMA_VERSION</code> environment variable with the install script to install a specific version of Ollama, including pre-releases. You can find the version numbers in the <a href=\"https://github.com/ollama/ollama/releases\">releases page</a>.</p>\n<p>For example:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh\n</code></pre>\n\n<h2 id=\"viewing-logs\">Viewing logs</h2>\n<p>To view logs of Ollama running as a startup service, run:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">journalctl -e -u ollama\n</code></pre>\n\n<h2 id=\"uninstall\">Uninstall</h2>\n<p>Remove the ollama service:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo systemctl stop ollama\nsudo systemctl disable ollama\nsudo rm /etc/systemd/system/ollama.service\n</code></pre>\n\n<p>Remove the ollama binary from your bin directory (either <code>/usr/local/bin</code>, <code>/usr/bin</code>, or <code>/bin</code>):</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo rm $(which ollama)\n</code></pre>\n\n<p>Remove the downloaded models and Ollama service user and group:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo rm -r /usr/share/ollama\nsudo userdel ollama\nsudo groupdel ollama\n</code></pre>\n\n<p>Remove installed libraries:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">sudo rm -rf /usr/local/lib/ollama\n</code></pre>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Linux\n\n## Install\n\nTo install Ollama, run the following command:\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\n## Manual install\n\n> [!NOTE]\n> If you are upgrading from a prior version, you should remove the old libraries with `sudo rm -rf /usr/lib/ollama` first.\n\nDownload and extract the package:\n\n```shell\ncurl -LO https://ollama.com/download/ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n```\n\nStart Ollama:\n\n```shell\nollama serve\n```\n\nIn another terminal, verify that Ollama is running:\n\n```shell\nollama -v\n```\n\n### AMD GPU install\n\nIf you have an AMD GPU, also download and extract the additional ROCm package:\n\n```shell\ncurl -L https://ollama.com/download/ollama-linux-amd64-rocm.tgz -o ollama-linux-amd64-rocm.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64-rocm.tgz\n```\n\n### ARM64 install\n\nDownload and extract the ARM64-specific package:\n\n```shell\ncurl -L https://ollama.com/download/ollama-linux-arm64.tgz -o ollama-linux-arm64.tgz\nsudo tar -C /usr -xzf ollama-linux-arm64.tgz\n```\n\n### Adding Ollama as a startup service (recommended)\n\nCreate a user and group for Ollama:\n\n```shell\nsudo useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama\nsudo usermod -a -G ollama $(whoami)\n```\n\nCreate a service file in `/etc/systemd/system/ollama.service`:\n\n```ini\n[Unit]\nDescription=Ollama Service\nAfter=network-online.target\n\n[Service]\nExecStart=/usr/bin/ollama serve\nUser=ollama\nGroup=ollama\nRestart=always\nRestartSec=3\nEnvironment=\"PATH=$PATH\"\n\n[Install]\nWantedBy=multi-user.target\n```\n\nThen start the service:\n\n```shell\nsudo systemctl daemon-reload\nsudo systemctl enable ollama\n```\n\n### Install CUDA drivers (optional)\n\n[Download and install](https://developer.nvidia.com/cuda-downloads) CUDA.\n\nVerify that the drivers are installed by running the following command, which should print details about your GPU:\n\n```shell\nnvidia-smi\n```\n\n### Install AMD ROCm drivers (optional)\n\n[Download and Install](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/tutorial/quick-start.html) ROCm v6.\n\n### Start Ollama\n\nStart Ollama and verify it is running:\n\n```shell\nsudo systemctl start ollama\nsudo systemctl status ollama\n```\n\n> [!NOTE]\n> While AMD has contributed the `amdgpu` driver upstream to the official linux\n> kernel source, the version is older and may not support all ROCm features. We\n> recommend you install the latest driver from\n> [AMD](https://www.amd.com/en/support/download/linux-drivers.html) for best support\n> of your Radeon GPU.\n\n## Customizing\n\nTo customize the installation of Ollama, you can edit the systemd service file or the environment variables by running:\n\n```shell\nsudo systemctl edit ollama\n```\n\nAlternatively, create an override file manually in `/etc/systemd/system/ollama.service.d/override.conf`:\n\n```ini\n[Service]\nEnvironment=\"OLLAMA_DEBUG=1\"\n```\n\n## Updating\n\nUpdate Ollama by running the install script again:\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | sh\n```\n\nOr by re-downloading Ollama:\n\n```shell\ncurl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\nsudo tar -C /usr -xzf ollama-linux-amd64.tgz\n```\n\n## Installing specific versions\n\nUse `OLLAMA_VERSION` environment variable with the install script to install a specific version of Ollama, including pre-releases. You can find the version numbers in the [releases page](https://github.com/ollama/ollama/releases).\n\nFor example:\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh\n```\n\n## Viewing logs\n\nTo view logs of Ollama running as a startup service, run:\n\n```shell\njournalctl -e -u ollama\n```\n\n## Uninstall\n\nRemove the ollama service:\n\n```shell\nsudo systemctl stop ollama\nsudo systemctl disable ollama\nsudo rm /etc/systemd/system/ollama.service\n```\n\nRemove the ollama binary from your bin directory (either `/usr/local/bin`, `/usr/bin`, or `/bin`):\n\n```shell\nsudo rm $(which ollama)\n```\n\nRemove the downloaded models and Ollama service user and group:\n\n```shell\nsudo rm -r /usr/share/ollama\nsudo userdel ollama\nsudo groupdel ollama\n```\n\nRemove installed libraries:\n\n```shell\nsudo rm -rf /usr/local/lib/ollama\n```\n"
  },
  {
    "name": "macos.md",
    "relative_path": "macos.md",
    "path": "docs/macos.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/macos.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>macos.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>macos.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/macos.md\">docs/macos.md</a></em></p>\n                <hr>\n                <h1 id=\"ollama-for-macos\">Ollama for macOS</h1>\n<h2 id=\"system-requirements\">System Requirements</h2>\n<ul>\n<li>MacOS Monterey (v12) or newer</li>\n<li>Apple M series (CPU and GPU support) or x86 (CPU only)</li>\n</ul>\n<h2 id=\"filesystem-requirements\">Filesystem Requirements</h2>\n<p>The preferred method of installation is to mount the <code>ollama.dmg</code> and drag-and-drop the Ollama application to the system-wide <code>Applications</code> folder.  Upon startup, the Ollama app will verify the <code>ollama</code> CLI is present in your PATH, and if not detected, will prompt for permission to create a link in <code>/usr/local/bin</code></p>\n<p>Once you've installed Ollama, you'll need additional space for storing the Large Language models, which can be tens to hundreds of GB in size.  If your home directory doesn't have enough space, you can change where the binaries are installed, and where the models are stored.</p>\n<h3 id=\"changing-install-location\">Changing Install Location</h3>\n<p>To install the Ollama application somewhere other than <code>Applications</code>, place the Ollama application in the desired location, and ensure the CLI <code>Ollama.app/Contents/Resources/ollama</code> or a sym-link to the CLI can be found in your path.  Upon first start decline the \"Move to Applications?\" request.</p>\n<h2 id=\"troubleshooting\">Troubleshooting</h2>\n<p>Ollama on MacOS stores files in a few different locations.\n- <code>~/.ollama</code> contains models and configuration\n- <code>~/.ollama/logs</code> contains logs\n    - <em>app.log</em> contains most recent logs from the GUI application\n    - <em>server.log</em> contains the most recent server logs\n- <code>&lt;install location&gt;/Ollama.app/Contents/Resources/ollama</code> the CLI binary</p>\n<h2 id=\"uninstall\">Uninstall</h2>\n<p>To fully remove Ollama from your system, remove the following files and folders:</p>\n<pre class=\"codehilite\"><code>sudo rm -rf /Applications/Ollama.app\nsudo rm /usr/local/bin/ollama\nrm -rf &quot;~/Library/Application Support/Ollama&quot;\nrm -rf &quot;~/Library/Saved Application State/com.electron.ollama.savedState&quot;\nrm -rf ~/Library/Caches/com.electron.ollama/\nrm -rf ~/Library/Caches/ollama\nrm -rf ~/Library/WebKit/com.electron.ollama\nrm -rf ~/.ollama\n</code></pre>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Ollama for macOS\n\n## System Requirements\n\n* MacOS Monterey (v12) or newer\n* Apple M series (CPU and GPU support) or x86 (CPU only)\n\n\n## Filesystem Requirements\n\nThe preferred method of installation is to mount the `ollama.dmg` and drag-and-drop the Ollama application to the system-wide `Applications` folder.  Upon startup, the Ollama app will verify the `ollama` CLI is present in your PATH, and if not detected, will prompt for permission to create a link in `/usr/local/bin`\n\nOnce you've installed Ollama, you'll need additional space for storing the Large Language models, which can be tens to hundreds of GB in size.  If your home directory doesn't have enough space, you can change where the binaries are installed, and where the models are stored.\n\n### Changing Install Location\n\nTo install the Ollama application somewhere other than `Applications`, place the Ollama application in the desired location, and ensure the CLI `Ollama.app/Contents/Resources/ollama` or a sym-link to the CLI can be found in your path.  Upon first start decline the \"Move to Applications?\" request.\n\n\n## Troubleshooting\n\nOllama on MacOS stores files in a few different locations.\n- `~/.ollama` contains models and configuration\n- `~/.ollama/logs` contains logs\n    - *app.log* contains most recent logs from the GUI application\n    - *server.log* contains the most recent server logs\n- `<install location>/Ollama.app/Contents/Resources/ollama` the CLI binary\n\n## Uninstall\n\nTo fully remove Ollama from your system, remove the following files and folders:\n\n```\nsudo rm -rf /Applications/Ollama.app\nsudo rm /usr/local/bin/ollama\nrm -rf \"~/Library/Application Support/Ollama\"\nrm -rf \"~/Library/Saved Application State/com.electron.ollama.savedState\"\nrm -rf ~/Library/Caches/com.electron.ollama/\nrm -rf ~/Library/Caches/ollama\nrm -rf ~/Library/WebKit/com.electron.ollama\nrm -rf ~/.ollama\n```\n"
  },
  {
    "name": "modelfile.md",
    "relative_path": "modelfile.md",
    "path": "docs/modelfile.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/modelfile.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>modelfile.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>modelfile.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/modelfile.md\">docs/modelfile.md</a></em></p>\n                <hr>\n                <h1 id=\"ollama-model-file\">Ollama Model File</h1>\n<blockquote>\n<p>[!NOTE]\n<code>Modelfile</code> syntax is in development</p>\n</blockquote>\n<p>A model file is the blueprint to create and share models with Ollama.</p>\n<h2 id=\"table-of-contents\">Table of Contents</h2>\n<ul>\n<li><a href=\"#format\">Format</a></li>\n<li><a href=\"#examples\">Examples</a></li>\n<li><a href=\"#instructions\">Instructions</a></li>\n<li><a href=\"#from-required\">FROM (Required)</a><ul>\n<li><a href=\"#build-from-existing-model\">Build from existing model</a></li>\n<li><a href=\"#build-from-a-safetensors-model\">Build from a Safetensors model</a></li>\n<li><a href=\"#build-from-a-gguf-file\">Build from a GGUF file</a></li>\n</ul>\n</li>\n<li><a href=\"#parameter\">PARAMETER</a><ul>\n<li><a href=\"#valid-parameters-and-values\">Valid Parameters and Values</a></li>\n</ul>\n</li>\n<li><a href=\"#template\">TEMPLATE</a><ul>\n<li><a href=\"#template-variables\">Template Variables</a></li>\n</ul>\n</li>\n<li><a href=\"#system\">SYSTEM</a></li>\n<li><a href=\"#adapter\">ADAPTER</a></li>\n<li><a href=\"#license\">LICENSE</a></li>\n<li><a href=\"#message\">MESSAGE</a></li>\n<li><a href=\"#notes\">Notes</a></li>\n</ul>\n<h2 id=\"format\">Format</h2>\n<p>The format of the <code>Modelfile</code>:</p>\n<pre class=\"codehilite\"><code># comment\nINSTRUCTION arguments\n</code></pre>\n\n<table>\n<thead>\n<tr>\n<th>Instruction</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><a href=\"#from-required\"><code>FROM</code></a> (required)</td>\n<td>Defines the base model to use.</td>\n</tr>\n<tr>\n<td><a href=\"#parameter\"><code>PARAMETER</code></a></td>\n<td>Sets the parameters for how Ollama will run the model.</td>\n</tr>\n<tr>\n<td><a href=\"#template\"><code>TEMPLATE</code></a></td>\n<td>The full prompt template to be sent to the model.</td>\n</tr>\n<tr>\n<td><a href=\"#system\"><code>SYSTEM</code></a></td>\n<td>Specifies the system message that will be set in the template.</td>\n</tr>\n<tr>\n<td><a href=\"#adapter\"><code>ADAPTER</code></a></td>\n<td>Defines the (Q)LoRA adapters to apply to the model.</td>\n</tr>\n<tr>\n<td><a href=\"#license\"><code>LICENSE</code></a></td>\n<td>Specifies the legal license.</td>\n</tr>\n<tr>\n<td><a href=\"#message\"><code>MESSAGE</code></a></td>\n<td>Specify message history.</td>\n</tr>\n</tbody>\n</table>\n<h2 id=\"examples\">Examples</h2>\n<h3 id=\"basic-modelfile\">Basic <code>Modelfile</code></h3>\n<p>An example of a <code>Modelfile</code> creating a mario blueprint:</p>\n<pre class=\"codehilite\"><code>FROM llama3.2\n# sets the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token\nPARAMETER num_ctx 4096\n\n# sets a custom system message to specify the behavior of the chat assistant\nSYSTEM You are Mario from super mario bros, acting as an assistant.\n</code></pre>\n\n<p>To use this:</p>\n<ol>\n<li>Save it as a file (e.g. <code>Modelfile</code>)</li>\n<li><code>ollama create choose-a-model-name -f &lt;location of the file e.g. ./Modelfile&gt;</code></li>\n<li><code>ollama run choose-a-model-name</code></li>\n<li>Start using the model!</li>\n</ol>\n<p>To view the Modelfile of a given model, use the <code>ollama show --modelfile</code> command.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama show --modelfile llama3.2\n</code></pre>\n\n<blockquote>\n<p><strong>Output</strong>:</p>\n<p>```</p>\n<h1 id=\"modelfile-generated-by-ollama-show\">Modelfile generated by \"ollama show\"</h1>\n<h1 id=\"to-build-a-new-modelfile-based-on-this-one-replace-the-from-line-with\">To build a new Modelfile based on this one, replace the FROM line with:</h1>\n<h1 id=\"from-llama32latest\">FROM llama3.2:latest</h1>\n<p>FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29\nTEMPLATE \"\"\"{{ if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</p>\n<p>{{ .System }}&lt;|eot_id|&gt;{{ end }}{{ if .Prompt }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</p>\n<p>{{ .Prompt }}&lt;|eot_id|&gt;{{ end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</p>\n<p>{{ .Response }}&lt;|eot_id|&gt;\"\"\"\nPARAMETER stop \"&lt;|start_header_id|&gt;\"\nPARAMETER stop \"&lt;|end_header_id|&gt;\"\nPARAMETER stop \"&lt;|eot_id|&gt;\"\nPARAMETER stop \"&lt;|reserved_special_token\"\n```</p>\n</blockquote>\n<h2 id=\"instructions\">Instructions</h2>\n<h3 id=\"from-required\">FROM (Required)</h3>\n<p>The <code>FROM</code> instruction defines the base model to use when creating a model.</p>\n<pre class=\"codehilite\"><code>FROM &lt;model name&gt;:&lt;tag&gt;\n</code></pre>\n\n<h4 id=\"build-from-existing-model\">Build from existing model</h4>\n<pre class=\"codehilite\"><code>FROM llama3.2\n</code></pre>\n\n<p>A list of available base models:\n<a href=\"https://github.com/ollama/ollama#model-library\">https://github.com/ollama/ollama#model-library</a>\nAdditional models can be found at:\n<a href=\"https://ollama.com/library\">https://ollama.com/library</a></p>\n<h4 id=\"build-from-a-safetensors-model\">Build from a Safetensors model</h4>\n<pre class=\"codehilite\"><code>FROM &lt;model directory&gt;\n</code></pre>\n\n<p>The model directory should contain the Safetensors weights for a supported architecture.</p>\n<p>Currently supported model architectures:\n  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2)\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral)\n  * Gemma (including Gemma 1 and Gemma 2)\n  * Phi3</p>\n<h4 id=\"build-from-a-gguf-file\">Build from a GGUF file</h4>\n<pre class=\"codehilite\"><code>FROM ./ollama-model.gguf\n</code></pre>\n\n<p>The GGUF file location should be specified as an absolute path or relative to the <code>Modelfile</code> location.</p>\n<h3 id=\"parameter\">PARAMETER</h3>\n<p>The <code>PARAMETER</code> instruction defines a parameter that can be set when the model is run.</p>\n<pre class=\"codehilite\"><code>PARAMETER &lt;parameter&gt; &lt;parametervalue&gt;\n</code></pre>\n\n<h4 id=\"valid-parameters-and-values\">Valid Parameters and Values</h4>\n<table>\n<thead>\n<tr>\n<th>Parameter</th>\n<th>Description</th>\n<th>Value Type</th>\n<th>Example Usage</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>num_ctx</td>\n<td>Sets the size of the context window used to generate the next token. (Default: 4096)</td>\n<td>int</td>\n<td>num_ctx 4096</td>\n</tr>\n<tr>\n<td>repeat_last_n</td>\n<td>Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)</td>\n<td>int</td>\n<td>repeat_last_n 64</td>\n</tr>\n<tr>\n<td>repeat_penalty</td>\n<td>Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)</td>\n<td>float</td>\n<td>repeat_penalty 1.1</td>\n</tr>\n<tr>\n<td>temperature</td>\n<td>The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)</td>\n<td>float</td>\n<td>temperature 0.7</td>\n</tr>\n<tr>\n<td>seed</td>\n<td>Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)</td>\n<td>int</td>\n<td>seed 42</td>\n</tr>\n<tr>\n<td>stop</td>\n<td>Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate <code>stop</code> parameters in a modelfile.</td>\n<td>string</td>\n<td>stop \"AI assistant:\"</td>\n</tr>\n<tr>\n<td>num_predict</td>\n<td>Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)</td>\n<td>int</td>\n<td>num_predict 42</td>\n</tr>\n<tr>\n<td>top_k</td>\n<td>Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)</td>\n<td>int</td>\n<td>top_k 40</td>\n</tr>\n<tr>\n<td>top_p</td>\n<td>Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)</td>\n<td>float</td>\n<td>top_p 0.9</td>\n</tr>\n<tr>\n<td>min_p</td>\n<td>Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter <em>p</em> represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with <em>p</em>=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0)</td>\n<td>float</td>\n<td>min_p 0.05</td>\n</tr>\n</tbody>\n</table>\n<h3 id=\"template\">TEMPLATE</h3>\n<p><code>TEMPLATE</code> of the full prompt template to be passed into the model. It may include (optionally) a system message, a user's message and the response from the model. Note: syntax may be model specific. Templates use Go <a href=\"https://pkg.go.dev/text/template\">template syntax</a>.</p>\n<h4 id=\"template-variables\">Template Variables</h4>\n<table>\n<thead>\n<tr>\n<th>Variable</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><code>{{ .System }}</code></td>\n<td>The system message used to specify custom behavior.</td>\n</tr>\n<tr>\n<td><code>{{ .Prompt }}</code></td>\n<td>The user prompt message.</td>\n</tr>\n<tr>\n<td><code>{{ .Response }}</code></td>\n<td>The response from the model. When generating a response, text after this variable is omitted.</td>\n</tr>\n</tbody>\n</table>\n<pre class=\"codehilite\"><code>TEMPLATE &quot;&quot;&quot;{{ if .System }}&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n{{ end }}{{ if .Prompt }}&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n{{ end }}&lt;|im_start|&gt;assistant\n&quot;&quot;&quot;\n</code></pre>\n\n<h3 id=\"system\">SYSTEM</h3>\n<p>The <code>SYSTEM</code> instruction specifies the system message to be used in the template, if applicable.</p>\n<pre class=\"codehilite\"><code>SYSTEM &quot;&quot;&quot;&lt;system message&gt;&quot;&quot;&quot;\n</code></pre>\n\n<h3 id=\"adapter\">ADAPTER</h3>\n<p>The <code>ADAPTER</code> instruction specifies a fine tuned LoRA adapter that should apply to the base model. The value of the adapter should be an absolute path or a path relative to the Modelfile. The base model should be specified with a <code>FROM</code> instruction. If the base model is not the same as the base model that the adapter was tuned from the behaviour will be erratic.</p>\n<h4 id=\"safetensor-adapter\">Safetensor adapter</h4>\n<pre class=\"codehilite\"><code>ADAPTER &lt;path to safetensor adapter&gt;\n</code></pre>\n\n<p>Currently supported Safetensor adapters:\n  * Llama (including Llama 2, Llama 3, and Llama 3.1)\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral)\n  * Gemma (including Gemma 1 and Gemma 2)</p>\n<h4 id=\"gguf-adapter\">GGUF adapter</h4>\n<pre class=\"codehilite\"><code>ADAPTER ./ollama-lora.gguf\n</code></pre>\n\n<h3 id=\"license\">LICENSE</h3>\n<p>The <code>LICENSE</code> instruction allows you to specify the legal license under which the model used with this Modelfile is shared or distributed.</p>\n<pre class=\"codehilite\"><code>LICENSE &quot;&quot;&quot;\n&lt;license text&gt;\n&quot;&quot;&quot;\n</code></pre>\n\n<h3 id=\"message\">MESSAGE</h3>\n<p>The <code>MESSAGE</code> instruction allows you to specify a message history for the model to use when responding. Use multiple iterations of the MESSAGE command to build up a conversation which will guide the model to answer in a similar way.</p>\n<pre class=\"codehilite\"><code>MESSAGE &lt;role&gt; &lt;message&gt;\n</code></pre>\n\n<h4 id=\"valid-roles\">Valid roles</h4>\n<table>\n<thead>\n<tr>\n<th>Role</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>system</td>\n<td>Alternate way of providing the SYSTEM message for the model.</td>\n</tr>\n<tr>\n<td>user</td>\n<td>An example message of what the user could have asked.</td>\n</tr>\n<tr>\n<td>assistant</td>\n<td>An example message of how the model should respond.</td>\n</tr>\n</tbody>\n</table>\n<h4 id=\"example-conversation\">Example conversation</h4>\n<pre class=\"codehilite\"><code>MESSAGE user Is Toronto in Canada?\nMESSAGE assistant yes\nMESSAGE user Is Sacramento in Canada?\nMESSAGE assistant no\nMESSAGE user Is Ontario in Canada?\nMESSAGE assistant yes\n</code></pre>\n\n<h2 id=\"notes\">Notes</h2>\n<ul>\n<li>the <strong><code>Modelfile</code> is not case sensitive</strong>. In the examples, uppercase instructions are used to make it easier to distinguish it from arguments.</li>\n<li>Instructions can be in any order. In the examples, the <code>FROM</code> instruction is first to keep it easily readable.</li>\n</ul>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Ollama Model File\n\n> [!NOTE]\n> `Modelfile` syntax is in development\n\nA model file is the blueprint to create and share models with Ollama.\n\n## Table of Contents\n\n- [Format](#format)\n- [Examples](#examples)\n- [Instructions](#instructions)\n  - [FROM (Required)](#from-required)\n    - [Build from existing model](#build-from-existing-model)\n    - [Build from a Safetensors model](#build-from-a-safetensors-model)\n    - [Build from a GGUF file](#build-from-a-gguf-file)\n  - [PARAMETER](#parameter)\n    - [Valid Parameters and Values](#valid-parameters-and-values)\n  - [TEMPLATE](#template)\n    - [Template Variables](#template-variables)\n  - [SYSTEM](#system)\n  - [ADAPTER](#adapter)\n  - [LICENSE](#license)\n  - [MESSAGE](#message)\n- [Notes](#notes)\n\n## Format\n\nThe format of the `Modelfile`:\n\n```\n# comment\nINSTRUCTION arguments\n```\n\n| Instruction                         | Description                                                    |\n| ----------------------------------- | -------------------------------------------------------------- |\n| [`FROM`](#from-required) (required) | Defines the base model to use.                                 |\n| [`PARAMETER`](#parameter)           | Sets the parameters for how Ollama will run the model.         |\n| [`TEMPLATE`](#template)             | The full prompt template to be sent to the model.              |\n| [`SYSTEM`](#system)                 | Specifies the system message that will be set in the template. |\n| [`ADAPTER`](#adapter)               | Defines the (Q)LoRA adapters to apply to the model.            |\n| [`LICENSE`](#license)               | Specifies the legal license.                                   |\n| [`MESSAGE`](#message)               | Specify message history.                                       |\n\n## Examples\n\n### Basic `Modelfile`\n\nAn example of a `Modelfile` creating a mario blueprint:\n\n```\nFROM llama3.2\n# sets the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 1\n# sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token\nPARAMETER num_ctx 4096\n\n# sets a custom system message to specify the behavior of the chat assistant\nSYSTEM You are Mario from super mario bros, acting as an assistant.\n```\n\nTo use this:\n\n1. Save it as a file (e.g. `Modelfile`)\n2. `ollama create choose-a-model-name -f <location of the file e.g. ./Modelfile>`\n3. `ollama run choose-a-model-name`\n4. Start using the model!\n\nTo view the Modelfile of a given model, use the `ollama show --modelfile` command.\n\n```shell\nollama show --modelfile llama3.2\n```\n\n> **Output**:\n>\n> ```\n> # Modelfile generated by \"ollama show\"\n> # To build a new Modelfile based on this one, replace the FROM line with:\n> # FROM llama3.2:latest\n> FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29\n> TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>\n>\n> {{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n>\n> {{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n>\n> {{ .Response }}<|eot_id|>\"\"\"\n> PARAMETER stop \"<|start_header_id|>\"\n> PARAMETER stop \"<|end_header_id|>\"\n> PARAMETER stop \"<|eot_id|>\"\n> PARAMETER stop \"<|reserved_special_token\"\n> ```\n\n\n## Instructions\n\n### FROM (Required)\n\nThe `FROM` instruction defines the base model to use when creating a model.\n\n```\nFROM <model name>:<tag>\n```\n\n#### Build from existing model\n\n```\nFROM llama3.2\n```\n\nA list of available base models:\n<https://github.com/ollama/ollama#model-library>\nAdditional models can be found at:\n<https://ollama.com/library>\n\n#### Build from a Safetensors model\n\n```\nFROM <model directory>\n```\n\nThe model directory should contain the Safetensors weights for a supported architecture.\n\nCurrently supported model architectures:\n  * Llama (including Llama 2, Llama 3, Llama 3.1, and Llama 3.2)\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral)\n  * Gemma (including Gemma 1 and Gemma 2)\n  * Phi3\n\n#### Build from a GGUF file\n\n```\nFROM ./ollama-model.gguf\n```\n\nThe GGUF file location should be specified as an absolute path or relative to the `Modelfile` location.\n\n\n### PARAMETER\n\nThe `PARAMETER` instruction defines a parameter that can be set when the model is run.\n\n```\nPARAMETER <parameter> <parametervalue>\n```\n\n#### Valid Parameters and Values\n\n| Parameter      | Description                                                                                                                                                                                                                                             | Value Type | Example Usage        |\n| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | -------------------- |\n| num_ctx        | Sets the size of the context window used to generate the next token. (Default: 4096)                                                                                                                                                                    | int        | num_ctx 4096         |\n| repeat_last_n  | Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)                                                                                                                                           | int        | repeat_last_n 64     |\n| repeat_penalty | Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)                                                                     | float      | repeat_penalty 1.1   |\n| temperature    | The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8)                                                                                                                                     | float      | temperature 0.7      |\n| seed           | Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0)                                                                                       | int        | seed 42              |\n| stop           | Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate `stop` parameters in a modelfile.                                      | string     | stop \"AI assistant:\" |\n| num_predict    | Maximum number of tokens to predict when generating text. (Default: -1, infinite generation)                                                                                                                                   | int        | num_predict 42       |\n| top_k          | Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)                                                                        | int        | top_k 40             |\n| top_p          | Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)                                                                 | float      | top_p 0.9            |\n| min_p          | Alternative to the top_p, and aims to ensure a balance of quality and variety. The parameter *p* represents the minimum probability for a token to be considered, relative to the probability of the most likely token. For example, with *p*=0.05 and the most likely token having a probability of 0.9, logits with a value less than 0.045 are filtered out. (Default: 0.0) | float      | min_p 0.05            |\n\n### TEMPLATE\n\n`TEMPLATE` of the full prompt template to be passed into the model. It may include (optionally) a system message, a user's message and the response from the model. Note: syntax may be model specific. Templates use Go [template syntax](https://pkg.go.dev/text/template).\n\n#### Template Variables\n\n| Variable          | Description                                                                                   |\n| ----------------- | --------------------------------------------------------------------------------------------- |\n| `{{ .System }}`   | The system message used to specify custom behavior.                                           |\n| `{{ .Prompt }}`   | The user prompt message.                                                                      |\n| `{{ .Response }}` | The response from the model. When generating a response, text after this variable is omitted. |\n\n```\nTEMPLATE \"\"\"{{ if .System }}<|im_start|>system\n{{ .System }}<|im_end|>\n{{ end }}{{ if .Prompt }}<|im_start|>user\n{{ .Prompt }}<|im_end|>\n{{ end }}<|im_start|>assistant\n\"\"\"\n```\n\n### SYSTEM\n\nThe `SYSTEM` instruction specifies the system message to be used in the template, if applicable.\n\n```\nSYSTEM \"\"\"<system message>\"\"\"\n```\n\n### ADAPTER\n\nThe `ADAPTER` instruction specifies a fine tuned LoRA adapter that should apply to the base model. The value of the adapter should be an absolute path or a path relative to the Modelfile. The base model should be specified with a `FROM` instruction. If the base model is not the same as the base model that the adapter was tuned from the behaviour will be erratic.\n\n#### Safetensor adapter\n\n```\nADAPTER <path to safetensor adapter>\n```\n\nCurrently supported Safetensor adapters:\n  * Llama (including Llama 2, Llama 3, and Llama 3.1)\n  * Mistral (including Mistral 1, Mistral 2, and Mixtral)\n  * Gemma (including Gemma 1 and Gemma 2)\n\n#### GGUF adapter\n\n```\nADAPTER ./ollama-lora.gguf\n```\n\n### LICENSE\n\nThe `LICENSE` instruction allows you to specify the legal license under which the model used with this Modelfile is shared or distributed.\n\n```\nLICENSE \"\"\"\n<license text>\n\"\"\"\n```\n\n### MESSAGE\n\nThe `MESSAGE` instruction allows you to specify a message history for the model to use when responding. Use multiple iterations of the MESSAGE command to build up a conversation which will guide the model to answer in a similar way.\n\n```\nMESSAGE <role> <message>\n```\n\n#### Valid roles\n\n| Role      | Description                                                  |\n| --------- | ------------------------------------------------------------ |\n| system    | Alternate way of providing the SYSTEM message for the model. |\n| user      | An example message of what the user could have asked.        |\n| assistant | An example message of how the model should respond.          |\n\n\n#### Example conversation\n\n```\nMESSAGE user Is Toronto in Canada?\nMESSAGE assistant yes\nMESSAGE user Is Sacramento in Canada?\nMESSAGE assistant no\nMESSAGE user Is Ontario in Canada?\nMESSAGE assistant yes\n```\n\n\n## Notes\n\n- the **`Modelfile` is not case sensitive**. In the examples, uppercase instructions are used to make it easier to distinguish it from arguments.\n- Instructions can be in any order. In the examples, the `FROM` instruction is first to keep it easily readable.\n\n[1]: https://ollama.com/library\n"
  },
  {
    "name": "openai.md",
    "relative_path": "openai.md",
    "path": "docs/openai.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/openai.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>openai.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>openai.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/openai.md\">docs/openai.md</a></em></p>\n                <hr>\n                <h1 id=\"openai-compatibility\">OpenAI compatibility</h1>\n<blockquote>\n<p>[!NOTE]\nOpenAI compatibility is experimental and is subject to major adjustments including breaking changes. For fully-featured access to the Ollama API, see the Ollama <a href=\"https://github.com/ollama/ollama-python\">Python library</a>, <a href=\"https://github.com/ollama/ollama-js\">JavaScript library</a> and <a href=\"https://github.com/ollama/ollama/blob/main/docs/api.md\">REST API</a>.</p>\n</blockquote>\n<p>Ollama provides experimental compatibility with parts of the <a href=\"https://platform.openai.com/docs/api-reference\">OpenAI API</a> to help connect existing applications to Ollama.</p>\n<h2 id=\"usage\">Usage</h2>\n<h3 id=\"openai-python-library\">OpenAI Python library</h3>\n<pre class=\"codehilite\"><code class=\"language-python\">from openai import OpenAI\n\nclient = OpenAI(\n    base_url='http://localhost:11434/v1/',\n\n    # required but ignored\n    api_key='ollama',\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            'role': 'user',\n            'content': 'Say this is a test',\n        }\n    ],\n    model='llama3.2',\n)\n\nresponse = client.chat.completions.create(\n    model=&quot;llava&quot;,\n    messages=[\n        {\n            &quot;role&quot;: &quot;user&quot;,\n            &quot;content&quot;: [\n                {&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What's in this image?&quot;},\n                {\n                    &quot;type&quot;: &quot;image_url&quot;,\n                    &quot;image_url&quot;: &quot;data:image/png;base64,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&quot;,\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\ncompletion = client.completions.create(\n    model=&quot;llama3.2&quot;,\n    prompt=&quot;Say this is a test&quot;,\n)\n\nlist_completion = client.models.list()\n\nmodel = client.models.retrieve(&quot;llama3.2&quot;)\n\nembeddings = client.embeddings.create(\n    model=&quot;all-minilm&quot;,\n    input=[&quot;why is the sky blue?&quot;, &quot;why is the grass green?&quot;],\n)\n</code></pre>\n\n<h4 id=\"structured-outputs\">Structured outputs</h4>\n<pre class=\"codehilite\"><code class=\"language-python\">from pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=&quot;http://localhost:11434/v1&quot;, api_key=&quot;ollama&quot;)\n\n# Define the schema for the response\nclass FriendInfo(BaseModel):\n    name: str\n    age: int\n    is_available: bool\n\nclass FriendList(BaseModel):\n    friends: list[FriendInfo]\n\ntry:\n    completion = client.beta.chat.completions.parse(\n        temperature=0,\n        model=&quot;llama3.1:8b&quot;,\n        messages=[\n            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format&quot;}\n        ],\n        response_format=FriendList,\n    )\n\n    friends_response = completion.choices[0].message\n    if friends_response.parsed:\n        print(friends_response.parsed)\n    elif friends_response.refusal:\n        print(friends_response.refusal)\nexcept Exception as e:\n    print(f&quot;Error: {e}&quot;)\n</code></pre>\n\n<h3 id=\"openai-javascript-library\">OpenAI JavaScript library</h3>\n<pre class=\"codehilite\"><code class=\"language-javascript\">import OpenAI from 'openai'\n\nconst openai = new OpenAI({\n  baseURL: 'http://localhost:11434/v1/',\n\n  // required but ignored\n  apiKey: 'ollama',\n})\n\nconst chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'llama3.2',\n})\n\nconst response = await openai.chat.completions.create({\n    model: &quot;llava&quot;,\n    messages: [\n        {\n        role: &quot;user&quot;,\n        content: [\n            { type: &quot;text&quot;, text: &quot;What's in this image?&quot; },\n            {\n            type: &quot;image_url&quot;,\n            image_url: &quot;data:image/png;base64,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&quot;,\n            },\n        ],\n        },\n    ],\n})\n\nconst completion = await openai.completions.create({\n    model: &quot;llama3.2&quot;,\n    prompt: &quot;Say this is a test.&quot;,\n})\n\nconst listCompletion = await openai.models.list()\n\nconst model = await openai.models.retrieve(&quot;llama3.2&quot;)\n\nconst embedding = await openai.embeddings.create({\n  model: &quot;all-minilm&quot;,\n  input: [&quot;why is the sky blue?&quot;, &quot;why is the grass green?&quot;],\n})\n</code></pre>\n\n<h3 id=\"curl\"><code>curl</code></h3>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/v1/chat/completions \\\n    -H &quot;Content-Type: application/json&quot; \\\n    -d '{\n        &quot;model&quot;: &quot;llama3.2&quot;,\n        &quot;messages&quot;: [\n            {\n                &quot;role&quot;: &quot;system&quot;,\n                &quot;content&quot;: &quot;You are a helpful assistant.&quot;\n            },\n            {\n                &quot;role&quot;: &quot;user&quot;,\n                &quot;content&quot;: &quot;Hello!&quot;\n            }\n        ]\n    }'\n\ncurl http://localhost:11434/v1/chat/completions \\\n  -H &quot;Content-Type: application/json&quot; \\\n  -d '{\n    &quot;model&quot;: &quot;llava&quot;,\n    &quot;messages&quot;: [\n      {\n        &quot;role&quot;: &quot;user&quot;,\n        &quot;content&quot;: [\n          {\n            &quot;type&quot;: &quot;text&quot;,\n            &quot;text&quot;: &quot;What'\\''s in this image?&quot;\n          },\n          {\n            &quot;type&quot;: &quot;image_url&quot;,\n            &quot;image_url&quot;: {\n               &quot;url&quot;: &quot;data:image/png;base64,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&quot;\n            }\n          }\n        ]\n      }\n    ],\n    &quot;max_tokens&quot;: 300\n  }'\n\ncurl http://localhost:11434/v1/completions \\\n    -H &quot;Content-Type: application/json&quot; \\\n    -d '{\n        &quot;model&quot;: &quot;llama3.2&quot;,\n        &quot;prompt&quot;: &quot;Say this is a test&quot;\n    }'\n\ncurl http://localhost:11434/v1/models\n\ncurl http://localhost:11434/v1/models/llama3.2\n\ncurl http://localhost:11434/v1/embeddings \\\n    -H &quot;Content-Type: application/json&quot; \\\n    -d '{\n        &quot;model&quot;: &quot;all-minilm&quot;,\n        &quot;input&quot;: [&quot;why is the sky blue?&quot;, &quot;why is the grass green?&quot;]\n    }'\n</code></pre>\n\n<h2 id=\"endpoints\">Endpoints</h2>\n<h3 id=\"v1chatcompletions\"><code>/v1/chat/completions</code></h3>\n<h4 id=\"supported-features\">Supported features</h4>\n<ul>\n<li>[x] Chat completions</li>\n<li>[x] Streaming</li>\n<li>[x] JSON mode</li>\n<li>[x] Reproducible outputs</li>\n<li>[x] Vision</li>\n<li>[x] Tools</li>\n<li>[ ] Logprobs</li>\n</ul>\n<h4 id=\"supported-request-fields\">Supported request fields</h4>\n<ul>\n<li>[x] <code>model</code></li>\n<li>[x] <code>messages</code></li>\n<li>[x] Text <code>content</code></li>\n<li>[x] Image <code>content</code><ul>\n<li>[x] Base64 encoded image</li>\n<li>[ ] Image URL</li>\n</ul>\n</li>\n<li>[x] Array of <code>content</code> parts</li>\n<li>[x] <code>frequency_penalty</code></li>\n<li>[x] <code>presence_penalty</code></li>\n<li>[x] <code>response_format</code></li>\n<li>[x] <code>seed</code></li>\n<li>[x] <code>stop</code></li>\n<li>[x] <code>stream</code></li>\n<li>[x] <code>stream_options</code></li>\n<li>[x] <code>include_usage</code></li>\n<li>[x] <code>temperature</code></li>\n<li>[x] <code>top_p</code></li>\n<li>[x] <code>max_tokens</code></li>\n<li>[x] <code>tools</code></li>\n<li>[ ] <code>tool_choice</code></li>\n<li>[ ] <code>logit_bias</code></li>\n<li>[ ] <code>user</code></li>\n<li>[ ] <code>n</code></li>\n</ul>\n<h3 id=\"v1completions\"><code>/v1/completions</code></h3>\n<h4 id=\"supported-features_1\">Supported features</h4>\n<ul>\n<li>[x] Completions</li>\n<li>[x] Streaming</li>\n<li>[x] JSON mode</li>\n<li>[x] Reproducible outputs</li>\n<li>[ ] Logprobs</li>\n</ul>\n<h4 id=\"supported-request-fields_1\">Supported request fields</h4>\n<ul>\n<li>[x] <code>model</code></li>\n<li>[x] <code>prompt</code></li>\n<li>[x] <code>frequency_penalty</code></li>\n<li>[x] <code>presence_penalty</code></li>\n<li>[x] <code>seed</code></li>\n<li>[x] <code>stop</code></li>\n<li>[x] <code>stream</code></li>\n<li>[x] <code>stream_options</code></li>\n<li>[x] <code>include_usage</code></li>\n<li>[x] <code>temperature</code></li>\n<li>[x] <code>top_p</code></li>\n<li>[x] <code>max_tokens</code></li>\n<li>[x] <code>suffix</code></li>\n<li>[ ] <code>best_of</code></li>\n<li>[ ] <code>echo</code></li>\n<li>[ ] <code>logit_bias</code></li>\n<li>[ ] <code>user</code></li>\n<li>[ ] <code>n</code></li>\n</ul>\n<h4 id=\"notes\">Notes</h4>\n<ul>\n<li><code>prompt</code> currently only accepts a string</li>\n</ul>\n<h3 id=\"v1models\"><code>/v1/models</code></h3>\n<h4 id=\"notes_1\">Notes</h4>\n<ul>\n<li><code>created</code> corresponds to when the model was last modified</li>\n<li><code>owned_by</code> corresponds to the ollama username, defaulting to <code>\"library\"</code></li>\n</ul>\n<h3 id=\"v1modelsmodel\"><code>/v1/models/{model}</code></h3>\n<h4 id=\"notes_2\">Notes</h4>\n<ul>\n<li><code>created</code> corresponds to when the model was last modified</li>\n<li><code>owned_by</code> corresponds to the ollama username, defaulting to <code>\"library\"</code></li>\n</ul>\n<h3 id=\"v1embeddings\"><code>/v1/embeddings</code></h3>\n<h4 id=\"supported-request-fields_2\">Supported request fields</h4>\n<ul>\n<li>[x] <code>model</code></li>\n<li>[x] <code>input</code></li>\n<li>[x] string</li>\n<li>[x] array of strings</li>\n<li>[ ] array of tokens</li>\n<li>[ ] array of token arrays</li>\n<li>[ ] <code>encoding format</code></li>\n<li>[ ] <code>dimensions</code></li>\n<li>[ ] <code>user</code></li>\n</ul>\n<h2 id=\"models\">Models</h2>\n<p>Before using a model, pull it locally <code>ollama pull</code>:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama pull llama3.2\n</code></pre>\n\n<h3 id=\"default-model-names\">Default model names</h3>\n<p>For tooling that relies on default OpenAI model names such as <code>gpt-3.5-turbo</code>, use <code>ollama cp</code> to copy an existing model name to a temporary name:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">ollama cp llama3.2 gpt-3.5-turbo\n</code></pre>\n\n<p>Afterwards, this new model name can be specified the <code>model</code> field:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/v1/chat/completions \\\n    -H &quot;Content-Type: application/json&quot; \\\n    -d '{\n        &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,\n        &quot;messages&quot;: [\n            {\n                &quot;role&quot;: &quot;user&quot;,\n                &quot;content&quot;: &quot;Hello!&quot;\n            }\n        ]\n    }'\n</code></pre>\n\n<h3 id=\"setting-the-context-size\">Setting the context size</h3>\n<p>The OpenAI API does not have a way of setting the context size for a model. If you need to change the context size, create a <code>Modelfile</code> which looks like:</p>\n<pre class=\"codehilite\"><code>FROM &lt;some model&gt;\nPARAMETER num_ctx &lt;context size&gt;\n</code></pre>\n\n<p>Use the <code>ollama create mymodel</code> command to create a new model with the updated context size. Call the API with the updated model name:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl http://localhost:11434/v1/chat/completions \\\n    -H &quot;Content-Type: application/json&quot; \\\n    -d '{\n        &quot;model&quot;: &quot;mymodel&quot;,\n        &quot;messages&quot;: [\n            {\n                &quot;role&quot;: &quot;user&quot;,\n                &quot;content&quot;: &quot;Hello!&quot;\n            }\n        ]\n    }'\n</code></pre>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# OpenAI compatibility\n\n> [!NOTE]\n> OpenAI compatibility is experimental and is subject to major adjustments including breaking changes. For fully-featured access to the Ollama API, see the Ollama [Python library](https://github.com/ollama/ollama-python), [JavaScript library](https://github.com/ollama/ollama-js) and [REST API](https://github.com/ollama/ollama/blob/main/docs/api.md).\n\nOllama provides experimental compatibility with parts of the [OpenAI API](https://platform.openai.com/docs/api-reference) to help connect existing applications to Ollama.\n\n## Usage\n\n### OpenAI Python library\n\n```python\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url='http://localhost:11434/v1/',\n\n    # required but ignored\n    api_key='ollama',\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            'role': 'user',\n            'content': 'Say this is a test',\n        }\n    ],\n    model='llama3.2',\n)\n\nresponse = client.chat.completions.create(\n    model=\"llava\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"data:image/png;base64,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\",\n                },\n            ],\n        }\n    ],\n    max_tokens=300,\n)\n\ncompletion = client.completions.create(\n    model=\"llama3.2\",\n    prompt=\"Say this is a test\",\n)\n\nlist_completion = client.models.list()\n\nmodel = client.models.retrieve(\"llama3.2\")\n\nembeddings = client.embeddings.create(\n    model=\"all-minilm\",\n    input=[\"why is the sky blue?\", \"why is the grass green?\"],\n)\n```\n\n#### Structured outputs\n\n```python\nfrom pydantic import BaseModel\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")\n\n# Define the schema for the response\nclass FriendInfo(BaseModel):\n    name: str\n    age: int\n    is_available: bool\n\nclass FriendList(BaseModel):\n    friends: list[FriendInfo]\n\ntry:\n    completion = client.beta.chat.completions.parse(\n        temperature=0,\n        model=\"llama3.1:8b\",\n        messages=[\n            {\"role\": \"user\", \"content\": \"I have two friends. The first is Ollama 22 years old busy saving the world, and the second is Alonso 23 years old and wants to hang out. Return a list of friends in JSON format\"}\n        ],\n        response_format=FriendList,\n    )\n\n    friends_response = completion.choices[0].message\n    if friends_response.parsed:\n        print(friends_response.parsed)\n    elif friends_response.refusal:\n        print(friends_response.refusal)\nexcept Exception as e:\n    print(f\"Error: {e}\")\n```\n\n### OpenAI JavaScript library\n\n```javascript\nimport OpenAI from 'openai'\n\nconst openai = new OpenAI({\n  baseURL: 'http://localhost:11434/v1/',\n\n  // required but ignored\n  apiKey: 'ollama',\n})\n\nconst chatCompletion = await openai.chat.completions.create({\n    messages: [{ role: 'user', content: 'Say this is a test' }],\n    model: 'llama3.2',\n})\n\nconst response = await openai.chat.completions.create({\n    model: \"llava\",\n    messages: [\n        {\n        role: \"user\",\n        content: [\n            { type: \"text\", text: \"What's in this image?\" },\n            {\n            type: \"image_url\",\n            image_url: \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717927ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6eT8h4ni/iIWpR5lPyA6ETkNXoSukvpJAD3AsXLiwpZs49+fPn5ke4j10TqYvegSfn0OnafC+Tv9ooA/JPkgQysqQNBzagXY55nO/oa1F7qvIPWkRL12WRpMWUvpVDYmxAPehxWSe8ZEXL20sadYIozfmNch4QJPAfeJgW3rNsnzphBKNJM2KKODo1rVOMRYik5ETy3ix4qWNI81qAAirizgMIc+yhTytx0JWZuNI03qsrgWlGtwjoS9XwgUhWGyhUaRZZQNNIEwCiXD16tXcAHUs79co0vSD8rrJCIW98pzvxpAWyyo3HYwqS0+H0BjStClcZJT5coMm6D2LOF8TolGJtK9fvyZpyiC5ePFi9nc/oJU4eiEP0jVoAnHa9wyJycITMP78+eMeP37sXrx44d6+fdt6f82aNdkx1pg9e3Zb5W+RSRE+n+VjksQWifvVaTKFhn5O8my63K8Qabdv33b379/PiAP//vuvW7BggZszZ072/+TJk91YgkafPn166zXB1rQHFvouAWHq9z3SEevSUerqCn2/dDCeta2jxYbr69evk4MHDyY7d+7MjhMnTiTPnz9Pfv/+nfQT2ggpO2dMF8cghuoM7Ygj5iWCqRlGFml0QC/ftGmTmzt3rmsaKDsgBSPh0/8yPeLLBihLkOKJc0jp8H8vUzcxIA1k6QJ/c78tWEyj5P3o4u9+jywNPdJi5rAH9x0KHcl4Hg570eQp3+vHXGyrmEeigzQsQsjavXt38ujRo44LQuDDhw+TW7duRS1HGgMxhNXHgflaNTOsHyKvHK5Ijo2jbFjJBQK9YwFd6RVMzfgRBmEfP37suBBm/p49e1qjEP2mwTViNRo0VJWH1deMXcNK08uUjVUu7s/zRaL+oLNxz1bpANco4npUgX4G2eFbpDFyQoQxojBCpEGSytmOH8qrH5Q9vuzD6ofQylkCUmh8DBAr+q8JCyVNtWQIidKQE9wNtLSQnS4jDSsxNHogzFuQBw4cyM61UKVsjfr3ooBkPSqqQHesUPWVtzi9/vQi1T+rJj7WiTz4Pt/l3LxUkr5P2VYZaZ4URpsE+st/dujQoaBBYokbrz/8TJNQYLSonrPS9kUaSkPeZyj1AWSj+d+VBoy1pIWVNed8P0Ll/ee5HdGRhrHhR5GGN0r4LGZBaj8oFDJitBTJzIZgFcmU0Y8ytWMZMzJOaXUSrUs5RxKnrxmbb5YXO9VGUhtpXldhEUogFr3IzIsvlpmdosVcGVGXFWp2oU9kLFL3dEkSz6NHEY1sjSRdIuDFWEhd8KxFqsRi1uM/nz9/zpxnwlESONdg6dKlbsaMGS4EHFHtjFIDHwKOo46l4TxSuxgDzi+rE2jg+BaFruOX4HXa0Nnf1lwAPufZeF8/r6zD97WK2qFnGjBxTw5qNGPxT+5T/r7/7RawFC3j4vTp09koCxkeHjqbHJqArmH5UrFKKksnxrK7FuRIs8STfBZv+luugXZ2pR/pP9Ois4z+TiMzUUkUjD0iEi1fzX8GmXyuxUBRcaUfykV0YZnlJGKQpOiGB76x5GeWkWWJc3mOrK6S7xdND+W5N6XyaRgtWJFe13GkaZnKOsYqGdOVVVbGupsyA/l7emTLHi7vwTdirNEt0qxnzAvBFcnQF16xh/TMpUuXHDowhlA9vQVraQhkudRdzOnK+04ZSP3DUhVSP61YsaLtd/ks7ZgtPcXqPqEafHkdqa84X6aCeL7YWlv6edGFHb+ZFICPlljHhg0bKuk0CSvVznWsotRu433alNdFrqG45ejoaPCaUkWERpLXjzFL2Rpllp7PJU2a/v7Ab8N05/9t27Z16KUqoFGsxnI9EosS2niSYg9SpU6B4JgTrvVW1flt1sT+0ADIJU2maXzcUTraGCRaL1Wp9rUMk16PMom8QhruxzvZIegJjFU7LLCePfS8uaQdPny4jTTL0dbee5mYokQsXTIWNY46kuMbnt8Kmec+LGWtOVIl9cT1rCB0V8WqkjAsRwta93TbwNYoGKsUSChN44lgBNCoHLHzquYKrU6qZ8lolCIN0Rh6cP0Q3U6I6IXILYOQI513hJaSKAorFpuHXJNfVlpRtmYBk1Su1obZr5dnKAO+L10Hrj3WZW+E3qh6IszE37F6EB+68mGpvKm4eb9bFrlzrok7fvr0Kfv727dvWRmdVTJHw0qiiCUSZ6wCK+7XL/AcsgNyL74DQQ730sv78Su7+t/A36MdY0sW5o40ahslXr58aZ5HtZB8GH64m9EmMZ7FpYw4T6QnrZfgenrhFxaSiSGXtPnz57e9TkNZLvTjeqhr734CNtrK41L40sUQckmj1lGKQ0rC37x544r8eNXRpnVE3ZZY7zXo8NomiO0ZUCj2uHz58rbXoZ6gc0uA+F6ZeKS/jhRDUq8MKrTho9fEkihMmhxtBI1DxKFY9XLpVcSkfoi8JGnToZO5sU5aiDQIW716ddt7ZLYtMQlhECdBGXZZMWldY5BHm5xgAroWj4C0hbYkSc/jBmggIrXJWlZM6pSETsEPGqZOndr2uuuR5rF169a2HoHPdurUKZM4CO1WTPqaDaAd+GFGKdIQkxAn9RuEWcTRyN2KSUgiSgF5aWzPTeA/lN5rZubMmR2bE4SIC4nJoltgAV/dVefZm72AtctUCJU2CMJ327hxY9t7EHbkyJFseq+EJSY16RPo3Dkq1kkr7+q0bNmyDuLQcZBEPYmHVdOBiJyIlrRDq41YPWfXOxUysi5fvtyaj+2BpcnsUV/oSoEMOk2CQGlr4ckhBwaetBhjCwH0ZHtJROPJkyc7UjcYLDjmrH7ADTEBXFfOYmB0k9oYBOjJ8b4aOYSe7QkKcYhFlq3QYLQhSidNmtS2RATwy8YOM3EQJsUjKiaWZ+vZToUQgzhkHXudb/PW5YMHD9yZM2faPsMwoc7RciYJXbGuBqJ1UIGKKLv915jsvgtJxCZDubdXr165mzdvtr1Hz5LONA8jrUwKPqsmVesKa49S3Q4WxmRPUEYdTjgiUcfUwLx589ySJUva3oMkP6IYddq6HMS4o55xBJBUeRjzfa4Zdeg56QZ43LhxoyPo7Lf1kNt7oO8wWAbNwaYjIv5lhyS7kRf96dvm5Jah8vfvX3flyhX35cuX6HfzFHOToS1H4BenCaHvO8pr8iDuwoUL7tevX+b5ZdbBair0xkFIlFDlW4ZknEClsp/TzXyAKVOmmHWFVSbDNw1l1+4f90U6IY/q4V27dpnE9bJ+v87QEydjqx/UamVVPRG+mwkNTYN+9tjkwzEx+atCm/X9WvWtDtAb68Wy9LXa1UmvCDDIpPkyOQ5ZwSzJ4jMrvFcr0rSjOUh+GcT4LSg5ugkW1Io0/SCDQBojh0hPlaJdah+tkVYrnTZowP8iq1F1TgMBBauufyB33x1v+NWFYmT5KmppgHC+NkAgbmRkpD3yn9QIseXymoTQFGQmIOKTxiZIWpvAatenVqRVXf2nTrAWMsPnKrMZHz6bJq5jvce6QK8J1cQNgKxlJapMPdZSR64/UivS9NztpkVEdKcrs5alhhWP9NeqlfWopzhZScI6QxseegZRGeg5a8C3Re1Mfl1ScP36ddcUaMuv24iOJtz7sbUjTS4qBvKmstYJoUauiuD3k5qhyr7QdUHMeCgLa1Ear9NquemdXgmum4fvJ6w1lqsuDhNrg1qSpleJK7K3TF0Q2jSd94uSZ60kK1e3qyVpQK6PVWXp2/FC3mp6jBhKKOiY2h3gtUV64TWM6wDETRPLDfSakXmH3w8g9Jlug8ZtTt4kVF0kLUYYmCCtD/DrQ5YhMGbA9L3ucdjh0y8kOHW5gU/VEEmJTcL4Pz/f7mgoAbYkAAAAAElFTkSuQmCC\",\n            },\n        ],\n        },\n    ],\n})\n\nconst completion = await openai.completions.create({\n    model: \"llama3.2\",\n    prompt: \"Say this is a test.\",\n})\n\nconst listCompletion = await openai.models.list()\n\nconst model = await openai.models.retrieve(\"llama3.2\")\n\nconst embedding = await openai.embeddings.create({\n  model: \"all-minilm\",\n  input: [\"why is the sky blue?\", \"why is the grass green?\"],\n})\n```\n\n### `curl`\n\n```shell\ncurl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama3.2\",\n        \"messages\": [\n            {\n                \"role\": \"system\",\n                \"content\": \"You are a helpful assistant.\"\n            },\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n\ncurl http://localhost:11434/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"llava\",\n    \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What'\\''s in this image?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n               \"url\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717927ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6eT8h4ni/iIWpR5lPyA6ETkNXoSukvpJAD3AsXLiwpZs49+fPn5ke4j10TqYvegSfn0OnafC+Tv9ooA/JPkgQysqQNBzagXY55nO/oa1F7qvIPWkRL12WRpMWUvpVDYmxAPehxWSe8ZEXL20sadYIozfmNch4QJPAfeJgW3rNsnzphBKNJM2KKODo1rVOMRYik5ETy3ix4qWNI81qAAirizgMIc+yhTytx0JWZuNI03qsrgWlGtwjoS9XwgUhWGyhUaRZZQNNIEwCiXD16tXcAHUs79co0vSD8rrJCIW98pzvxpAWyyo3HYwqS0+H0BjStClcZJT5coMm6D2LOF8TolGJtK9fvyZpyiC5ePFi9nc/oJU4eiEP0jVoAnHa9wyJycITMP78+eMeP37sXrx44d6+fdt6f82aNdkx1pg9e3Zb5W+RSRE+n+VjksQWifvVaTKFhn5O8my63K8Qabdv33b379/PiAP//vuvW7BggZszZ072/+TJk91YgkafPn166zXB1rQHFvouAWHq9z3SEevSUerqCn2/dDCeta2jxYbr69evk4MHDyY7d+7MjhMnTiTPnz9Pfv/+nfQT2ggpO2dMF8cghuoM7Ygj5iWCqRlGFml0QC/ftGmTmzt3rmsaKDsgBSPh0/8yPeLLBihLkOKJc0jp8H8vUzcxIA1k6QJ/c78tWEyj5P3o4u9+jywNPdJi5rAH9x0KHcl4Hg570eQp3+vHXGyrmEeigzQsQsjavXt38ujRo44LQuDDhw+TW7duRS1HGgMxhNXHgflaNTOsHyKvHK5Ijo2jbFjJBQK9YwFd6RVMzfgRBmEfP37suBBm/p49e1qjEP2mwTViNRo0VJWH1deMXcNK08uUjVUu7s/zRaL+oLNxz1bpANco4npUgX4G2eFbpDFyQoQxojBCpEGSytmOH8qrH5Q9vuzD6ofQylkCUmh8DBAr+q8JCyVNtWQIidKQE9wNtLSQnS4jDSsxNHogzFuQBw4cyM61UKVsjfr3ooBkPSqqQHesUPWVtzi9/vQi1T+rJj7WiTz4Pt/l3LxUkr5P2VYZaZ4URpsE+st/dujQoaBBYokbrz/8TJNQYLSonrPS9kUaSkPeZyj1AWSj+d+VBoy1pIWVNed8P0Ll/ee5HdGRhrHhR5GGN0r4LGZBaj8oFDJitBTJzIZgFcmU0Y8ytWMZMzJOaXUSrUs5RxKnrxmbb5YXO9VGUhtpXldhEUogFr3IzIsvlpmdosVcGVGXFWp2oU9kLFL3dEkSz6NHEY1sjSRdIuDFWEhd8KxFqsRi1uM/nz9/zpxnwlESONdg6dKlbsaMGS4EHFHtjFIDHwKOo46l4TxSuxgDzi+rE2jg+BaFruOX4HXa0Nnf1lwAPufZeF8/r6zD97WK2qFnGjBxTw5qNGPxT+5T/r7/7RawFC3j4vTp09koCxkeHjqbHJqArmH5UrFKKksnxrK7FuRIs8STfBZv+luugXZ2pR/pP9Ois4z+TiMzUUkUjD0iEi1fzX8GmXyuxUBRcaUfykV0YZnlJGKQpOiGB76x5GeWkWWJc3mOrK6S7xdND+W5N6XyaRgtWJFe13GkaZnKOsYqGdOVVVbGupsyA/l7emTLHi7vwTdirNEt0qxnzAvBFcnQF16xh/TMpUuXHDowhlA9vQVraQhkudRdzOnK+04ZSP3DUhVSP61YsaLtd/ks7ZgtPcXqPqEafHkdqa84X6aCeL7YWlv6edGFHb+ZFICPlljHhg0bKuk0CSvVznWsotRu433alNdFrqG45ejoaPCaUkWERpLXjzFL2Rpllp7PJU2a/v7Ab8N05/9t27Z16KUqoFGsxnI9EosS2niSYg9SpU6B4JgTrvVW1flt1sT+0ADIJU2maXzcUTraGCRaL1Wp9rUMk16PMom8QhruxzvZIegJjFU7LLCePfS8uaQdPny4jTTL0dbee5mYokQsXTIWNY46kuMbnt8Kmec+LGWtOVIl9cT1rCB0V8WqkjAsRwta93TbwNYoGKsUSChN44lgBNCoHLHzquYKrU6qZ8lolCIN0Rh6cP0Q3U6I6IXILYOQI513hJaSKAorFpuHXJNfVlpRtmYBk1Su1obZr5dnKAO+L10Hrj3WZW+E3qh6IszE37F6EB+68mGpvKm4eb9bFrlzrok7fvr0Kfv727dvWRmdVTJHw0qiiCUSZ6wCK+7XL/AcsgNyL74DQQ730sv78Su7+t/A36MdY0sW5o40ahslXr58aZ5HtZB8GH64m9EmMZ7FpYw4T6QnrZfgenrhFxaSiSGXtPnz57e9TkNZLvTjeqhr734CNtrK41L40sUQckmj1lGKQ0rC37x544r8eNXRpnVE3ZZY7zXo8NomiO0ZUCj2uHz58rbXoZ6gc0uA+F6ZeKS/jhRDUq8MKrTho9fEkihMmhxtBI1DxKFY9XLpVcSkfoi8JGnToZO5sU5aiDQIW716ddt7ZLYtMQlhECdBGXZZMWldY5BHm5xgAroWj4C0hbYkSc/jBmggIrXJWlZM6pSETsEPGqZOndr2uuuR5rF169a2HoHPdurUKZM4CO1WTPqaDaAd+GFGKdIQkxAn9RuEWcTRyN2KSUgiSgF5aWzPTeA/lN5rZubMmR2bE4SIC4nJoltgAV/dVefZm72AtctUCJU2CMJ327hxY9t7EHbkyJFseq+EJSY16RPo3Dkq1kkr7+q0bNmyDuLQcZBEPYmHVdOBiJyIlrRDq41YPWfXOxUysi5fvtyaj+2BpcnsUV/oSoEMOk2CQGlr4ckhBwaetBhjCwH0ZHtJROPJkyc7UjcYLDjmrH7ADTEBXFfOYmB0k9oYBOjJ8b4aOYSe7QkKcYhFlq3QYLQhSidNmtS2RATwy8YOM3EQJsUjKiaWZ+vZToUQgzhkHXudb/PW5YMHD9yZM2faPsMwoc7RciYJXbGuBqJ1UIGKKLv915jsvgtJxCZDubdXr165mzdvtr1Hz5LONA8jrUwKPqsmVesKa49S3Q4WxmRPUEYdTjgiUcfUwLx589ySJUva3oMkP6IYddq6HMS4o55xBJBUeRjzfa4Zdeg56QZ43LhxoyPo7Lf1kNt7oO8wWAbNwaYjIv5lhyS7kRf96dvm5Jah8vfvX3flyhX35cuX6HfzFHOToS1H4BenCaHvO8pr8iDuwoUL7tevX+b5ZdbBair0xkFIlFDlW4ZknEClsp/TzXyAKVOmmHWFVSbDNw1l1+4f90U6IY/q4V27dpnE9bJ+v87QEydjqx/UamVVPRG+mwkNTYN+9tjkwzEx+atCm/X9WvWtDtAb68Wy9LXa1UmvCDDIpPkyOQ5ZwSzJ4jMrvFcr0rSjOUh+GcT4LSg5ugkW1Io0/SCDQBojh0hPlaJdah+tkVYrnTZowP8iq1F1TgMBBauufyB33x1v+NWFYmT5KmppgHC+NkAgbmRkpD3yn9QIseXymoTQFGQmIOKTxiZIWpvAatenVqRVXf2nTrAWMsPnKrMZHz6bJq5jvce6QK8J1cQNgKxlJapMPdZSR64/UivS9NztpkVEdKcrs5alhhWP9NeqlfWopzhZScI6QxseegZRGeg5a8C3Re1Mfl1ScP36ddcUaMuv24iOJtz7sbUjTS4qBvKmstYJoUauiuD3k5qhyr7QdUHMeCgLa1Ear9NquemdXgmum4fvJ6w1lqsuDhNrg1qSpleJK7K3TF0Q2jSd94uSZ60kK1e3qyVpQK6PVWXp2/FC3mp6jBhKKOiY2h3gtUV64TWM6wDETRPLDfSakXmH3w8g9Jlug8ZtTt4kVF0kLUYYmCCtD/DrQ5YhMGbA9L3ucdjh0y8kOHW5gU/VEEmJTcL4Pz/f7mgoAbYkAAAAAElFTkSuQmCC\"\n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\": 300\n  }'\n\ncurl http://localhost:11434/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"llama3.2\",\n        \"prompt\": \"Say this is a test\"\n    }'\n\ncurl http://localhost:11434/v1/models\n\ncurl http://localhost:11434/v1/models/llama3.2\n\ncurl http://localhost:11434/v1/embeddings \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"all-minilm\",\n        \"input\": [\"why is the sky blue?\", \"why is the grass green?\"]\n    }'\n```\n\n## Endpoints\n\n### `/v1/chat/completions`\n\n#### Supported features\n\n- [x] Chat completions\n- [x] Streaming\n- [x] JSON mode\n- [x] Reproducible outputs\n- [x] Vision\n- [x] Tools\n- [ ] Logprobs\n\n#### Supported request fields\n\n- [x] `model`\n- [x] `messages`\n  - [x] Text `content`\n  - [x] Image `content`\n    - [x] Base64 encoded image\n    - [ ] Image URL\n  - [x] Array of `content` parts\n- [x] `frequency_penalty`\n- [x] `presence_penalty`\n- [x] `response_format`\n- [x] `seed`\n- [x] `stop`\n- [x] `stream`\n- [x] `stream_options`\n  - [x] `include_usage`\n- [x] `temperature`\n- [x] `top_p`\n- [x] `max_tokens`\n- [x] `tools`\n- [ ] `tool_choice`\n- [ ] `logit_bias`\n- [ ] `user`\n- [ ] `n`\n\n### `/v1/completions`\n\n#### Supported features\n\n- [x] Completions\n- [x] Streaming\n- [x] JSON mode\n- [x] Reproducible outputs\n- [ ] Logprobs\n\n#### Supported request fields\n\n- [x] `model`\n- [x] `prompt`\n- [x] `frequency_penalty`\n- [x] `presence_penalty`\n- [x] `seed`\n- [x] `stop`\n- [x] `stream`\n- [x] `stream_options`\n  - [x] `include_usage`\n- [x] `temperature`\n- [x] `top_p`\n- [x] `max_tokens`\n- [x] `suffix`\n- [ ] `best_of`\n- [ ] `echo`\n- [ ] `logit_bias`\n- [ ] `user`\n- [ ] `n`\n\n#### Notes\n\n- `prompt` currently only accepts a string\n\n### `/v1/models`\n\n#### Notes\n\n- `created` corresponds to when the model was last modified\n- `owned_by` corresponds to the ollama username, defaulting to `\"library\"`\n\n### `/v1/models/{model}`\n\n#### Notes\n\n- `created` corresponds to when the model was last modified\n- `owned_by` corresponds to the ollama username, defaulting to `\"library\"`\n\n### `/v1/embeddings`\n\n#### Supported request fields\n\n- [x] `model`\n- [x] `input`\n  - [x] string\n  - [x] array of strings\n  - [ ] array of tokens\n  - [ ] array of token arrays\n- [ ] `encoding format`\n- [ ] `dimensions`\n- [ ] `user`\n\n## Models\n\nBefore using a model, pull it locally `ollama pull`:\n\n```shell\nollama pull llama3.2\n```\n\n### Default model names\n\nFor tooling that relies on default OpenAI model names such as `gpt-3.5-turbo`, use `ollama cp` to copy an existing model name to a temporary name:\n\n```shell\nollama cp llama3.2 gpt-3.5-turbo\n```\n\nAfterwards, this new model name can be specified the `model` field:\n\n```shell\ncurl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"gpt-3.5-turbo\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n```\n\n### Setting the context size\n\nThe OpenAI API does not have a way of setting the context size for a model. If you need to change the context size, create a `Modelfile` which looks like:\n\n```\nFROM <some model>\nPARAMETER num_ctx <context size>\n```\n\nUse the `ollama create mymodel` command to create a new model with the updated context size. Call the API with the updated model name:\n\n```shell\ncurl http://localhost:11434/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"mymodel\",\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Hello!\"\n            }\n        ]\n    }'\n```\n"
  },
  {
    "name": "template.md",
    "relative_path": "template.md",
    "path": "docs/template.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/template.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>template.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>template.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/template.md\">docs/template.md</a></em></p>\n                <hr>\n                <h1 id=\"template\">Template</h1>\n<p>Ollama provides a powerful templating engine backed by Go's built-in templating engine to construct prompts for your large language model. This feature is a valuable tool to get the most out of your models.</p>\n<h2 id=\"basic-template-structure\">Basic Template Structure</h2>\n<p>A basic Go template consists of three main parts:</p>\n<ul>\n<li><strong>Layout</strong>: The overall structure of the template.</li>\n<li><strong>Variables</strong>: Placeholders for dynamic data that will be replaced with actual values when the template is rendered.</li>\n<li><strong>Functions</strong>: Custom functions or logic that can be used to manipulate the template's content.</li>\n</ul>\n<p>Here's an example of a simple chat template:</p>\n<pre class=\"codehilite\"><code class=\"language-go\">{{- range .Messages }}\n{{ .Role }}: {{ .Content }}\n{{- end }}\n</code></pre>\n\n<p>In this example, we have:</p>\n<ul>\n<li>A basic messages structure (layout)</li>\n<li>Three variables: <code>Messages</code>, <code>Role</code>, and <code>Content</code> (variables)</li>\n<li>A custom function (action) that iterates over an array of items (<code>range .Messages</code>) and displays each item</li>\n</ul>\n<h2 id=\"adding-templates-to-your-model\">Adding templates to your model</h2>\n<p>By default, models imported into Ollama have a default template of <code>{{ .Prompt }}</code>, i.e. user inputs are sent verbatim to the LLM. This is appropriate for text or code completion models but lacks essential markers for chat or instruction models.</p>\n<p>Omitting a template in these models puts the responsibility of correctly templating input onto the user. Adding a template allows users to easily get the best results from the model.</p>\n<p>To add templates in your model, you'll need to add a <code>TEMPLATE</code> command to the Modelfile. Here's an example using Meta's Llama 3.</p>\n<pre class=\"codehilite\"><code class=\"language-dockerfile\">FROM llama3.2\n\nTEMPLATE &quot;&quot;&quot;{{- if .System }}&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{{ .System }}&lt;|eot_id|&gt;\n{{- end }}\n{{- range .Messages }}&lt;|start_header_id|&gt;{{ .Role }}&lt;|end_header_id|&gt;\n\n{{ .Content }}&lt;|eot_id|&gt;\n{{- end }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n&quot;&quot;&quot;\n</code></pre>\n\n<h2 id=\"variables\">Variables</h2>\n<p><code>System</code> (string): system prompt</p>\n<p><code>Prompt</code> (string): user prompt</p>\n<p><code>Response</code> (string): assistant response</p>\n<p><code>Suffix</code> (string): text inserted after the assistant's response</p>\n<p><code>Messages</code> (list): list of messages</p>\n<p><code>Messages[].Role</code> (string): role which can be one of <code>system</code>, <code>user</code>, <code>assistant</code>, or <code>tool</code></p>\n<p><code>Messages[].Content</code> (string):  message content</p>\n<p><code>Messages[].ToolCalls</code> (list): list of tools the model wants to call</p>\n<p><code>Messages[].ToolCalls[].Function</code> (object): function to call</p>\n<p><code>Messages[].ToolCalls[].Function.Name</code> (string): function name</p>\n<p><code>Messages[].ToolCalls[].Function.Arguments</code> (map): mapping of argument name to argument value</p>\n<p><code>Tools</code> (list): list of tools the model can access</p>\n<p><code>Tools[].Type</code> (string): schema type. <code>type</code> is always <code>function</code></p>\n<p><code>Tools[].Function</code> (object): function definition</p>\n<p><code>Tools[].Function.Name</code> (string): function name</p>\n<p><code>Tools[].Function.Description</code> (string): function description</p>\n<p><code>Tools[].Function.Parameters</code> (object): function parameters</p>\n<p><code>Tools[].Function.Parameters.Type</code> (string): schema type. <code>type</code> is always <code>object</code></p>\n<p><code>Tools[].Function.Parameters.Required</code> (list): list of required properties</p>\n<p><code>Tools[].Function.Parameters.Properties</code> (map): mapping of property name to property definition</p>\n<p><code>Tools[].Function.Parameters.Properties[].Type</code> (string): property type</p>\n<p><code>Tools[].Function.Parameters.Properties[].Description</code> (string): property description</p>\n<p><code>Tools[].Function.Parameters.Properties[].Enum</code> (list): list of valid values</p>\n<h2 id=\"tips-and-best-practices\">Tips and Best Practices</h2>\n<p>Keep the following tips and best practices in mind when working with Go templates:</p>\n<ul>\n<li><strong>Be mindful of dot</strong>: Control flow structures like <code>range</code> and <code>with</code> changes the value <code>.</code></li>\n<li><strong>Out-of-scope variables</strong>: Use <code>$.</code> to reference variables not currently in scope, starting from the root</li>\n<li><strong>Whitespace control</strong>: Use <code>-</code> to trim leading (<code>{{-</code>) and trailing (<code>-}}</code>) whitespace</li>\n</ul>\n<h2 id=\"examples\">Examples</h2>\n<h3 id=\"example-messages\">Example Messages</h3>\n<h4 id=\"chatml\">ChatML</h4>\n<p>ChatML is a popular template format. It can be used for models such as Databrick's DBRX, Intel's Neural Chat, and Microsoft's Orca 2.</p>\n<pre class=\"codehilite\"><code class=\"language-go\">{{- range .Messages }}&lt;|im_start|&gt;{{ .Role }}\n{{ .Content }}&lt;|im_end|&gt;\n{{ end }}&lt;|im_start|&gt;assistant\n</code></pre>\n\n<h3 id=\"example-tools\">Example Tools</h3>\n<p>Tools support can be added to a model by adding a <code>{{ .Tools }}</code> node to the template. This feature is useful for models trained to call external tools and can a powerful tool for retrieving real-time data or performing complex tasks.</p>\n<h4 id=\"mistral\">Mistral</h4>\n<p>Mistral v0.3 and Mixtral 8x22B supports tool calling.</p>\n<pre class=\"codehilite\"><code class=\"language-go\">{{- range $index, $_ := .Messages }}\n{{- if eq .Role &quot;user&quot; }}\n{{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS] {{ json $.Tools }}[/AVAILABLE_TOOLS]\n{{- end }}[INST] {{ if and (eq (len (slice $.Messages $index)) 1) $.System }}{{ $.System }}\n\n{{ end }}{{ .Content }}[/INST]\n{{- else if eq .Role &quot;assistant&quot; }}\n{{- if .Content }} {{ .Content }}&lt;/s&gt;\n{{- else if .ToolCalls }}[TOOL_CALLS] [\n{{- range .ToolCalls }}{&quot;name&quot;: &quot;{{ .Function.Name }}&quot;, &quot;arguments&quot;: {{ json .Function.Arguments }}}\n{{- end }}]&lt;/s&gt;\n{{- end }}\n{{- else if eq .Role &quot;tool&quot; }}[TOOL_RESULTS] {&quot;content&quot;: {{ .Content }}}[/TOOL_RESULTS]\n{{- end }}\n{{- end }}\n</code></pre>\n\n<h3 id=\"example-fill-in-middle\">Example Fill-in-Middle</h3>\n<p>Fill-in-middle support can be added to a model by adding a <code>{{ .Suffix }}</code> node to the template. This feature is useful for models that are trained to generate text in the middle of user input, such as code completion models.</p>\n<h4 id=\"codellama\">CodeLlama</h4>\n<p>CodeLlama <a href=\"https://ollama.com/library/codellama:7b-code\">7B</a> and <a href=\"https://ollama.com/library/codellama:13b-code\">13B</a> code completion models support fill-in-middle.</p>\n<pre class=\"codehilite\"><code class=\"language-go\">&lt;PRE&gt; {{ .Prompt }} &lt;SUF&gt;{{ .Suffix }} &lt;MID&gt;\n</code></pre>\n\n<blockquote>\n<p>[!NOTE]\nCodeLlama 34B and 70B code completion and all instruct and Python fine-tuned models do not support fill-in-middle.</p>\n</blockquote>\n<h4 id=\"codestral\">Codestral</h4>\n<p>Codestral <a href=\"https://ollama.com/library/codestral:22b\">22B</a> supports fill-in-middle.</p>\n<pre class=\"codehilite\"><code class=\"language-go\">[SUFFIX]{{ .Suffix }}[PREFIX] {{ .Prompt }}\n</code></pre>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Template\n\nOllama provides a powerful templating engine backed by Go's built-in templating engine to construct prompts for your large language model. This feature is a valuable tool to get the most out of your models.\n\n## Basic Template Structure\n\nA basic Go template consists of three main parts:\n\n* **Layout**: The overall structure of the template.\n* **Variables**: Placeholders for dynamic data that will be replaced with actual values when the template is rendered.\n* **Functions**: Custom functions or logic that can be used to manipulate the template's content.\n\nHere's an example of a simple chat template:\n\n```go\n{{- range .Messages }}\n{{ .Role }}: {{ .Content }}\n{{- end }}\n```\n\nIn this example, we have:\n\n* A basic messages structure (layout)\n* Three variables: `Messages`, `Role`, and `Content` (variables)\n* A custom function (action) that iterates over an array of items (`range .Messages`) and displays each item\n\n## Adding templates to your model\n\nBy default, models imported into Ollama have a default template of `{{ .Prompt }}`, i.e. user inputs are sent verbatim to the LLM. This is appropriate for text or code completion models but lacks essential markers for chat or instruction models.\n\nOmitting a template in these models puts the responsibility of correctly templating input onto the user. Adding a template allows users to easily get the best results from the model.\n\nTo add templates in your model, you'll need to add a `TEMPLATE` command to the Modelfile. Here's an example using Meta's Llama 3.\n\n```dockerfile\nFROM llama3.2\n\nTEMPLATE \"\"\"{{- if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>\n{{- end }}\n{{- range .Messages }}<|start_header_id|>{{ .Role }}<|end_header_id|>\n\n{{ .Content }}<|eot_id|>\n{{- end }}<|start_header_id|>assistant<|end_header_id|>\n\n\"\"\"\n```\n\n## Variables\n\n`System` (string): system prompt\n\n`Prompt` (string): user prompt\n\n`Response` (string): assistant response\n\n`Suffix` (string): text inserted after the assistant's response\n\n`Messages` (list): list of messages\n\n`Messages[].Role` (string): role which can be one of `system`, `user`, `assistant`, or `tool`\n\n`Messages[].Content` (string):  message content\n\n`Messages[].ToolCalls` (list): list of tools the model wants to call\n\n`Messages[].ToolCalls[].Function` (object): function to call\n\n`Messages[].ToolCalls[].Function.Name` (string): function name\n\n`Messages[].ToolCalls[].Function.Arguments` (map): mapping of argument name to argument value\n\n`Tools` (list): list of tools the model can access\n\n`Tools[].Type` (string): schema type. `type` is always `function`\n\n`Tools[].Function` (object): function definition\n\n`Tools[].Function.Name` (string): function name\n\n`Tools[].Function.Description` (string): function description\n\n`Tools[].Function.Parameters` (object): function parameters\n\n`Tools[].Function.Parameters.Type` (string): schema type. `type` is always `object`\n\n`Tools[].Function.Parameters.Required` (list): list of required properties\n\n`Tools[].Function.Parameters.Properties` (map): mapping of property name to property definition\n\n`Tools[].Function.Parameters.Properties[].Type` (string): property type\n\n`Tools[].Function.Parameters.Properties[].Description` (string): property description\n\n`Tools[].Function.Parameters.Properties[].Enum` (list): list of valid values\n\n## Tips and Best Practices\n\nKeep the following tips and best practices in mind when working with Go templates:\n\n* **Be mindful of dot**: Control flow structures like `range` and `with` changes the value `.`\n* **Out-of-scope variables**: Use `$.` to reference variables not currently in scope, starting from the root\n* **Whitespace control**: Use `-` to trim leading (`{{-`) and trailing (`-}}`) whitespace\n\n## Examples\n\n### Example Messages\n\n#### ChatML\n\nChatML is a popular template format. It can be used for models such as Databrick's DBRX, Intel's Neural Chat, and Microsoft's Orca 2.\n\n```go\n{{- range .Messages }}<|im_start|>{{ .Role }}\n{{ .Content }}<|im_end|>\n{{ end }}<|im_start|>assistant\n```\n\n### Example Tools\n\nTools support can be added to a model by adding a `{{ .Tools }}` node to the template. This feature is useful for models trained to call external tools and can a powerful tool for retrieving real-time data or performing complex tasks.\n\n#### Mistral\n\nMistral v0.3 and Mixtral 8x22B supports tool calling.\n\n```go\n{{- range $index, $_ := .Messages }}\n{{- if eq .Role \"user\" }}\n{{- if and (le (len (slice $.Messages $index)) 2) $.Tools }}[AVAILABLE_TOOLS] {{ json $.Tools }}[/AVAILABLE_TOOLS]\n{{- end }}[INST] {{ if and (eq (len (slice $.Messages $index)) 1) $.System }}{{ $.System }}\n\n{{ end }}{{ .Content }}[/INST]\n{{- else if eq .Role \"assistant\" }}\n{{- if .Content }} {{ .Content }}</s>\n{{- else if .ToolCalls }}[TOOL_CALLS] [\n{{- range .ToolCalls }}{\"name\": \"{{ .Function.Name }}\", \"arguments\": {{ json .Function.Arguments }}}\n{{- end }}]</s>\n{{- end }}\n{{- else if eq .Role \"tool\" }}[TOOL_RESULTS] {\"content\": {{ .Content }}}[/TOOL_RESULTS]\n{{- end }}\n{{- end }}\n```\n\n### Example Fill-in-Middle\n\nFill-in-middle support can be added to a model by adding a `{{ .Suffix }}` node to the template. This feature is useful for models that are trained to generate text in the middle of user input, such as code completion models.\n\n#### CodeLlama\n\nCodeLlama [7B](https://ollama.com/library/codellama:7b-code) and [13B](https://ollama.com/library/codellama:13b-code) code completion models support fill-in-middle.\n\n```go\n<PRE> {{ .Prompt }} <SUF>{{ .Suffix }} <MID>\n```\n\n> [!NOTE]\n> CodeLlama 34B and 70B code completion and all instruct and Python fine-tuned models do not support fill-in-middle.\n\n#### Codestral\n\nCodestral [22B](https://ollama.com/library/codestral:22b) supports fill-in-middle.\n\n```go\n[SUFFIX]{{ .Suffix }}[PREFIX] {{ .Prompt }}\n```\n"
  },
  {
    "name": "troubleshooting.md",
    "relative_path": "troubleshooting.md",
    "path": "docs/troubleshooting.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/troubleshooting.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>troubleshooting.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>troubleshooting.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/troubleshooting.md\">docs/troubleshooting.md</a></em></p>\n                <hr>\n                <h1 id=\"how-to-troubleshoot-issues\">How to troubleshoot issues</h1>\n<p>Sometimes Ollama may not perform as expected. One of the best ways to figure out what happened is to take a look at the logs. Find the logs on <strong>Mac</strong> by running the command:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">cat ~/.ollama/logs/server.log\n</code></pre>\n\n<p>On <strong>Linux</strong> systems with systemd, the logs can be found with this command:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">journalctl -u ollama --no-pager --follow --pager-end\n</code></pre>\n\n<p>When you run Ollama in a <strong>container</strong>, the logs go to stdout/stderr in the container:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">docker logs &lt;container-name&gt;\n</code></pre>\n\n<p>(Use <code>docker ps</code> to find the container name)</p>\n<p>If manually running <code>ollama serve</code> in a terminal, the logs will be on that terminal.</p>\n<p>When you run Ollama on <strong>Windows</strong>, there are a few different locations. You can view them in the explorer window by hitting <code>&lt;cmd&gt;+R</code> and type in:\n- <code>explorer %LOCALAPPDATA%\\Ollama</code> to view logs.  The most recent server logs will be in <code>server.log</code> and older logs will be in <code>server-#.log</code>\n- <code>explorer %LOCALAPPDATA%\\Programs\\Ollama</code> to browse the binaries (The installer adds this to your user PATH)\n- <code>explorer %HOMEPATH%\\.ollama</code> to browse where models and configuration is stored</p>\n<p>To enable additional debug logging to help troubleshoot problems, first <strong>Quit the running app from the tray menu</strong> then in a powershell terminal</p>\n<pre class=\"codehilite\"><code class=\"language-powershell\">$env:OLLAMA_DEBUG=&quot;1&quot;\n&amp; &quot;ollama app.exe&quot;\n</code></pre>\n\n<p>Join the <a href=\"https://discord.gg/ollama\">Discord</a> for help interpreting the logs.</p>\n<h2 id=\"llm-libraries\">LLM libraries</h2>\n<p>Ollama includes multiple LLM libraries compiled for different GPUs and CPU vector features. Ollama tries to pick the best one based on the capabilities of your system. If this autodetection has problems, or you run into other problems (e.g. crashes in your GPU) you can workaround this by forcing a specific LLM library. <code>cpu_avx2</code> will perform the best, followed by <code>cpu_avx</code> and the slowest but most compatible is <code>cpu</code>. Rosetta emulation under MacOS will work with the <code>cpu</code> library.</p>\n<p>In the server log, you will see a message that looks something like this (varies from release to release):</p>\n<pre class=\"codehilite\"><code>Dynamic LLM libraries [rocm_v6 cpu cpu_avx cpu_avx2 cuda_v12 rocm_v5]\n</code></pre>\n\n<p><strong>Experimental LLM Library Override</strong></p>\n<p>You can set OLLAMA_LLM_LIBRARY to any of the available LLM libraries to bypass autodetection, so for example, if you have a CUDA card, but want to force the CPU LLM library with AVX2 vector support, use:</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">OLLAMA_LLM_LIBRARY=&quot;cpu_avx2&quot; ollama serve\n</code></pre>\n\n<p>You can see what features your CPU has with the following.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">cat /proc/cpuinfo| grep flags | head -1\n</code></pre>\n\n<h2 id=\"installing-older-or-pre-release-versions-on-linux\">Installing older or pre-release versions on Linux</h2>\n<p>If you run into problems on Linux and want to install an older version, or you'd like to try out a pre-release before it's officially released, you can tell the install script which version to install.</p>\n<pre class=\"codehilite\"><code class=\"language-shell\">curl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh\n</code></pre>\n\n<h2 id=\"linux-docker\">Linux docker</h2>\n<p>If Ollama initially works on the GPU in a docker container, but then switches to running on CPU after some period of time with errors in the server log reporting GPU discovery failures, this can be resolved by disabling systemd cgroup management in Docker.  Edit <code>/etc/docker/daemon.json</code> on the host and add <code>\"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]</code> to the docker configuration.</p>\n<h2 id=\"nvidia-gpu-discovery\">NVIDIA GPU Discovery</h2>\n<p>When Ollama starts up, it takes inventory of the GPUs present in the system to determine compatibility and how much VRAM is available.  Sometimes this discovery can fail to find your GPUs.  In general, running the latest driver will yield the best results.</p>\n<h3 id=\"linux-nvidia-troubleshooting\">Linux NVIDIA Troubleshooting</h3>\n<p>If you are using a container to run Ollama, make sure you've set up the container runtime first as described in <a href=\"./docker.md\">docker.md</a></p>\n<p>Sometimes the Ollama can have difficulties initializing the GPU. When you check the server logs, this can show up as various error codes, such as \"3\" (not initialized), \"46\" (device unavailable), \"100\" (no device), \"999\" (unknown), or others. The following troubleshooting techniques may help resolve the problem</p>\n<ul>\n<li>If you are using a container, is the container runtime working?  Try <code>docker run --gpus all ubuntu nvidia-smi</code> - if this doesn't work, Ollama won't be able to see your NVIDIA GPU.</li>\n<li>Is the uvm driver loaded? <code>sudo nvidia-modprobe -u</code></li>\n<li>Try reloading the nvidia_uvm driver - <code>sudo rmmod nvidia_uvm</code> then <code>sudo modprobe nvidia_uvm</code></li>\n<li>Try rebooting</li>\n<li>Make sure you're running the latest nvidia drivers</li>\n</ul>\n<p>If none of those resolve the problem, gather additional information and file an issue:\n- Set <code>CUDA_ERROR_LEVEL=50</code> and try again to get more diagnostic logs\n- Check dmesg for any errors <code>sudo dmesg | grep -i nvrm</code> and <code>sudo dmesg | grep -i nvidia</code></p>\n<h2 id=\"amd-gpu-discovery\">AMD GPU Discovery</h2>\n<p>On linux, AMD GPU access typically requires <code>video</code> and/or <code>render</code> group membership to access the <code>/dev/kfd</code> device.  If permissions are not set up correctly, Ollama will detect this and report an error in the server log.</p>\n<p>When running in a container, in some Linux distributions and container runtimes, the ollama process may be unable to access the GPU.  Use <code>ls -lnd /dev/kfd /dev/dri /dev/dri/*</code> on the host system to determine the <strong>numeric</strong> group IDs on your system, and pass additional <code>--group-add ...</code> arguments to the container so it can access the required devices.   For example, in the following output <code>crw-rw---- 1 0  44 226,   0 Sep 16 16:55 /dev/dri/card0</code> the group ID column is <code>44</code></p>\n<p>If you are experiencing problems getting Ollama to correctly discover or use your GPU for inference, the following may help isolate the failure.\n- <code>AMD_LOG_LEVEL=3</code> Enable info log levels in the AMD HIP/ROCm libraries.  This can help show more detailed error codes that can help troubleshoot problems\n- <code>OLLAMA_DEBUG=1</code> During GPU discovery additional information will be reported\n- Check dmesg for any errors from amdgpu or kfd drivers <code>sudo dmesg | grep -i amdgpu</code> and <code>sudo dmesg | grep -i kfd</code></p>\n<h2 id=\"multiple-amd-gpus\">Multiple AMD GPUs</h2>\n<p>If you experience gibberish responses when models load across multiple AMD GPUs on Linux, see the following guide.</p>\n<ul>\n<li>https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/mgpu.html#mgpu-known-issues-and-limitations</li>\n</ul>\n<h2 id=\"windows-terminal-errors\">Windows Terminal Errors</h2>\n<p>Older versions of Windows 10 (e.g., 21H1) are known to have a bug where the standard terminal program does not display control characters correctly.  This can result in a long string of strings like <code>â[?25hâ[?25l</code> being displayed, sometimes erroring with <code>The parameter is incorrect</code>  To resolve this problem, please update to Win 10 22H1 or newer.</p>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# How to troubleshoot issues\n\nSometimes Ollama may not perform as expected. One of the best ways to figure out what happened is to take a look at the logs. Find the logs on **Mac** by running the command:\n\n```shell\ncat ~/.ollama/logs/server.log\n```\n\nOn **Linux** systems with systemd, the logs can be found with this command:\n\n```shell\njournalctl -u ollama --no-pager --follow --pager-end\n```\n\nWhen you run Ollama in a **container**, the logs go to stdout/stderr in the container:\n\n```shell\ndocker logs <container-name>\n```\n\n(Use `docker ps` to find the container name)\n\nIf manually running `ollama serve` in a terminal, the logs will be on that terminal.\n\nWhen you run Ollama on **Windows**, there are a few different locations. You can view them in the explorer window by hitting `<cmd>+R` and type in:\n- `explorer %LOCALAPPDATA%\\Ollama` to view logs.  The most recent server logs will be in `server.log` and older logs will be in `server-#.log`\n- `explorer %LOCALAPPDATA%\\Programs\\Ollama` to browse the binaries (The installer adds this to your user PATH)\n- `explorer %HOMEPATH%\\.ollama` to browse where models and configuration is stored\n\nTo enable additional debug logging to help troubleshoot problems, first **Quit the running app from the tray menu** then in a powershell terminal\n\n```powershell\n$env:OLLAMA_DEBUG=\"1\"\n& \"ollama app.exe\"\n```\n\nJoin the [Discord](https://discord.gg/ollama) for help interpreting the logs.\n\n## LLM libraries\n\nOllama includes multiple LLM libraries compiled for different GPUs and CPU vector features. Ollama tries to pick the best one based on the capabilities of your system. If this autodetection has problems, or you run into other problems (e.g. crashes in your GPU) you can workaround this by forcing a specific LLM library. `cpu_avx2` will perform the best, followed by `cpu_avx` and the slowest but most compatible is `cpu`. Rosetta emulation under MacOS will work with the `cpu` library.\n\nIn the server log, you will see a message that looks something like this (varies from release to release):\n\n```\nDynamic LLM libraries [rocm_v6 cpu cpu_avx cpu_avx2 cuda_v12 rocm_v5]\n```\n\n**Experimental LLM Library Override**\n\nYou can set OLLAMA_LLM_LIBRARY to any of the available LLM libraries to bypass autodetection, so for example, if you have a CUDA card, but want to force the CPU LLM library with AVX2 vector support, use:\n\n```shell\nOLLAMA_LLM_LIBRARY=\"cpu_avx2\" ollama serve\n```\n\nYou can see what features your CPU has with the following.\n\n```shell\ncat /proc/cpuinfo| grep flags | head -1\n```\n\n## Installing older or pre-release versions on Linux\n\nIf you run into problems on Linux and want to install an older version, or you'd like to try out a pre-release before it's officially released, you can tell the install script which version to install.\n\n```shell\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=0.5.7 sh\n```\n\n## Linux docker\n\nIf Ollama initially works on the GPU in a docker container, but then switches to running on CPU after some period of time with errors in the server log reporting GPU discovery failures, this can be resolved by disabling systemd cgroup management in Docker.  Edit `/etc/docker/daemon.json` on the host and add `\"exec-opts\": [\"native.cgroupdriver=cgroupfs\"]` to the docker configuration.\n\n## NVIDIA GPU Discovery\n\nWhen Ollama starts up, it takes inventory of the GPUs present in the system to determine compatibility and how much VRAM is available.  Sometimes this discovery can fail to find your GPUs.  In general, running the latest driver will yield the best results.\n\n### Linux NVIDIA Troubleshooting\n\nIf you are using a container to run Ollama, make sure you've set up the container runtime first as described in [docker.md](./docker.md)\n\nSometimes the Ollama can have difficulties initializing the GPU. When you check the server logs, this can show up as various error codes, such as \"3\" (not initialized), \"46\" (device unavailable), \"100\" (no device), \"999\" (unknown), or others. The following troubleshooting techniques may help resolve the problem\n\n- If you are using a container, is the container runtime working?  Try `docker run --gpus all ubuntu nvidia-smi` - if this doesn't work, Ollama won't be able to see your NVIDIA GPU.\n- Is the uvm driver loaded? `sudo nvidia-modprobe -u`\n- Try reloading the nvidia_uvm driver - `sudo rmmod nvidia_uvm` then `sudo modprobe nvidia_uvm`\n- Try rebooting\n- Make sure you're running the latest nvidia drivers\n\nIf none of those resolve the problem, gather additional information and file an issue:\n- Set `CUDA_ERROR_LEVEL=50` and try again to get more diagnostic logs\n- Check dmesg for any errors `sudo dmesg | grep -i nvrm` and `sudo dmesg | grep -i nvidia`\n\n\n## AMD GPU Discovery\n\nOn linux, AMD GPU access typically requires `video` and/or `render` group membership to access the `/dev/kfd` device.  If permissions are not set up correctly, Ollama will detect this and report an error in the server log.\n\nWhen running in a container, in some Linux distributions and container runtimes, the ollama process may be unable to access the GPU.  Use `ls -lnd /dev/kfd /dev/dri /dev/dri/*` on the host system to determine the **numeric** group IDs on your system, and pass additional `--group-add ...` arguments to the container so it can access the required devices.   For example, in the following output `crw-rw---- 1 0  44 226,   0 Sep 16 16:55 /dev/dri/card0` the group ID column is `44`\n\nIf you are experiencing problems getting Ollama to correctly discover or use your GPU for inference, the following may help isolate the failure.\n- `AMD_LOG_LEVEL=3` Enable info log levels in the AMD HIP/ROCm libraries.  This can help show more detailed error codes that can help troubleshoot problems\n- `OLLAMA_DEBUG=1` During GPU discovery additional information will be reported\n- Check dmesg for any errors from amdgpu or kfd drivers `sudo dmesg | grep -i amdgpu` and `sudo dmesg | grep -i kfd`\n\n## Multiple AMD GPUs\n\nIf you experience gibberish responses when models load across multiple AMD GPUs on Linux, see the following guide.\n\n- https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/mgpu.html#mgpu-known-issues-and-limitations\n\n## Windows Terminal Errors\n\nOlder versions of Windows 10 (e.g., 21H1) are known to have a bug where the standard terminal program does not display control characters correctly.  This can result in a long string of strings like `â[?25hâ[?25l` being displayed, sometimes erroring with `The parameter is incorrect`  To resolve this problem, please update to Win 10 22H1 or newer.\n"
  },
  {
    "name": "windows.md",
    "relative_path": "windows.md",
    "path": "docs/windows.md",
    "url": "https://raw.githubusercontent.com/ollama/ollama/main/docs/windows.md",
    "content": "\n            <!DOCTYPE html>\n            <html>\n            <head>\n                <meta charset=\"utf-8\">\n                <title>windows.md</title>\n                <style>\n                    body { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; margin: 2em; line-height: 1.6; }\n                    pre { background: #f6f8fa; padding: 1em; overflow: auto; border-radius: 6px; border: 1px solid #d0d7de; }\n                    code { background: #f6f8fa; padding: 0.2em 0.4em; border-radius: 3px; font-size: 85%; }\n                    h1, h2, h3, h4, h5, h6 { color: #24292f; margin-top: 24px; margin-bottom: 16px; }\n                    h1 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    h2 { border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }\n                    blockquote { border-left: 4px solid #d0d7de; margin: 0; padding: 0 1em; color: #656d76; }\n                    table { border-collapse: collapse; width: 100%; margin: 16px 0; }\n                    th, td { border: 1px solid #d0d7de; padding: 6px 13px; text-align: left; }\n                    th { background-color: #f6f8fa; font-weight: 600; }\n                    .toc { background: #f6f8fa; padding: 1em; border-radius: 6px; margin: 1em 0; }\n                </style>\n            </head>\n            <body>\n                <h1>windows.md</h1>\n                <p><em>Source: <a href=\"https://raw.githubusercontent.com/ollama/ollama/main/docs/windows.md\">docs/windows.md</a></em></p>\n                <hr>\n                <h1 id=\"ollama-windows\">Ollama Windows</h1>\n<p>Welcome to Ollama for Windows.</p>\n<p>No more WSL required!</p>\n<p>Ollama now runs as a native Windows application, including NVIDIA and AMD Radeon GPU support.\nAfter installing Ollama for Windows, Ollama will run in the background and\nthe <code>ollama</code> command line is available in <code>cmd</code>, <code>powershell</code> or your favorite\nterminal application. As usual the Ollama <a href=\"./api.md\">api</a> will be served on\n<code>http://localhost:11434</code>.</p>\n<h2 id=\"system-requirements\">System Requirements</h2>\n<ul>\n<li>Windows 10 22H2 or newer, Home or Pro</li>\n<li>NVIDIA 452.39 or newer Drivers if you have an NVIDIA card</li>\n<li>AMD Radeon Driver https://www.amd.com/en/support if you have a Radeon card</li>\n</ul>\n<p>Ollama uses unicode characters for progress indication, which may render as unknown squares in some older terminal fonts in Windows 10. If you see this, try changing your terminal font settings.</p>\n<h2 id=\"filesystem-requirements\">Filesystem Requirements</h2>\n<p>The Ollama install does not require Administrator, and installs in your home directory by default.  You'll need at least 4GB of space for the binary install.  Once you've installed Ollama, you'll need additional space for storing the Large Language models, which can be tens to hundreds of GB in size.  If your home directory doesn't have enough space, you can change where the binaries are installed, and where the models are stored.</p>\n<h3 id=\"changing-install-location\">Changing Install Location</h3>\n<p>To install the Ollama application in a location different than your home directory, start the installer with the following flag</p>\n<pre class=\"codehilite\"><code class=\"language-powershell\">OllamaSetup.exe /DIR=&quot;d:\\some\\location&quot;\n</code></pre>\n\n<h2 id=\"api-access\">API Access</h2>\n<p>Here's a quick example showing API access from <code>powershell</code></p>\n<pre class=\"codehilite\"><code class=\"language-powershell\">(Invoke-WebRequest -method POST -Body '{&quot;model&quot;:&quot;llama3.2&quot;, &quot;prompt&quot;:&quot;Why is the sky blue?&quot;, &quot;stream&quot;: false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json\n</code></pre>\n\n<h2 id=\"troubleshooting\">Troubleshooting</h2>\n<p>Ollama on Windows stores files in a few different locations.  You can view them in\nthe explorer window by hitting <code>&lt;Ctrl&gt;+R</code> and type in:\n- <code>explorer %LOCALAPPDATA%\\Ollama</code> contains logs, and downloaded updates\n    - <em>app.log</em> contains most resent logs from the GUI application\n    - <em>server.log</em> contains the most recent server logs\n    - <em>upgrade.log</em> contains log output for upgrades\n- <code>explorer %LOCALAPPDATA%\\Programs\\Ollama</code> contains the binaries (The installer adds this to your user PATH)\n- <code>explorer %HOMEPATH%\\.ollama</code> contains models and configuration</p>\n<h2 id=\"uninstall\">Uninstall</h2>\n<p>The Ollama Windows installer registers an Uninstaller application.  Under <code>Add or remove programs</code> in Windows Settings, you can uninstall Ollama.</p>\n<blockquote>\n<p>[!NOTE]\nIf you have <a href=\"#changing-model-location\">changed the OLLAMA_MODELS location</a>, the installer will not remove your downloaded models</p>\n</blockquote>\n<h2 id=\"standalone-cli\">Standalone CLI</h2>\n<p>The easiest way to install Ollama on Windows is to use the <code>OllamaSetup.exe</code>\ninstaller. It installs in your account without requiring Administrator rights.\nWe update Ollama regularly to support the latest models, and this installer will\nhelp you keep up to date.</p>\n<p>If you'd like to install or integrate Ollama as a service, a standalone\n<code>ollama-windows-amd64.zip</code> zip file is available containing only the Ollama CLI\nand GPU library dependencies for Nvidia.  If you have an AMD GPU, also download\nand extract the additional ROCm package <code>ollama-windows-amd64-rocm.zip</code> into the\nsame directory.  This allows for embedding Ollama in existing applications, or\nrunning it as a system service via <code>ollama serve</code> with tools such as\n<a href=\"https://nssm.cc/\">NSSM</a>. </p>\n<blockquote>\n<p>[!NOTE]<br />\nIf you are upgrading from a prior version, you should remove the old directories first.</p>\n</blockquote>\n            </body>\n            </html>\n            ",
    "raw_markdown": "# Ollama Windows\n\nWelcome to Ollama for Windows.\n\nNo more WSL required!\n\nOllama now runs as a native Windows application, including NVIDIA and AMD Radeon GPU support.\nAfter installing Ollama for Windows, Ollama will run in the background and\nthe `ollama` command line is available in `cmd`, `powershell` or your favorite\nterminal application. As usual the Ollama [api](./api.md) will be served on\n`http://localhost:11434`.\n\n## System Requirements\n\n* Windows 10 22H2 or newer, Home or Pro\n* NVIDIA 452.39 or newer Drivers if you have an NVIDIA card\n* AMD Radeon Driver https://www.amd.com/en/support if you have a Radeon card\n\nOllama uses unicode characters for progress indication, which may render as unknown squares in some older terminal fonts in Windows 10. If you see this, try changing your terminal font settings.\n\n## Filesystem Requirements\n\nThe Ollama install does not require Administrator, and installs in your home directory by default.  You'll need at least 4GB of space for the binary install.  Once you've installed Ollama, you'll need additional space for storing the Large Language models, which can be tens to hundreds of GB in size.  If your home directory doesn't have enough space, you can change where the binaries are installed, and where the models are stored.\n\n### Changing Install Location\n\nTo install the Ollama application in a location different than your home directory, start the installer with the following flag\n\n```powershell\nOllamaSetup.exe /DIR=\"d:\\some\\location\"\n```\n\n## API Access\n\nHere's a quick example showing API access from `powershell`\n\n```powershell\n(Invoke-WebRequest -method POST -Body '{\"model\":\"llama3.2\", \"prompt\":\"Why is the sky blue?\", \"stream\": false}' -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json\n```\n\n## Troubleshooting\n\nOllama on Windows stores files in a few different locations.  You can view them in\nthe explorer window by hitting `<Ctrl>+R` and type in:\n- `explorer %LOCALAPPDATA%\\Ollama` contains logs, and downloaded updates\n    - *app.log* contains most resent logs from the GUI application\n    - *server.log* contains the most recent server logs\n    - *upgrade.log* contains log output for upgrades\n- `explorer %LOCALAPPDATA%\\Programs\\Ollama` contains the binaries (The installer adds this to your user PATH)\n- `explorer %HOMEPATH%\\.ollama` contains models and configuration\n\n## Uninstall\n\nThe Ollama Windows installer registers an Uninstaller application.  Under `Add or remove programs` in Windows Settings, you can uninstall Ollama.\n\n> [!NOTE]\n> If you have [changed the OLLAMA_MODELS location](#changing-model-location), the installer will not remove your downloaded models\n\n\n## Standalone CLI\n\nThe easiest way to install Ollama on Windows is to use the `OllamaSetup.exe`\ninstaller. It installs in your account without requiring Administrator rights.\nWe update Ollama regularly to support the latest models, and this installer will\nhelp you keep up to date.\n\nIf you'd like to install or integrate Ollama as a service, a standalone\n`ollama-windows-amd64.zip` zip file is available containing only the Ollama CLI\nand GPU library dependencies for Nvidia.  If you have an AMD GPU, also download\nand extract the additional ROCm package `ollama-windows-amd64-rocm.zip` into the\nsame directory.  This allows for embedding Ollama in existing applications, or\nrunning it as a system service via `ollama serve` with tools such as\n[NSSM](https://nssm.cc/). \n\n> [!NOTE]  \n> If you are upgrading from a prior version, you should remove the old directories first.\n"
  }
]